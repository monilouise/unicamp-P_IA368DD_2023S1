{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3drPVvJ-orF1"
      },
      "source": [
        "# Efficiency vs quality tradeoffs\n",
        "\n",
        "Author: Monique Monteiro - moniquelouise@gmail.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ3_Poc4ouzM",
        "outputId": "33b6220b-7b1b-4191-ba3d-7e3255cb9516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lV0aDFt4o3iW"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/gdrive/MyDrive/Unicamp-aula-10\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EYi0xNgpJHf"
      },
      "source": [
        "## Libraries installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKeAssOh8LYB",
        "outputId": "3b922fc5-ea2c-4811-ad86-b48947630ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=e395c7562144b17daeb1f64c54ee50a00f8261d9c96eaa0a6c9a3120a65d1222\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvF-TJxA-gMe",
        "outputId": "c1037c0b-c1ec-429a-beac-c2b6b778f5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hRh0qAhsKEeW"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install pyserini -q\n",
        "!pip install faiss-cpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dDzq5gqrVzqv"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate -q\n",
        "!pip install trectools -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IkDbAShW0rMm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfzi7hH2pkKY"
      },
      "source": [
        "## Passages download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDUGtSRSpKSs",
        "outputId": "6e09e959-fad7-47d4-fb8a-4332706e7f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-06 21:33:47--  https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.68.44, 18.155.68.121, 18.155.68.38, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.68.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/e9e97686e3138eaff989f67c04cd32e8f8f4c0d4857187e3f180275b23e24e85?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27corpus.jsonl.gz%3B+filename%3D%22corpus.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668029&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvZTllOTc2ODZlMzEzOGVhZmY5ODlmNjdjMDRjZDMyZThmOGY0YzBkNDg1NzE4N2UzZjE4MDI3NWIyM2UyNGU4NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwMjl9fX1dfQ__&Signature=SndAv-KsuG-ohRWzB3nzcv7gwcZRZUtwZ078nBzTXE3PvtMmgbSjt6fmJfuWMObwm2yWfmAtv1wLW7zls0EFVlD5hpDqnHRdrl3SiKRvIf53WJbD9auvSfSlhFRTpaLdX3ODK9nbmSiJRExLYo1o3UtvzzbzOqt1zI7UsrMCs%7EvvbCTn03%7ETREmrfUN1ulLe5xXRU%7EefSiWW1gxMlh4xVAbSQLGwkjz3KLGCTMXU4iFDryqviIt3mbfGUxVraAjQoOgerTI9N1O6OpS7q2SkTWadYWu0YvNTtrkaKqp-SoFE9fB-lz1Y3wo1R72WmUh46JDHGgiguh762PxV71sPTw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-06 21:33:48--  https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/e9e97686e3138eaff989f67c04cd32e8f8f4c0d4857187e3f180275b23e24e85?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27corpus.jsonl.gz%3B+filename%3D%22corpus.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668029&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvZTllOTc2ODZlMzEzOGVhZmY5ODlmNjdjMDRjZDMyZThmOGY0YzBkNDg1NzE4N2UzZjE4MDI3NWIyM2UyNGU4NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwMjl9fX1dfQ__&Signature=SndAv-KsuG-ohRWzB3nzcv7gwcZRZUtwZ078nBzTXE3PvtMmgbSjt6fmJfuWMObwm2yWfmAtv1wLW7zls0EFVlD5hpDqnHRdrl3SiKRvIf53WJbD9auvSfSlhFRTpaLdX3ODK9nbmSiJRExLYo1o3UtvzzbzOqt1zI7UsrMCs%7EvvbCTn03%7ETREmrfUN1ulLe5xXRU%7EefSiWW1gxMlh4xVAbSQLGwkjz3KLGCTMXU4iFDryqviIt3mbfGUxVraAjQoOgerTI9N1O6OpS7q2SkTWadYWu0YvNTtrkaKqp-SoFE9fB-lz1Y3wo1R72WmUh46JDHGgiguh762PxV71sPTw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.98, 18.155.68.94, 18.155.68.73, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73452199 (70M) [application/gzip]\n",
            "Saving to: ‘corpus.jsonl.gz’\n",
            "\n",
            "corpus.jsonl.gz     100%[===================>]  70.05M   304MB/s    in 0.2s    \n",
            "\n",
            "2023-05-06 21:33:48 (304 MB/s) - ‘corpus.jsonl.gz’ saved [73452199/73452199]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m8ror6dpokw"
      },
      "outputs": [],
      "source": [
        "!mv corpus.jsonl.gz {main_dir}/trec-covid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVXGvmSz4i8R",
        "outputId": "c809ba02-5f17-462d-abfb-d6ae8b4c4ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gzip: /content/gdrive/MyDrive/Unicamp-aula-10/trec-covid/corpus.jsonl already exists; do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "!gunzip {main_dir}/trec-covid/corpus.jsonl.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXnckUxf4-wL"
      },
      "source": [
        "## Queries vs passages download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqCxXLfV4_gE",
        "outputId": "30e23791-b70f-4148-b75d-e0ad2001b3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-06 21:34:54--  https://huggingface.co/datasets/BeIR/trec-covid-qrels/raw/main/test.tsv\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.68.44, 18.155.68.116, 18.155.68.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.68.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 980831 (958K) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>] 957.84K  1.09MB/s    in 0.9s    \n",
            "\n",
            "2023-05-06 21:34:55 (1.09 MB/s) - ‘test.tsv’ saved [980831/980831]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/BeIR/trec-covid-qrels/raw/main/test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO-OYpej5ElH"
      },
      "outputs": [],
      "source": [
        "!mv test.tsv {main_dir}/trec-covid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdoSaFnH5P7p"
      },
      "source": [
        "## Queries download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP5BWbgv5Gz3",
        "outputId": "39a631c4-9373-4ff3-8c94-fffe2fa2bd4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-06 21:34:56--  https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/queries.jsonl.gz\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.68.44, 18.155.68.116, 18.155.68.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.68.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/9eadcc2cdf140addc9dae83648bb2c6611f5e4b66eaed7475fa5a0ca48eda371?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27queries.jsonl.gz%3B+filename%3D%22queries.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668096&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvOWVhZGNjMmNkZjE0MGFkZGM5ZGFlODM2NDhiYjJjNjYxMWY1ZTRiNjZlYWVkNzQ3NWZhNWEwY2E0OGVkYTM3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwOTZ9fX1dfQ__&Signature=m2lx5g8Ip6rMW3I9jGfzPPtZINejDEj2f8XwtGpuukp5CKEWuXrd163mK3%7EVR1n3hhzfiNLuBToxx0lqarAtV09mxvsY36c%7Et3PsKmT4lz3raQBNoFP7OfmgAoXLR%7EMi-Vx7woOO46rsugcf63XPI7kqqv9SppFKcA35VI-v0tMxwcIMGYvCjwj3wwpWDEoW5QEQjLxyJNrMzG0Q%7EivdvzhZ4VHyygj8NkdntMBijB3ZIsbG-OrwJG6PaqiZ2mD1S5V7aERr7HL927UZJLUwoHVL5Gx1MCW3M8IslD1BekktsVqj7qM9NS8AEk382pV7Y4PUvQldnxQYjfp1O9lAgw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-06 21:34:56--  https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/9eadcc2cdf140addc9dae83648bb2c6611f5e4b66eaed7475fa5a0ca48eda371?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27queries.jsonl.gz%3B+filename%3D%22queries.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668096&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvOWVhZGNjMmNkZjE0MGFkZGM5ZGFlODM2NDhiYjJjNjYxMWY1ZTRiNjZlYWVkNzQ3NWZhNWEwY2E0OGVkYTM3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwOTZ9fX1dfQ__&Signature=m2lx5g8Ip6rMW3I9jGfzPPtZINejDEj2f8XwtGpuukp5CKEWuXrd163mK3%7EVR1n3hhzfiNLuBToxx0lqarAtV09mxvsY36c%7Et3PsKmT4lz3raQBNoFP7OfmgAoXLR%7EMi-Vx7woOO46rsugcf63XPI7kqqv9SppFKcA35VI-v0tMxwcIMGYvCjwj3wwpWDEoW5QEQjLxyJNrMzG0Q%7EivdvzhZ4VHyygj8NkdntMBijB3ZIsbG-OrwJG6PaqiZ2mD1S5V7aERr7HL927UZJLUwoHVL5Gx1MCW3M8IslD1BekktsVqj7qM9NS8AEk382pV7Y4PUvQldnxQYjfp1O9lAgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.128, 18.155.68.98, 18.155.68.94, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4702 (4.6K) [application/gzip]\n",
            "Saving to: ‘queries.jsonl.gz’\n",
            "\n",
            "queries.jsonl.gz    100%[===================>]   4.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-06 21:34:56 (191 MB/s) - ‘queries.jsonl.gz’ saved [4702/4702]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/queries.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiRancl45JXE"
      },
      "outputs": [],
      "source": [
        "!mv queries.jsonl.gz {main_dir}/trec-covid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kCDz7Xn5LO5",
        "outputId": "89d28777-939a-40c5-c48f-0ab17ddc4b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gzip: /content/gdrive/MyDrive/Unicamp-aula-10/trec-covid/queries.jsonl already exists; do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "!gunzip {main_dir}/trec-covid/queries.jsonl.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqHnZbI9eb6"
      },
      "source": [
        "## Pipeline 1: Pyserini BM25 + Reranking with cross-encoder/ms-marco-MiniLM-L-6-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51HR2Sid9skM"
      },
      "source": [
        "### 1st step: Pre-indexing TREC-COVID corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZWmH9OB90OS"
      },
      "source": [
        "Here, we need to measure indexing cost in dollars per hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "--xMWixxHlFd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psDPW3t-HmUL",
        "outputId": "a3515962-bdb6-459d-f59d-4d2218c1d95a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqL5bZuZ5axg",
        "outputId": "b4ba00d7-07b2-4a16-9974-178c66e96c20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/cross-encoder_ms-marco-MiniLM-L-6-v2. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/cross-encoder_ms-marco-MiniLM-L-6-v2 were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "\n",
        "embedder = SentenceTransformer(model_name, device=device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AxOt8PHY-ljT"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "passage_ids = []\n",
        "passage_texts = []\n",
        "id_to_passage = dict()\n",
        "\n",
        "#Maps each id to its position in the corpus\n",
        "id_to_index = dict()\n",
        "\n",
        "with jsonlines.open(f\"{main_dir}/trec-covid/corpus.jsonl\") as reader:\n",
        "  for item in reader:\n",
        "    id = item[\"_id\"]\n",
        "    id_to_index[id] = len(passage_ids)\n",
        "    passage_ids.append(id)\n",
        "    text = item[\"title\"] + ' ' + item[\"text\"]\n",
        "    passage_texts.append(text)\n",
        "    id_to_passage[id] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKwY_vUv-O26",
        "outputId": "3c5736b6-13e1-439b-b368-e36dbab4ac32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 29s, sys: 10.1 s, total: 6min 39s\n",
            "Wall time: 3min\n"
          ]
        }
      ],
      "source": [
        "%time corpus_embeddings = embedder.encode(passage_texts, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPmP5CgkAjat",
        "outputId": "2bd22f08-a607-4091-9bd7-0abd01d02b24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([171332, 384])"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJpefhdUGRu5",
        "outputId": "6bbbe292-2569-44bf-c08b-751b499756dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting gpu_usage.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile gpu_usage.sh\n",
        "#! /bin/bash\n",
        "#comment: run for 10 seconds, change it as per your use\n",
        "end=$((SECONDS+3600))\n",
        "\n",
        "while [ $SECONDS -lt $end ]; do\n",
        "    nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu >> gpu.log\n",
        "    #comment: or use below command and comment above using #\n",
        "    #nvidia-smi dmon -i 0 -s mu -d 1 -o TD >> gpu.log\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBHsfxtfGSiJ"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "bash gpu_usage.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ4_tEjKGYW-"
      },
      "outputs": [],
      "source": [
        "corpus_embeddings = embedder.encode(passage_texts, convert_to_tensor=True) #2m41s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Bqh9Y8GdGu",
        "outputId": "20cd7019-c4a5-4fa9-f9a7-798c7aab631f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([171332, 384])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0zz1_t2LzHN"
      },
      "source": [
        "### Pipeline - Searching with Pyserini BM25 + Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vY7H4KexI3t5"
      },
      "outputs": [],
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wikXMInXL8Eg"
      },
      "outputs": [],
      "source": [
        "def search_with_bm25(query,k = 1000):\n",
        "  searcher = LuceneSearcher.from_prebuilt_index('beir-v1.0.0-trec-covid.flat')\n",
        "  hits = searcher.search(query, k)\n",
        "  return hits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cQpIOs4pMR9P"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "query_ids = []\n",
        "query_texts = []\n",
        "\n",
        "with jsonlines.open(f\"{main_dir}/trec-covid/queries.jsonl\") as reader:\n",
        "  for item in reader:\n",
        "    id = item[\"_id\"]\n",
        "    query_ids.append(id)\n",
        "    text = item[\"text\"]\n",
        "    query_texts.append(text)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yUsv0DchMZ04",
        "outputId": "44f8f99a-5367-40b1-9937-2e8e21c93e5e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'what is the origin of COVID-19'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onbNVfB4Mnh1"
      },
      "source": [
        "Generates the first request to download pre-indexed corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rxJ7IOrMb5g",
        "outputId": "cfc45b17-4dec-4627-e969-3f2b154c0afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading index at https://rgw.cs.uwaterloo.ca/pyserini/indexes/lucene-index.beir-v1.0.0-trec-covid.flat.20221116.505594.tar.gz...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "lucene-index.beir-v1.0.0-trec-covid.flat.20221116.505594.tar.gz: 216MB [00:20, 11.1MB/s]                           \n"
          ]
        }
      ],
      "source": [
        "hits = search_with_bm25(query_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME6w_F-8MtfF",
        "outputId": "519af31d-b7b3-45b7-ff5d-6a502916c382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(hits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktRX6xrnMuDC",
        "outputId": "eb2394bb-f027-456b-ba47-a59704383b74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'_id': 'dv9m19yk',\n",
              " 'title': '[What is the origin of SARS-CoV-2?]',\n",
              " 'text': 'Every time a pandemic occurs, dozens of theories emerge to attribute the origin of the event to different facts. The COVID-19 pandemic that has hit virtually all the globe has been no exception. What is known so far about the origin of the virus that causes COVID 19? The first investigations on the origin of this disease have determined that it is a new type of virus, the origin of which is most likely zoonotic.',\n",
              " 'metadata': {'url': 'https://www.ncbi.nlm.nih.gov/pubmed/32412715/',\n",
              "  'pubmed_id': '32412715'}}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jsondoc = json.loads(hits[0].raw)\n",
        "jsondoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "konl44TmM7Om",
        "outputId": "c6459aa3-d0d3-418c-ca6b-68c47e9b7d46"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[What is the origin of SARS-CoV-2?] Every time a pandemic occurs, dozens of theories emerge to attribute the origin of the event to different facts. The COVID-19 pandemic that has hit virtually all the globe has been no exception. What is known so far about the origin of the virus that causes COVID 19? The first investigations on the origin of this disease have determined that it is a new type of virus, the origin of which is most likely zoonotic.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_passage['dv9m19yk']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQdkFOT-NWlR",
        "outputId": "8d2136da-77bb-407d-9b68-5df6e01ee014"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "81848"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_index['dv9m19yk']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TGzcOPr6OD_A",
        "outputId": "617174f7-fd2f-49d8-c2e7-7c4a7bb2b185"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dv9m19yk'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passage_ids[81848]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7_wz8VbXOGB1",
        "outputId": "3162aac7-7176-42b3-eb75-c1b5a3134a9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[What is the origin of SARS-CoV-2?] Every time a pandemic occurs, dozens of theories emerge to attribute the origin of the event to different facts. The COVID-19 pandemic that has hit virtually all the globe has been no exception. What is known so far about the origin of the virus that causes COVID 19? The first investigations on the origin of this disease have determined that it is a new type of virus, the origin of which is most likely zoonotic.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passage_texts[81848]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtkoJKD1OH95",
        "outputId": "ac68c8d2-82a4-4bb3-a8e4-231c922f13e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([384])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_embeddings[81848].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8lXwOfTOUYG"
      },
      "outputs": [],
      "source": [
        "def search(query, top_k=1000):\n",
        "  #1st step: searches with BM25\n",
        "  bm25_hits = search_with_bm25(query, k=top_k)\n",
        "\n",
        "  ids = [json.loads(bm25_hits[i].raw)['_id'] for i in range(len(bm25_hits))]\n",
        "  indices = [id_to_index[id] for id in ids]\n",
        "\n",
        "  #Finds corpus embeddings\n",
        "  passages_embeddings = [corpus_embeddings[i] for i in indices]\n",
        "\n",
        "  query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "  hits = util.semantic_search(query_embedding, passages_embeddings, top_k=top_k)\n",
        "  hits = hits[0]      #Get the hits for the first query\n",
        "  results = dict()\n",
        "  for hit in hits:\n",
        "    assert hit['corpus_id'] < 1000\n",
        "    results[ids[hit['corpus_id']]] = hit['score']\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzpeJBOvSDeL"
      },
      "outputs": [],
      "source": [
        "%time search(query_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nW0z7M02SJlC",
        "outputId": "db1d5a8b-79cc-4c9c-e224-9c9c1ca0fcb2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What are the economic implications of COVID-19 '"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_passage['gh07kf93']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "loutUyHcSkNf",
        "outputId": "bbeb5838-0b72-4c6a-da52-222e16f1ed31"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What the Diamond Princess taught the world about covid-19 '"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_passage['ghuwz4w7']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x6aWpILLUV5l",
        "outputId": "623d57bc-4035-4c01-ec8a-2648e83f07a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What the COVID-19 Crisis Is Telling Humanity '"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_passage['x33dcjov']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1NpKv3c2UlUB",
        "outputId": "b6d74803-5d48-46f8-d3a5-9d1c2572a726"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What’s Important: Facing Fear in the Time of COVID-19 '"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_passage['xefpg0zo']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw6XUk1OVLl4"
      },
      "source": [
        "### Measuring NDCG@10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wzr4rJ3kWSBW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "qrel = pd.read_csv(f\"{main_dir}/trec-covid/test.tsv\", sep=\"\\t\", header=None, \n",
        "                   skiprows=1, names=[\"query\", \"docid\", \"rel\"])\n",
        "qrel[\"q0\"] = \"q0\"\n",
        "qrel = qrel.to_dict(orient=\"list\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioAKAyTYfg_i",
        "outputId": "2f43b89e-47a9-496a-dad3-53fecf8227cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66336"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(qrel[\"q0\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbC6zjDQVKP8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def search_all(top_k=1000):\n",
        "  run = defaultdict(list)\n",
        "\n",
        "  for id, query in tqdm(zip(query_ids, query_texts)):\n",
        "    results = search(query)\n",
        "    run[\"query\"] += [id] * top_k\n",
        "    run[\"docid\"] += results.keys()\n",
        "    run[\"score\"] += results.values()\n",
        "    run[\"q0\"] += [\"q0\"] * top_k\n",
        "    run[\"rank\"] += list(range(1,top_k+1))\n",
        "    run[\"system\"] += [model_name] * top_k\n",
        "\n",
        "  return run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7hFyxb2YExa",
        "outputId": "a724ab1e-2c62-408f-f5a5-741a95c584d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:07,  6.52it/s]\n"
          ]
        }
      ],
      "source": [
        "run = search_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKH8-OgNYtRi",
        "outputId": "8be9bf96-267e-4967-e517-d317501b1cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(run[\"system\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ANkR4QzgYF-a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from evaluate import load\n",
        "\n",
        "def eval_ndcg10(run):\n",
        "  trec_eval = load(\"trec_eval\")\n",
        "  results = trec_eval.compute(predictions=[run], references=[qrel])\n",
        "  return results['NDCG@10'] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_eK4xuIYiYg"
      },
      "source": [
        "Evaluates BM25 NDCG@10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buMr_hDlZE8s"
      },
      "outputs": [],
      "source": [
        "def search_all_bm25(top_k=1000):\n",
        "  run = defaultdict(list)\n",
        "\n",
        "  for id, query in tqdm(zip(query_ids, query_texts)):\n",
        "    bm25_hits = search_with_bm25(query)\n",
        "    ids = [json.loads(bm25_hits[i].raw)['_id'] for i in range(len(bm25_hits))]\n",
        "    run[\"query\"] += [id] * top_k\n",
        "    run[\"docid\"] += ids\n",
        "    run[\"score\"] += [1] * top_k\n",
        "    run[\"q0\"] += [\"q0\"] * top_k\n",
        "    run[\"rank\"] += list(range(1,top_k+1))\n",
        "    run[\"system\"] += [model_name] * top_k\n",
        "\n",
        "  return run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeDFUKYsZVi4",
        "outputId": "7c97a55b-587d-43a4-ffc0-26db1421e50d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:05,  9.08it/s]\n"
          ]
        }
      ],
      "source": [
        "run_bm25 = search_all_bm25()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHJLDlnbaOGR",
        "outputId": "b78fd958-d2c7-4ade-face-64c407c4f947"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5946917010118077"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_ndcg10(run_bm25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PhuNE_dYUhE",
        "outputId": "0ec2a170-b456-4d2e-d4a0-e42d4ea842b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.2754485142056899"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_ndcg10(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyX1m4eaeyi_"
      },
      "source": [
        "Conclusion: horrible quality for off-the-shelf usage.  It requiries fine tuning to be used on TREC-COVID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYQXlV9Hzx_J"
      },
      "source": [
        "## Pipeline 2: SPLADE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pDokC1fwvGg3"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, DistilBertTokenizer, DistilBertForMaskedLM\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DnuCdfuR0Q1i"
      },
      "outputs": [],
      "source": [
        "model_name_1 = 'naver/splade_v2_distil' \n",
        "model_name_2 = 'naver/splade-cocondenser-selfdistil'\n",
        "model_name_3 = 'naver/splade-cocondenser-ensembledistil' \n",
        "\n",
        "#tokenizer_1 = DistilBertTokenizer.from_pretrained(model_name_1)\n",
        "#model_1 = DistilBertForMaskedLM.from_pretrained(model_name_1)\n",
        "\n",
        "#tokenizer_2 = BertTokenizer.from_pretrained(model_name_2)\n",
        "#model_2 = BertForMaskedLM.from_pretrained(model_name_2)\n",
        "\n",
        "tokenizer_3 = AutoTokenizer.from_pretrained(model_name_3)\n",
        "model_3 = AutoModelForMaskedLM.from_pretrained(model_name_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BLF0gopU1wFh"
      },
      "outputs": [],
      "source": [
        "model = model_3\n",
        "tokenizer = tokenizer_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4uRoAXTB-KOc"
      },
      "outputs": [],
      "source": [
        "def vectorize_to_sparse_batch(batch, model=model, remove_special_tokens=True):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    output = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "\n",
        "  w_ij = output.logits.to(device)\n",
        "\n",
        "  #Kudos Leandro Carísio\n",
        "  mask_tokens_validos = batch['attention_mask'].to(device)\n",
        "  mask = mask_tokens_validos.unsqueeze(-1).expand(w_ij.size())\n",
        "  \n",
        "  wj = torch.max(torch.log(1 + relu(w_ij*mask)), dim=1)[0]\n",
        "\n",
        "  #According to ChatGPT: In PyTorch, the to_sparse() method is used to convert a \n",
        "  #dense tensor into a sparse tensor.  A dense tensor is a tensor that contains \n",
        "  #all elements, including those with zero values. In contrast, a sparse tensor \n",
        "  #is a tensor that only stores the non-zero elements, along with their indices. \n",
        "  #Sparse tensors are useful when working with large tensors with mostly zero \n",
        "  #values, as they can save memory and computational resources.\n",
        "  return wj.to_sparse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "citydImK8bGO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.functional import relu\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def vectorize_to_sparse(text, tokenizer=tokenizer, model=model, remove_special_tokens=False):\n",
        "  # Kudos to Marcos Piau\n",
        "  with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):\n",
        "    tokenized_text = tokenizer(text, max_length=max_length, truncation=True, \n",
        "                              return_tensors='pt', \n",
        "                              return_special_tokens_mask=True).to(device)\n",
        "    \n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      output = model(tokenized_text['input_ids'].to(device), \n",
        "                     attention_mask=tokenized_text['attention_mask'].to(device))\n",
        "\n",
        "    w_ij = output.logits[0,:]\n",
        "\n",
        "    mask_tokens_validos = 1 - tokenized_text['special_tokens_mask'].to(device)\n",
        "\n",
        "    #Kudos Leandro Carísio\n",
        "    #1) Removes all dimenstions of size 1 (here, it's the first one)\n",
        "    #2) Adds a 1 dimension to the end (equivalent to transpose, I think)\n",
        "    #3) Replicates (expands) up to the vocabulary size -> 5 X 30k \n",
        "    if remove_special_tokens:\n",
        "      mask = mask_tokens_validos.squeeze().unsqueeze(-1).expand(w_ij.size())\n",
        "    else:\n",
        "      mask = mask_tokens_validos.squeeze().unsqueeze(-1).expand(w_ij.size())\n",
        "      mask = torch.ones(mask.size()).to(device)\n",
        "\n",
        "    wj = torch.max(torch.log(1 + relu(w_ij*mask)), dim=0)[0]\n",
        "\n",
        "  #According to ChatGPT: In PyTorch, the to_sparse() method is used to convert a \n",
        "  #dense tensor into a sparse tensor.  A dense tensor is a tensor that contains \n",
        "  #all elements, including those with zero values. In contrast, a sparse tensor \n",
        "  #is a tensor that only stores the non-zero elements, along with their indices. \n",
        "  #Sparse tensors are useful when working with large tensors with mostly zero \n",
        "  #values, as they can save memory and computational resources.\n",
        "  return wj.to_sparse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CLVlQ93a-kwX"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "passage_ids = []\n",
        "passage_texts = []\n",
        "id_to_text = dict()\n",
        "\n",
        "with jsonlines.open(f\"{main_dir}/trec-covid/corpus.jsonl\") as reader:\n",
        "  for item in reader:\n",
        "    id = item[\"_id\"]\n",
        "    passage_ids.append(id)\n",
        "    text = item[\"title\"] + ' ' + item[\"text\"]\n",
        "    passage_texts.append(text)\n",
        "    id_to_text[id] = text\n",
        "\n",
        "#Sorts the passages by length\n",
        "passage_indices = sorted(range(len(passage_texts)), \n",
        "                         key=lambda k: len(passage_texts[k]))\n",
        "passage_texts = sorted(passage_texts, key=lambda k: len(k))\n",
        "passage_ids = sorted(passage_ids, key=lambda k: len(id_to_text[k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7tJlFJz2-7mc"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts['input_ids'])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        #return self.tokenized_texts[idx]\n",
        "        return {\n",
        "            'input_ids': self.tokenized_texts['input_ids'][idx],\n",
        "            'attention_mask': self.tokenized_texts['attention_mask'][idx],\n",
        "            'special_tokens_mask': self.tokenized_texts['special_tokens_mask'][idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sZ2YPGiX-8Dd"
      },
      "outputs": [],
      "source": [
        "from transformers import BatchEncoding\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-2GsOPUN_OBw"
      },
      "outputs": [],
      "source": [
        "max_length=256\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBrX5YcM--UH",
        "outputId": "ffe1f8e6-14e9-433a-ed7e-ed91ef63bd2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time spent on tokenization =  30.496001720428467\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "test_passages_tokenized = tokenizer(passage_texts, max_length=max_length, \n",
        "                                   truncation=True, padding=True, \n",
        "                                   return_special_tokens_mask=True)\n",
        "dataset_passages_test = Dataset(test_passages_tokenized)\n",
        "dataloader_passages_test = data.DataLoader(dataset_passages_test, \n",
        "                                          batch_size=batch_size, shuffle=False, \n",
        "                                          collate_fn=collate_fn)\n",
        "end = time.time()\n",
        "print(\"Time spent on tokenization = \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WbyP6mXLb5T",
        "outputId": "b670ad8a-46db-42f3-d6e2-e4d26a356c15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "171332"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset_passages_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiSghN2d_vVD",
        "outputId": "a85506bc-ed4d-4ce3-ce34-c5ec4146af7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Test:   0%|          | 0/5355 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Test: 100%|██████████| 5355/5355 [38:01<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time spent on matrix building =  2291.792766571045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "docs_matrix = None\n",
        "\n",
        "# kudos Marcos Piau\n",
        "with torch.autocast(device_type=str(device), dtype=torch.float16, enabled=True):\n",
        "  for doc_batch in tqdm(dataloader_passages_test, mininterval=0.5, desc='Test', \n",
        "                                          disable=False):\n",
        "    doc_vector = vectorize_to_sparse_batch(doc_batch, model=model, sum_or_max=False)\n",
        "\n",
        "    if docs_matrix is None:\n",
        "      docs_matrix = doc_vector\n",
        "    else:\n",
        "      docs_matrix = torch.cat((docs_matrix, doc_vector), dim=0)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time spent on matrix building = \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k4EMAxpSjwE"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(f\"{main_dir}/trec-covid/docs_matrix.pickle\", \"wb\") as f:\n",
        "  pickle.dump(docs_matrix, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAv4t6SH_GMu"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCABYDthMKQq"
      },
      "source": [
        "### Inverted Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VZ-A93emSVZE"
      },
      "outputs": [],
      "source": [
        "import array\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "index_path = f\"{main_dir}/index.pickle\"\n",
        "\n",
        "def load_or_build_inverted_index():\n",
        "  if os.path.exists(index_path):\n",
        "    with open(index_path, \"rb\") as f:\n",
        "      print(\"Loading index...\")\n",
        "      index = pickle.load(f)\n",
        "  else:\n",
        "    print(\"Building inverted index...\")\n",
        "    inverted_index = dict()\n",
        "    idx = 0\n",
        "\n",
        "    def process(doc_id, idx):\n",
        "      assert passage_ids[idx] == doc_id\n",
        "      doc_vec = docs_matrix[idx]\n",
        "      doc_vec = doc_vec.coalesce()\n",
        "      indices = doc_vec.indices()[0]\n",
        "      values = doc_vec.values()\n",
        "\n",
        "      for token_id, wj in zip(indices, values):\n",
        "        token_id = token_id.item()\n",
        "        wj = wj.item()\n",
        "        inverted_index.setdefault(\n",
        "            token_id, {\"docs\":array.array(\"L\", []), \n",
        "                       \"wj\":array.array(\"f\", [])})[\"docs\"].append(idx)\n",
        "        inverted_index.setdefault(\n",
        "            token_id, {\"docs\":array.array(\"L\", []), \n",
        "                       \"wj\":array.array(\"f\", [])})[\"wj\"].append(wj)\n",
        "\n",
        "    for i in tqdm(range(docs_matrix.shape[0])):\n",
        "      process(passage_ids[i], i)\n",
        "      \n",
        "    index = {\"inverted_index\": inverted_index}\n",
        "\n",
        "    with open(index_path, \"wb\") as f:\n",
        "      pickle.dump(index, f)\n",
        "\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T0XpRosS4Xq",
        "outputId": "21308dca-0b69-4efd-cfd0-139ddf8fdd14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading index...\n",
            "Time spent to build inverted index =  0.8949294090270996\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "index = load_or_build_inverted_index()\n",
        "end = time.time()\n",
        "print(\"Time spent to build inverted index = \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x1sNsrTwkWOT"
      },
      "outputs": [],
      "source": [
        "inverted_index = index[\"inverted_index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "HxOKvo7vkXzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4b5d2e-178a-471c-a5ee-3cffc4a2834e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26050"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "len(inverted_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xssh11HJTyuG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def search_by_query_vector_in_inverted_index(query_vec, k, ids=None):\n",
        "  query_vec = query_vec.coalesce()\n",
        "  doc_scores = defaultdict(int) # int (doc_id) -> int (score)\n",
        "  doc_ids = []\n",
        "  indices = query_vec.indices()[0]\n",
        "  values = query_vec.values()\n",
        "\n",
        "  for token_id, wj in zip(indices, values):\n",
        "    token_id = token_id.item()\n",
        "    wj = wj.item()\n",
        "    \n",
        "    if token_id in inverted_index:\n",
        "      doc_ids = inverted_index[token_id][\"docs\"]\n",
        "      wjs = inverted_index[token_id][\"wj\"]\n",
        "\n",
        "      for idx, doc_wj in zip(doc_ids, wjs):\n",
        "        if ids is not None and passage_ids[idx] in ids:\n",
        "          doc_scores[passage_ids[idx]] += wj * doc_wj\n",
        "        elif ids is None:\n",
        "          doc_scores[passage_ids[idx]] += wj * doc_wj\n",
        "        \n",
        "  doc_scores = dict(sorted(doc_scores.items(), key=lambda x:x[1], \n",
        "                           reverse=True)[:k])\n",
        "          \n",
        "  return doc_scores\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "v15qnGosVKMZ"
      },
      "outputs": [],
      "source": [
        "k = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yZ8Jh6MIdmbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258ede8e-f7d3-42d7-dfce-5857b7ea0bda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "len(query_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l11sMARGTrQn",
        "outputId": "03e6f459-ba7d-431d-859b-8842795e4d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time spent =  65.70718932151794\n",
            "Time spent by query =  1.314143786430359\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "run_splade_inv = defaultdict(list)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i, query in zip(query_ids, query_texts):\n",
        "  query_vec = vectorize_to_sparse(query, tokenizer=tokenizer, model=model, \n",
        "                        sum_or_max=False)\n",
        "  doc_scores = search_by_query_vector_in_inverted_index(query_vec, k)\n",
        "  run_splade_inv[\"query\"] += [i] * k\n",
        "  run_splade_inv[\"docid\"] += doc_scores.keys()\n",
        "  run_splade_inv[\"score\"] += doc_scores.values()\n",
        "  run_splade_inv[\"q0\"] += [\"q0\"] * k\n",
        "  run_splade_inv[\"rank\"] += list(range(1,k+1))\n",
        "  run_splade_inv[\"system\"] += [\"splade\"] * k\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time spent = \", end - start)\n",
        "print(\"Time spent by query = \", (end - start)/len(query_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5suO-f2eTN25",
        "outputId": "35dbc790-89fa-4e44-ebf3-f270b8ed340d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7268509214947312"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_ndcg10(run_splade_inv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBxK8SBMBIf6"
      },
      "source": [
        "## 3rd Pipelie: Pyserini BM25 + Splade Reranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCkNJuHYLHWM"
      },
      "source": [
        "Here, in the SAPLDE step, as we already have a filered list with 1000 documents, we use directly the matrix of documents instead of inverted index.  So we assume BM25 has already used an inverted index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uJN9X4tqhBkL"
      },
      "outputs": [],
      "source": [
        "k=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJiCu66GrzCd",
        "outputId": "3ae6d271-7fc7-4abb-fbff-796ee75f867a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(indices=tensor([[     0,      0,      0,  ..., 171331, 171331, 171331],\n",
              "                       [  1037,   1041,   1996,  ...,  29514,  29566,  29610]]),\n",
              "       values=tensor([0.4171, 0.0068, 0.1628,  ..., 0.1091, 0.2448, 0.0680]),\n",
              "       device='cuda:0', size=(171332, 30522), nnz=27975739,\n",
              "       layout=torch.sparse_coo)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TSmuykc6fBAC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def search_bm25_splade(query, top_k=1000):\n",
        "  #1st step: searches with BM25\n",
        "  bm25_hits = search_with_bm25(query, k=top_k)\n",
        "  \n",
        "  ids = [json.loads(bm25_hits[i].raw)['_id'] for i in range(len(bm25_hits))]\n",
        "  ids = set(ids)\n",
        "\n",
        "  query_embedding = vectorize_to_sparse(query, tokenizer=tokenizer, model=model)\n",
        "  \n",
        "  doc_scores = search_by_query_vector_in_inverted_index(query_embedding, k, ids=ids)\n",
        "  \n",
        "  return doc_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LxEWqFvmMViV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6eed97-12fc-46d5-e582-5532960fae40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time spent =  26.299543142318726\n",
            "Time spent by query =  0.5259908628463745\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "run_splade_matrix = defaultdict(list)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i, query in zip(query_ids, query_texts):\n",
        "  doc_scores = search_bm25_splade(query, k)\n",
        "  n = len(doc_scores)\n",
        "  run_splade_matrix[\"query\"] += [i] * n\n",
        "  run_splade_matrix[\"docid\"] += doc_scores.keys()\n",
        "  run_splade_matrix[\"score\"] += doc_scores.values()\n",
        "  run_splade_matrix[\"q0\"] += [\"q0\"] * n\n",
        "  run_splade_matrix[\"rank\"] += list(range(1,n+1))\n",
        "  run_splade_matrix[\"system\"] += [\"bm25_splade\"] * n\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time spent = \", end - start)\n",
        "print(\"Time spent by query = \", (end - start)/len(query_ids))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(run_splade_matrix[\"query\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYaxiEoiD0uP",
        "outputId": "8be6356f-f887-45ed-cae8-015d8edad4c0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(run_splade_matrix[\"docid\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5E1Uz0QD5tq",
        "outputId": "4db7488e-94dd-46ca-8756-68afb8e9398b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49946"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(run_splade_matrix[\"score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUkg7QyxFCBe",
        "outputId": "7db9381c-fa6e-4765-a07a-a3ebffb4e2dd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49946"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "t_SRs6jkhFmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d9a2ab-d9d9-49fa-b1d6-12135503e0f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7419530766303556"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "eval_ndcg10(run_splade_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Hfzi7hH2pkKY",
        "DXnckUxf4-wL",
        "AdoSaFnH5P7p"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}