{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3drPVvJ-orF1"
      },
      "source": [
        "# Efficiency vs quality tradeoffs\n",
        "\n",
        "Author: Monique Monteiro - moniquelouise@gmail.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ3_Poc4ouzM",
        "outputId": "33b6220b-7b1b-4191-ba3d-7e3255cb9516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV0aDFt4o3iW"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/gdrive/MyDrive/Unicamp-aula-10\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EYi0xNgpJHf"
      },
      "source": [
        "## Libraries installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKeAssOh8LYB",
        "outputId": "3b922fc5-ea2c-4811-ad86-b48947630ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=e395c7562144b17daeb1f64c54ee50a00f8261d9c96eaa0a6c9a3120a65d1222\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvF-TJxA-gMe",
        "outputId": "c1037c0b-c1ec-429a-beac-c2b6b778f5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRh0qAhsKEeW"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install pyserini -q\n",
        "!pip install faiss-cpu -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDzq5gqrVzqv"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate -q\n",
        "!pip install trectools -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkDbAShW0rMm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfzi7hH2pkKY"
      },
      "source": [
        "## Passages download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDUGtSRSpKSs",
        "outputId": "6e09e959-fad7-47d4-fb8a-4332706e7f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-06 21:33:47--  https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.68.44, 18.155.68.121, 18.155.68.38, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.68.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/e9e97686e3138eaff989f67c04cd32e8f8f4c0d4857187e3f180275b23e24e85?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27corpus.jsonl.gz%3B+filename%3D%22corpus.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668029&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvZTllOTc2ODZlMzEzOGVhZmY5ODlmNjdjMDRjZDMyZThmOGY0YzBkNDg1NzE4N2UzZjE4MDI3NWIyM2UyNGU4NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwMjl9fX1dfQ__&Signature=SndAv-KsuG-ohRWzB3nzcv7gwcZRZUtwZ078nBzTXE3PvtMmgbSjt6fmJfuWMObwm2yWfmAtv1wLW7zls0EFVlD5hpDqnHRdrl3SiKRvIf53WJbD9auvSfSlhFRTpaLdX3ODK9nbmSiJRExLYo1o3UtvzzbzOqt1zI7UsrMCs%7EvvbCTn03%7ETREmrfUN1ulLe5xXRU%7EefSiWW1gxMlh4xVAbSQLGwkjz3KLGCTMXU4iFDryqviIt3mbfGUxVraAjQoOgerTI9N1O6OpS7q2SkTWadYWu0YvNTtrkaKqp-SoFE9fB-lz1Y3wo1R72WmUh46JDHGgiguh762PxV71sPTw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-06 21:33:48--  https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/e9e97686e3138eaff989f67c04cd32e8f8f4c0d4857187e3f180275b23e24e85?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27corpus.jsonl.gz%3B+filename%3D%22corpus.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668029&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvZTllOTc2ODZlMzEzOGVhZmY5ODlmNjdjMDRjZDMyZThmOGY0YzBkNDg1NzE4N2UzZjE4MDI3NWIyM2UyNGU4NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwMjl9fX1dfQ__&Signature=SndAv-KsuG-ohRWzB3nzcv7gwcZRZUtwZ078nBzTXE3PvtMmgbSjt6fmJfuWMObwm2yWfmAtv1wLW7zls0EFVlD5hpDqnHRdrl3SiKRvIf53WJbD9auvSfSlhFRTpaLdX3ODK9nbmSiJRExLYo1o3UtvzzbzOqt1zI7UsrMCs%7EvvbCTn03%7ETREmrfUN1ulLe5xXRU%7EefSiWW1gxMlh4xVAbSQLGwkjz3KLGCTMXU4iFDryqviIt3mbfGUxVraAjQoOgerTI9N1O6OpS7q2SkTWadYWu0YvNTtrkaKqp-SoFE9fB-lz1Y3wo1R72WmUh46JDHGgiguh762PxV71sPTw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.98, 18.155.68.94, 18.155.68.73, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73452199 (70M) [application/gzip]\n",
            "Saving to: ‘corpus.jsonl.gz’\n",
            "\n",
            "corpus.jsonl.gz     100%[===================>]  70.05M   304MB/s    in 0.2s    \n",
            "\n",
            "2023-05-06 21:33:48 (304 MB/s) - ‘corpus.jsonl.gz’ saved [73452199/73452199]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/corpus.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m8ror6dpokw"
      },
      "outputs": [],
      "source": [
        "!mv corpus.jsonl.gz {main_dir}/trec-covid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVXGvmSz4i8R",
        "outputId": "c809ba02-5f17-462d-abfb-d6ae8b4c4ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gzip: /content/gdrive/MyDrive/Unicamp-aula-10/trec-covid/corpus.jsonl already exists; do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "!gunzip {main_dir}/trec-covid/corpus.jsonl.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXnckUxf4-wL"
      },
      "source": [
        "## Queries vs passages download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqCxXLfV4_gE",
        "outputId": "30e23791-b70f-4148-b75d-e0ad2001b3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-06 21:34:54--  https://huggingface.co/datasets/BeIR/trec-covid-qrels/raw/main/test.tsv\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.68.44, 18.155.68.116, 18.155.68.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.68.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 980831 (958K) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>] 957.84K  1.09MB/s    in 0.9s    \n",
            "\n",
            "2023-05-06 21:34:55 (1.09 MB/s) - ‘test.tsv’ saved [980831/980831]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/BeIR/trec-covid-qrels/raw/main/test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO-OYpej5ElH"
      },
      "outputs": [],
      "source": [
        "!mv test.tsv {main_dir}/trec-covid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdoSaFnH5P7p"
      },
      "source": [
        "## Queries download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP5BWbgv5Gz3",
        "outputId": "39a631c4-9373-4ff3-8c94-fffe2fa2bd4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-06 21:34:56--  https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/queries.jsonl.gz\n",
            "Resolving huggingface.co (huggingface.co)... 18.155.68.44, 18.155.68.116, 18.155.68.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.155.68.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/9eadcc2cdf140addc9dae83648bb2c6611f5e4b66eaed7475fa5a0ca48eda371?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27queries.jsonl.gz%3B+filename%3D%22queries.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668096&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvOWVhZGNjMmNkZjE0MGFkZGM5ZGFlODM2NDhiYjJjNjYxMWY1ZTRiNjZlYWVkNzQ3NWZhNWEwY2E0OGVkYTM3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwOTZ9fX1dfQ__&Signature=m2lx5g8Ip6rMW3I9jGfzPPtZINejDEj2f8XwtGpuukp5CKEWuXrd163mK3%7EVR1n3hhzfiNLuBToxx0lqarAtV09mxvsY36c%7Et3PsKmT4lz3raQBNoFP7OfmgAoXLR%7EMi-Vx7woOO46rsugcf63XPI7kqqv9SppFKcA35VI-v0tMxwcIMGYvCjwj3wwpWDEoW5QEQjLxyJNrMzG0Q%7EivdvzhZ4VHyygj8NkdntMBijB3ZIsbG-OrwJG6PaqiZ2mD1S5V7aERr7HL927UZJLUwoHVL5Gx1MCW3M8IslD1BekktsVqj7qM9NS8AEk382pV7Y4PUvQldnxQYjfp1O9lAgw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-06 21:34:56--  https://cdn-lfs.huggingface.co/repos/a8/10/a810e88b0e7b233be82b89c1fa6ec2d75efc6d55784c2ada9dcac8434a634f3a/9eadcc2cdf140addc9dae83648bb2c6611f5e4b66eaed7475fa5a0ca48eda371?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27queries.jsonl.gz%3B+filename%3D%22queries.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1683668096&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E4LzEwL2E4MTBlODhiMGU3YjIzM2JlODJiODljMWZhNmVjMmQ3NWVmYzZkNTU3ODRjMmFkYTlkY2FjODQzNGE2MzRmM2EvOWVhZGNjMmNkZjE0MGFkZGM5ZGFlODM2NDhiYjJjNjYxMWY1ZTRiNjZlYWVkNzQ3NWZhNWEwY2E0OGVkYTM3MT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODM2NjgwOTZ9fX1dfQ__&Signature=m2lx5g8Ip6rMW3I9jGfzPPtZINejDEj2f8XwtGpuukp5CKEWuXrd163mK3%7EVR1n3hhzfiNLuBToxx0lqarAtV09mxvsY36c%7Et3PsKmT4lz3raQBNoFP7OfmgAoXLR%7EMi-Vx7woOO46rsugcf63XPI7kqqv9SppFKcA35VI-v0tMxwcIMGYvCjwj3wwpWDEoW5QEQjLxyJNrMzG0Q%7EivdvzhZ4VHyygj8NkdntMBijB3ZIsbG-OrwJG6PaqiZ2mD1S5V7aERr7HL927UZJLUwoHVL5Gx1MCW3M8IslD1BekktsVqj7qM9NS8AEk382pV7Y4PUvQldnxQYjfp1O9lAgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.155.68.128, 18.155.68.98, 18.155.68.94, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.155.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4702 (4.6K) [application/gzip]\n",
            "Saving to: ‘queries.jsonl.gz’\n",
            "\n",
            "queries.jsonl.gz    100%[===================>]   4.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-06 21:34:56 (191 MB/s) - ‘queries.jsonl.gz’ saved [4702/4702]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/BeIR/trec-covid/resolve/main/queries.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiRancl45JXE"
      },
      "outputs": [],
      "source": [
        "!mv queries.jsonl.gz {main_dir}/trec-covid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kCDz7Xn5LO5",
        "outputId": "89d28777-939a-40c5-c48f-0ab17ddc4b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gzip: /content/gdrive/MyDrive/Unicamp-aula-10/trec-covid/queries.jsonl already exists; do you wish to overwrite (y or n)? ^C\n"
          ]
        }
      ],
      "source": [
        "!gunzip {main_dir}/trec-covid/queries.jsonl.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqHnZbI9eb6"
      },
      "source": [
        "## Pipeline 1: Pyserini BM25 + Reranking with cross-encoder/ms-marco-MiniLM-L-6-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51HR2Sid9skM"
      },
      "source": [
        "### 1st step: Pre-indexing TREC-COVID corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZWmH9OB90OS"
      },
      "source": [
        "Here, we need to measure indexing cost in dollars per hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--xMWixxHlFd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psDPW3t-HmUL",
        "outputId": "a3515962-bdb6-459d-f59d-4d2218c1d95a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqL5bZuZ5axg",
        "outputId": "b4ba00d7-07b2-4a16-9974-178c66e96c20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/cross-encoder_ms-marco-MiniLM-L-6-v2. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /root/.cache/torch/sentence_transformers/cross-encoder_ms-marco-MiniLM-L-6-v2 were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
        "\n",
        "embedder = SentenceTransformer(model_name, device=device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxOt8PHY-ljT"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "passage_ids = []\n",
        "passage_texts = []\n",
        "id_to_passage = dict()\n",
        "\n",
        "#Maps each id to its position in the corpus\n",
        "id_to_index = dict()\n",
        "\n",
        "with jsonlines.open(f\"{main_dir}/trec-covid/corpus.jsonl\") as reader:\n",
        "  for item in reader:\n",
        "    id = item[\"_id\"]\n",
        "    id_to_index[id] = len(passage_ids)\n",
        "    passage_ids.append(id)\n",
        "    text = item[\"title\"] + ' ' + item[\"text\"]\n",
        "    passage_texts.append(text)\n",
        "    id_to_passage[id] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKwY_vUv-O26",
        "outputId": "3c5736b6-13e1-439b-b368-e36dbab4ac32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 29s, sys: 10.1 s, total: 6min 39s\n",
            "Wall time: 3min\n"
          ]
        }
      ],
      "source": [
        "%time corpus_embeddings = embedder.encode(passage_texts, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJpefhdUGRu5",
        "outputId": "6bbbe292-2569-44bf-c08b-751b499756dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting gpu_usage.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile gpu_usage.sh\n",
        "#! /bin/bash\n",
        "#comment: run for 10 seconds, change it as per your use\n",
        "end=$((SECONDS+3600))\n",
        "\n",
        "while [ $SECONDS -lt $end ]; do\n",
        "    nvidia-smi --format=csv --query-gpu=power.draw,utilization.gpu,memory.used,memory.free,fan.speed,temperature.gpu >> gpu.log\n",
        "    #comment: or use below command and comment above using #\n",
        "    #nvidia-smi dmon -i 0 -s mu -d 1 -o TD >> gpu.log\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBHsfxtfGSiJ"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "bash gpu_usage.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ4_tEjKGYW-"
      },
      "outputs": [],
      "source": [
        "corpus_embeddings = embedder.encode(passage_texts, convert_to_tensor=True) #2m41s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Bqh9Y8GdGu",
        "outputId": "20cd7019-c4a5-4fa9-f9a7-798c7aab631f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([171332, 384])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0zz1_t2LzHN"
      },
      "source": [
        "### Pipeline - Searching with Pyserini BM25 + Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY7H4KexI3t5"
      },
      "outputs": [],
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wikXMInXL8Eg"
      },
      "outputs": [],
      "source": [
        "def search_with_bm25(query,k = 1000, index_name='beir-v1.0.0-trec-covid.flat'):\n",
        "  if index_name == 'beir-v1.0.0-trec-covid.flat':   \n",
        "    searcher = LuceneSearcher.from_prebuilt_index(index_name)\n",
        "  else:\n",
        "    searcher = LuceneSearcher(index_name)\n",
        "  hits = searcher.search(query, k)\n",
        "  return hits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQpIOs4pMR9P"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "query_ids = []\n",
        "query_texts = []\n",
        "\n",
        "with jsonlines.open(f\"{main_dir}/trec-covid/queries.jsonl\") as reader:\n",
        "  for item in reader:\n",
        "    id = item[\"_id\"]\n",
        "    query_ids.append(id)\n",
        "    text = item[\"text\"]\n",
        "    query_texts.append(text)\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onbNVfB4Mnh1"
      },
      "source": [
        "Generates the first request to download pre-indexed corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8lXwOfTOUYG"
      },
      "outputs": [],
      "source": [
        "def search(query, top_k=1000):\n",
        "  #1st step: searches with BM25\n",
        "  bm25_hits = search_with_bm25(query, k=top_k)\n",
        "\n",
        "  ids = [json.loads(bm25_hits[i].raw)['_id'] for i in range(len(bm25_hits))]\n",
        "  indices = [id_to_index[id] for id in ids]\n",
        "\n",
        "  #Finds corpus embeddings\n",
        "  passages_embeddings = [corpus_embeddings[i] for i in indices]\n",
        "\n",
        "  query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "  hits = util.semantic_search(query_embedding, passages_embeddings, top_k=top_k)\n",
        "  hits = hits[0]      #Get the hits for the first query\n",
        "  results = dict()\n",
        "  for hit in hits:\n",
        "    assert hit['corpus_id'] < 1000\n",
        "    results[ids[hit['corpus_id']]] = hit['score']\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzpeJBOvSDeL"
      },
      "outputs": [],
      "source": [
        "%time search(query_texts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw6XUk1OVLl4"
      },
      "source": [
        "### Measuring NDCG@10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzr4rJ3kWSBW"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "qrel = pd.read_csv(f\"{main_dir}/trec-covid/test.tsv\", sep=\"\\t\", header=None, \n",
        "                   skiprows=1, names=[\"query\", \"docid\", \"rel\"])\n",
        "qrel[\"q0\"] = \"q0\"\n",
        "qrel = qrel.to_dict(orient=\"list\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioAKAyTYfg_i",
        "outputId": "2f43b89e-47a9-496a-dad3-53fecf8227cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66336"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(qrel[\"q0\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbC6zjDQVKP8"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def search_all(top_k=1000):\n",
        "  run = defaultdict(list)\n",
        "\n",
        "  for id, query in tqdm(zip(query_ids, query_texts)):\n",
        "    results = search(query)\n",
        "    run[\"query\"] += [id] * top_k\n",
        "    run[\"docid\"] += results.keys()\n",
        "    run[\"score\"] += results.values()\n",
        "    run[\"q0\"] += [\"q0\"] * top_k\n",
        "    run[\"rank\"] += list(range(1,top_k+1))\n",
        "    run[\"system\"] += [model_name] * top_k\n",
        "\n",
        "  return run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7hFyxb2YExa",
        "outputId": "a724ab1e-2c62-408f-f5a5-741a95c584d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:07,  6.52it/s]\n"
          ]
        }
      ],
      "source": [
        "run = search_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKH8-OgNYtRi",
        "outputId": "8be9bf96-267e-4967-e517-d317501b1cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(run[\"system\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANkR4QzgYF-a"
      },
      "outputs": [],
      "source": [
        "\n",
        "from evaluate import load\n",
        "\n",
        "def eval_ndcg10(run):\n",
        "  trec_eval = load(\"trec_eval\")\n",
        "  results = trec_eval.compute(predictions=[run], references=[qrel])\n",
        "  return results['NDCG@10'] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_eK4xuIYiYg"
      },
      "source": [
        "Evaluates BM25 NDCG@10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buMr_hDlZE8s"
      },
      "outputs": [],
      "source": [
        "def search_all_bm25(top_k=1000):\n",
        "  run = defaultdict(list)\n",
        "\n",
        "  for id, query in tqdm(zip(query_ids, query_texts)):\n",
        "    bm25_hits = search_with_bm25(query)\n",
        "    ids = [json.loads(bm25_hits[i].raw)['_id'] for i in range(len(bm25_hits))]\n",
        "    run[\"query\"] += [id] * top_k\n",
        "    run[\"docid\"] += ids\n",
        "    run[\"score\"] += [1] * top_k\n",
        "    run[\"q0\"] += [\"q0\"] * top_k\n",
        "    run[\"rank\"] += list(range(1,top_k+1))\n",
        "    run[\"system\"] += [model_name] * top_k\n",
        "\n",
        "  return run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeDFUKYsZVi4",
        "outputId": "7c97a55b-587d-43a4-ffc0-26db1421e50d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "50it [00:05,  9.08it/s]\n"
          ]
        }
      ],
      "source": [
        "run_bm25 = search_all_bm25()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHJLDlnbaOGR",
        "outputId": "b78fd958-d2c7-4ade-face-64c407c4f947"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5946917010118077"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_ndcg10(run_bm25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PhuNE_dYUhE",
        "outputId": "0ec2a170-b456-4d2e-d4a0-e42d4ea842b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.2754485142056899"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_ndcg10(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyX1m4eaeyi_"
      },
      "source": [
        "Conclusion: horrible quality for off-the-shelf usage.  It requiries fine tuning to be used on TREC-COVID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYQXlV9Hzx_J"
      },
      "source": [
        "## Pipeline 2: SPLADE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDokC1fwvGg3"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, DistilBertTokenizer, DistilBertForMaskedLM\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnuCdfuR0Q1i"
      },
      "outputs": [],
      "source": [
        "model_name_1 = 'naver/splade_v2_distil' \n",
        "model_name_2 = 'naver/splade-cocondenser-selfdistil'\n",
        "model_name_3 = 'naver/splade-cocondenser-ensembledistil' \n",
        "\n",
        "#tokenizer_1 = DistilBertTokenizer.from_pretrained(model_name_1)\n",
        "#model_1 = DistilBertForMaskedLM.from_pretrained(model_name_1)\n",
        "\n",
        "#tokenizer_2 = BertTokenizer.from_pretrained(model_name_2)\n",
        "#model_2 = BertForMaskedLM.from_pretrained(model_name_2)\n",
        "\n",
        "tokenizer_3 = AutoTokenizer.from_pretrained(model_name_3)\n",
        "model_3 = AutoModelForMaskedLM.from_pretrained(model_name_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLF0gopU1wFh"
      },
      "outputs": [],
      "source": [
        "model = model_3\n",
        "tokenizer = tokenizer_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uRoAXTB-KOc"
      },
      "outputs": [],
      "source": [
        "def vectorize_to_sparse_batch(batch, model=model, remove_special_tokens=True):\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    output = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "\n",
        "  w_ij = output.logits.to(device)\n",
        "\n",
        "  #Kudos Leandro Carísio\n",
        "  mask_tokens_validos = batch['attention_mask'].to(device)\n",
        "  mask = mask_tokens_validos.unsqueeze(-1).expand(w_ij.size())\n",
        "  \n",
        "  wj = torch.max(torch.log(1 + relu(w_ij*mask)), dim=1)[0]\n",
        "\n",
        "  #According to ChatGPT: In PyTorch, the to_sparse() method is used to convert a \n",
        "  #dense tensor into a sparse tensor.  A dense tensor is a tensor that contains \n",
        "  #all elements, including those with zero values. In contrast, a sparse tensor \n",
        "  #is a tensor that only stores the non-zero elements, along with their indices. \n",
        "  #Sparse tensors are useful when working with large tensors with mostly zero \n",
        "  #values, as they can save memory and computational resources.\n",
        "  return wj.to_sparse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "citydImK8bGO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.functional import relu\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def vectorize_to_sparse(text, tokenizer=tokenizer, model=model, remove_special_tokens=False):\n",
        "  # Kudos to Marcos Piau\n",
        "  with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True):\n",
        "    tokenized_text = tokenizer(text, max_length=max_length, truncation=True, \n",
        "                              return_tensors='pt', \n",
        "                              return_special_tokens_mask=True).to(device)\n",
        "    \n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      output = model(tokenized_text['input_ids'].to(device), \n",
        "                     attention_mask=tokenized_text['attention_mask'].to(device))\n",
        "\n",
        "    w_ij = output.logits[0,:]\n",
        "\n",
        "    mask_tokens_validos = 1 - tokenized_text['special_tokens_mask'].to(device)\n",
        "\n",
        "    #Kudos Leandro Carísio\n",
        "    #1) Removes all dimenstions of size 1 (here, it's the first one)\n",
        "    #2) Adds a 1 dimension to the end (equivalent to transpose, I think)\n",
        "    #3) Replicates (expands) up to the vocabulary size -> 5 X 30k \n",
        "    if remove_special_tokens:\n",
        "      mask = mask_tokens_validos.squeeze().unsqueeze(-1).expand(w_ij.size())\n",
        "    else:\n",
        "      mask = mask_tokens_validos.squeeze().unsqueeze(-1).expand(w_ij.size())\n",
        "      mask = torch.ones(mask.size()).to(device)\n",
        "\n",
        "    wj = torch.max(torch.log(1 + relu(w_ij*mask)), dim=0)[0]\n",
        "\n",
        "  #According to ChatGPT: In PyTorch, the to_sparse() method is used to convert a \n",
        "  #dense tensor into a sparse tensor.  A dense tensor is a tensor that contains \n",
        "  #all elements, including those with zero values. In contrast, a sparse tensor \n",
        "  #is a tensor that only stores the non-zero elements, along with their indices. \n",
        "  #Sparse tensors are useful when working with large tensors with mostly zero \n",
        "  #values, as they can save memory and computational resources.\n",
        "  return wj.to_sparse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLVlQ93a-kwX"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "passage_ids = []\n",
        "passage_texts = []\n",
        "id_to_text = dict()\n",
        "\n",
        "with jsonlines.open(f\"{main_dir}/trec-covid/corpus.jsonl\") as reader:\n",
        "  for item in reader:\n",
        "    id = item[\"_id\"]\n",
        "    passage_ids.append(id)\n",
        "    text = item[\"title\"] + ' ' + item[\"text\"]\n",
        "    passage_texts.append(text)\n",
        "    id_to_text[id] = text\n",
        "\n",
        "#Sorts the passages by length\n",
        "passage_indices = sorted(range(len(passage_texts)), \n",
        "                         key=lambda k: len(passage_texts[k]))\n",
        "passage_texts = sorted(passage_texts, key=lambda k: len(k))\n",
        "passage_ids = sorted(passage_ids, key=lambda k: len(id_to_text[k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tJlFJz2-7mc"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts['input_ids'])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        #return self.tokenized_texts[idx]\n",
        "        return {\n",
        "            'input_ids': self.tokenized_texts['input_ids'][idx],\n",
        "            'attention_mask': self.tokenized_texts['attention_mask'][idx],\n",
        "            'special_tokens_mask': self.tokenized_texts['special_tokens_mask'][idx]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ2YPGiX-8Dd"
      },
      "outputs": [],
      "source": [
        "from transformers import BatchEncoding\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2GsOPUN_OBw"
      },
      "outputs": [],
      "source": [
        "max_length=256\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBrX5YcM--UH",
        "outputId": "ffe1f8e6-14e9-433a-ed7e-ed91ef63bd2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time spent on tokenization =  30.496001720428467\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "test_passages_tokenized = tokenizer(passage_texts, max_length=max_length, \n",
        "                                   truncation=True, padding=True, \n",
        "                                   return_special_tokens_mask=True)\n",
        "dataset_passages_test = Dataset(test_passages_tokenized)\n",
        "dataloader_passages_test = data.DataLoader(dataset_passages_test, \n",
        "                                          batch_size=batch_size, shuffle=False, \n",
        "                                          collate_fn=collate_fn)\n",
        "end = time.time()\n",
        "print(\"Time spent on tokenization = \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WbyP6mXLb5T",
        "outputId": "b670ad8a-46db-42f3-d6e2-e4d26a356c15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "171332"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset_passages_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiSghN2d_vVD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def build_docs_matrix(dataloader_passages_test):\n",
        "  start = time.time()\n",
        "\n",
        "  docs_matrix = None\n",
        "\n",
        "  # kudos Marcos Piau\n",
        "  with torch.autocast(device_type=str(device), dtype=torch.float16, enabled=True):\n",
        "    for doc_batch in tqdm(dataloader_passages_test, mininterval=0.5, desc='Test', \n",
        "                                            disable=False):\n",
        "      doc_vector = vectorize_to_sparse_batch(doc_batch, model=model)\n",
        "\n",
        "      if docs_matrix is None:\n",
        "        docs_matrix = doc_vector\n",
        "      else:\n",
        "        docs_matrix = torch.cat((docs_matrix, doc_vector), dim=0)\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Time spent on matrix building = \", end - start)\n",
        "  return docs_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_matrix = build_docs_matrix(dataloader_passages_test)"
      ],
      "metadata": {
        "id": "jebSYhb0uJEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAv4t6SH_GMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0a1529-0d40-4238-d193-945d5cc872b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  8 23:36:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    48W / 400W |   2301MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCABYDthMKQq"
      },
      "source": [
        "### Inverted Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ-A93emSVZE"
      },
      "outputs": [],
      "source": [
        "import array\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "def load_or_build_inverted_index(index_path = f\"{main_dir}/index.pickle\", docs_matrix=docs_matrix):\n",
        "  if os.path.exists(index_path):\n",
        "    with open(index_path, \"rb\") as f:\n",
        "      print(\"Loading index...\")\n",
        "      index = pickle.load(f)\n",
        "  else:\n",
        "    print(\"Building inverted index...\")\n",
        "    inverted_index = dict()\n",
        "    idx = 0\n",
        "\n",
        "    def process(doc_id, idx):\n",
        "      assert passage_ids[idx] == doc_id\n",
        "      doc_vec = docs_matrix[idx]\n",
        "      doc_vec = doc_vec.coalesce()\n",
        "      indices = doc_vec.indices()[0]\n",
        "      values = doc_vec.values()\n",
        "\n",
        "      for token_id, wj in zip(indices, values):\n",
        "        token_id = token_id.item()\n",
        "        wj = wj.item()\n",
        "        inverted_index.setdefault(\n",
        "            token_id, {\"docs\":array.array(\"L\", []), \n",
        "                       \"wj\":array.array(\"f\", [])})[\"docs\"].append(idx)\n",
        "        inverted_index.setdefault(\n",
        "            token_id, {\"docs\":array.array(\"L\", []), \n",
        "                       \"wj\":array.array(\"f\", [])})[\"wj\"].append(wj)\n",
        "\n",
        "    for i in tqdm(range(docs_matrix.shape[0])):\n",
        "      process(passage_ids[i], i)\n",
        "      \n",
        "    index = {\"inverted_index\": inverted_index}\n",
        "\n",
        "    with open(index_path, \"wb\") as f:\n",
        "      pickle.dump(index, f)\n",
        "\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T0XpRosS4Xq",
        "outputId": "21308dca-0b69-4efd-cfd0-139ddf8fdd14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading index...\n",
            "Time spent to build inverted index =  0.8949294090270996\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "index = load_or_build_inverted_index()\n",
        "end = time.time()\n",
        "print(\"Time spent to build inverted index = \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1sNsrTwkWOT"
      },
      "outputs": [],
      "source": [
        "inverted_index = index[\"inverted_index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxOKvo7vkXzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4b5d2e-178a-471c-a5ee-3cffc4a2834e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26050"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "len(inverted_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xssh11HJTyuG"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def search_by_query_vector_in_inverted_index(query_vec, k, ids=None):\n",
        "  query_vec = query_vec.coalesce()\n",
        "  doc_scores = defaultdict(int) # int (doc_id) -> int (score)\n",
        "  doc_ids = []\n",
        "  indices = query_vec.indices()[0]\n",
        "  values = query_vec.values()\n",
        "\n",
        "  for token_id, wj in zip(indices, values):\n",
        "    token_id = token_id.item()\n",
        "    wj = wj.item()\n",
        "    \n",
        "    if token_id in inverted_index:\n",
        "      doc_ids = inverted_index[token_id][\"docs\"]\n",
        "      wjs = inverted_index[token_id][\"wj\"]\n",
        "\n",
        "      for idx, doc_wj in zip(doc_ids, wjs):\n",
        "        if ids is not None and passage_ids[idx] in ids:\n",
        "          doc_scores[passage_ids[idx]] += wj * doc_wj\n",
        "        elif ids is None:\n",
        "          doc_scores[passage_ids[idx]] += wj * doc_wj\n",
        "        \n",
        "  doc_scores = dict(sorted(doc_scores.items(), key=lambda x:x[1], \n",
        "                           reverse=True)[:k])\n",
        "          \n",
        "  return doc_scores\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v15qnGosVKMZ"
      },
      "outputs": [],
      "source": [
        "k = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ8Jh6MIdmbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258ede8e-f7d3-42d7-dfce-5857b7ea0bda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "len(query_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l11sMARGTrQn",
        "outputId": "03e6f459-ba7d-431d-859b-8842795e4d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time spent =  65.70718932151794\n",
            "Time spent by query =  1.314143786430359\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "run_splade_inv = defaultdict(list)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i, query in zip(query_ids, query_texts):\n",
        "  query_vec = vectorize_to_sparse(query, tokenizer=tokenizer, model=model, \n",
        "                        sum_or_max=False)\n",
        "  doc_scores = search_by_query_vector_in_inverted_index(query_vec, k)\n",
        "  run_splade_inv[\"query\"] += [i] * k\n",
        "  run_splade_inv[\"docid\"] += doc_scores.keys()\n",
        "  run_splade_inv[\"score\"] += doc_scores.values()\n",
        "  run_splade_inv[\"q0\"] += [\"q0\"] * k\n",
        "  run_splade_inv[\"rank\"] += list(range(1,k+1))\n",
        "  run_splade_inv[\"system\"] += [\"splade\"] * k\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time spent = \", end - start)\n",
        "print(\"Time spent by query = \", (end - start)/len(query_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5suO-f2eTN25",
        "outputId": "35dbc790-89fa-4e44-ebf3-f270b8ed340d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7268509214947312"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_ndcg10(run_splade_inv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBxK8SBMBIf6"
      },
      "source": [
        "## 3rd Pipeline: Pyserini BM25 + SPLADE Reranking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCkNJuHYLHWM"
      },
      "source": [
        "Here, in the SAPLDE step, as we already have a filered list with 1000 documents, we use directly the matrix of documents instead of inverted index.  So we assume BM25 has already used an inverted index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJN9X4tqhBkL"
      },
      "outputs": [],
      "source": [
        "k=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJiCu66GrzCd",
        "outputId": "3ae6d271-7fc7-4abb-fbff-796ee75f867a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(indices=tensor([[     0,      0,      0,  ..., 171331, 171331, 171331],\n",
              "                       [  1037,   1041,   1996,  ...,  29514,  29566,  29610]]),\n",
              "       values=tensor([0.4171, 0.0068, 0.1628,  ..., 0.1091, 0.2448, 0.0680]),\n",
              "       device='cuda:0', size=(171332, 30522), nnz=27975739,\n",
              "       layout=torch.sparse_coo)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSmuykc6fBAC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def search_bm25_splade(query, top_k=1000, \n",
        "                       index_name='beir-v1.0.0-trec-covid.flat'):\n",
        "  #1st step: searches with BM25\n",
        "  bm25_hits = search_with_bm25(query, k=top_k, index_name=index_name)\n",
        " \n",
        "  if index_name == 'beir-v1.0.0-trec-covid.flat':\n",
        "    ids = [json.loads(bm25_hits[i].raw)['_id'] for i in range(len(bm25_hits))]\n",
        "  else:\n",
        "    ids = [json.loads(bm25_hits[i].raw)['id'] for i in range(len(bm25_hits))]\n",
        "  ids = set(ids)\n",
        "\n",
        "  query_embedding = vectorize_to_sparse(query, tokenizer=tokenizer, model=model)\n",
        "  \n",
        "  doc_scores = search_by_query_vector_in_inverted_index(query_embedding, k, \n",
        "                                                        ids=ids)\n",
        "  \n",
        "  return doc_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxEWqFvmMViV"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "run_bm25_splade = defaultdict(list)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i, query in zip(query_ids, query_texts):\n",
        "  doc_scores = search_bm25_splade(query, k)\n",
        "  n = len(doc_scores)\n",
        "  run_bm25_splade[\"query\"] += [i] * n\n",
        "  run_bm25_splade[\"docid\"] += doc_scores.keys()\n",
        "  run_bm25_splade[\"score\"] += doc_scores.values()\n",
        "  run_bm25_splade[\"q0\"] += [\"q0\"] * n\n",
        "  run_bm25_splade[\"rank\"] += list(range(1,n+1))\n",
        "  run_bm25_splade[\"system\"] += [\"bm25_splade\"] * n\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time spent = \", end - start)\n",
        "print(\"Time spent by query = \", (end - start)/len(query_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_SRs6jkhFmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d9a2ab-d9d9-49fa-b1d6-12135503e0f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7419530766303556"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "eval_ndcg10(run_bm25_splade)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4th Pipeline: Doc2Query + Pyserini BM25 + SPLADE Reranking"
      ],
      "metadata": {
        "id": "JsUFtKYgWQeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODOL CALCULAR O CUSTO DE DE GERAÇÃO DO ÍNDICE!"
      ],
      "metadata": {
        "id": "3HC4ZcNUlIM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_path = '/content/gdrive/MyDrive/Unicamp-aula-6/collections/trec_covid_doc2query-adafactor-bs-32-split-1000-bf16-early-stoping_5_queries_expand_title'"
      ],
      "metadata": {
        "id": "S53iwgF8s4cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_path = '/content/gdrive/MyDrive/Unicamp-aula-6/expanded_trec_covid_5_queries_expand_title'"
      ],
      "metadata": {
        "id": "DtGokc5qk6Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "import os\n",
        "\n",
        "passage_ids = []\n",
        "passage_texts = []\n",
        "id_to_text = dict()\n",
        "\n",
        "for filename in os.listdir(json_path):\n",
        "  if filename.endswith('.json'):  # Check if the file ends with '.json'\n",
        "    with jsonlines.open(json_path + '/' + filename) as reader:\n",
        "      for item in reader:\n",
        "        id = item[\"id\"]\n",
        "        passage_ids.append(id)\n",
        "        text = item[\"contents\"]\n",
        "        passage_texts.append(text)\n",
        "        id_to_text[id] = text\n",
        "\n",
        "#Sorts the passages by length\n",
        "passage_indices = sorted(range(len(passage_texts)), \n",
        "                         key=lambda k: len(passage_texts[k]))\n",
        "passage_texts = sorted(passage_texts, key=lambda k: len(k))\n",
        "passage_ids = sorted(passage_ids, key=lambda k: len(id_to_text[k]))"
      ],
      "metadata": {
        "id": "Sqde4lALs8Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "test_passages_tokenized = tokenizer(passage_texts, max_length=max_length, \n",
        "                                   truncation=True, padding=True, \n",
        "                                   return_special_tokens_mask=True)\n",
        "dataset_passages_test = Dataset(test_passages_tokenized)\n",
        "dataloader_passages_test = data.DataLoader(dataset_passages_test, \n",
        "                                          batch_size=batch_size, shuffle=False, \n",
        "                                          collate_fn=collate_fn)\n",
        "end = time.time()\n",
        "print(\"Time spent on tokenization = \", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyAAArq2tyqO",
        "outputId": "197960dc-74a4-49fd-a1a8-484c95ad697a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time spent on tokenization =  34.331130027770996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset_passages_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XgoHozn5XNx",
        "outputId": "f9cffd78-7344-48bb-b154-ac1b0e8756d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171332"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_matrix = build_docs_matrix(dataloader_passages_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-LHT7suuehO",
        "outputId": "0d52091d-c0eb-46ae-feb9-3c6c417821c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 5355/5355 [37:36<00:00,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time spent on matrix building =  2256.5319912433624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3U35zHd41RC",
        "outputId": "820a2c5c-c2b3-4f13-f575-3a9484b5a940"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([171332, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "index = load_or_build_inverted_index(index_path=f\"{main_dir}/index_doc2_query\", docs_matrix=docs_matrix)\n",
        "end = time.time()\n",
        "print(\"Time spent to build inverted index = \", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrWdouIzoI_t",
        "outputId": "ae06dc87-4efa-4af7-8f2b-34e3d876fee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building inverted index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1196/171332 [00:54<22:55, 123.71it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_matrix[0].coalesce().values().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_LB9Hx0ab-p",
        "outputId": "7eb56a32-fa87-4006-c1b2-513f2a1bfe64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([150])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "def search_by_query_vector_in_docs_matrix(query_vec, ids, k):\n",
        "  start = time.time()\n",
        "  doc_scores = defaultdict(int) # int (doc_id) -> int (score)\n",
        "  \n",
        "  #passages_matrix = None\n",
        "  \n",
        "  #for id in ids:\n",
        "   # index = passage_ids.index(id)\n",
        "    #doc_vec = docs_matrix[index].to_dense()\n",
        "    \n",
        "    #if passages_matrix is None:\n",
        "     #   passages_matrix = doc_vec.unsqueeze(0)\n",
        "    #else:\n",
        "     #   passages_matrix = torch.cat((passages_matrix, doc_vec.to_dense().unsqueeze(0)), dim=0)\n",
        "\n",
        "  #print(passages_matrix.shape)\n",
        "  #passages_matrix = passages_matrix.to_sparse()\n",
        "\n",
        "  #scores = torch.sparse.mm(passages_matrix, query_vec.unsqueeze(-1))   \n",
        "  #indices = scores.indices()[0].cpu().numpy()\n",
        "  #values = scores.values().cpu().numpy()\n",
        "  for id in ids:\n",
        "    index = passage_ids.index(id)\n",
        "    doc_vec = docs_matrix[index]\n",
        "    doc_score = torch.sparse.mm(doc_vec.unsqueeze(-1).transpose(0,1), \n",
        "                                query_vec.unsqueeze(-1))\n",
        "    doc_scores[id] = doc_score.item()\n",
        "  \n",
        "  #doc_scores = {passage_ids[indices[i]]:values[i] for i in range(len(indices))}  \n",
        "  doc_scores = dict(sorted(doc_scores.items(), key=lambda x:x[1], \n",
        "                           reverse=True)[:k])\n",
        "  end = time.time()\n",
        "  print(\"Time spent for query_vec = \", end - start)  \n",
        "  return doc_scores"
      ],
      "metadata": {
        "id": "RuNbo41XVqGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def search_doc2query_bm25_splade(query, index_name, top_k=1000):\n",
        "  #1st step: searches with BM25\n",
        "  bm25_hits = search_with_bm25(query, k=top_k, index_name=index_name)\n",
        " \n",
        "  ids = [json.loads(bm25_hits[i].raw)['id'] for i in range(len(bm25_hits))]\n",
        "\n",
        "  query_embedding = vectorize_to_sparse(query, tokenizer=tokenizer, model=model)\n",
        "  \n",
        "  doc_scores = search_by_query_vector_in_docs_matrix(query_embedding, ids, k)\n",
        "  \n",
        "  return doc_scores"
      ],
      "metadata": {
        "id": "3wqVVlk-YM5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "run_doc2query_bm25_splade = defaultdict(list)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i, query in zip(query_ids, query_texts):\n",
        "  doc_scores = search_doc2query_bm25_splade(query, index_path, k)\n",
        "  n = len(doc_scores)\n",
        "  run_doc2query_bm25_splade[\"query\"] += [i] * n\n",
        "  run_doc2query_bm25_splade[\"docid\"] += doc_scores.keys()\n",
        "  run_doc2query_bm25_splade[\"score\"] += doc_scores.values()\n",
        "  run_doc2query_bm25_splade[\"q0\"] += [\"q0\"] * n\n",
        "  run_doc2query_bm25_splade[\"rank\"] += list(range(1,n+1))\n",
        "  run_doc2query_bm25_splade[\"system\"] += [\"bm25_splade\"] * n\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time spent = \", end - start)\n",
        "print(\"Time spent by query = \", (end - start)/len(query_ids))"
      ],
      "metadata": {
        "id": "wYoUSP9kkeIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "d1d924cc-f4c3-4808-ede1-e31cfa6d8b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time spent for query_vec =  24.56374168395996\n",
            "Time spent for query_vec =  34.32410383224487\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-4ec69d9dc531>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mdoc_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_doc2query_bm25_splade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mrun_doc2query_bm25_splade\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-6c35a2de7963>\u001b[0m in \u001b[0;36msearch_doc2query_bm25_splade\u001b[0;34m(query, index_name, top_k)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_to_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mdoc_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_by_query_vector_in_docs_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-141-0107cd18d38f>\u001b[0m in \u001b[0;36msearch_by_query_vector_in_docs_matrix\u001b[0;34m(query_vec, ids, k)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpassage_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdoc_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     doc_score = torch.sparse.mm(doc_vec.unsqueeze(-1).transpose(0,1), \n\u001b[0m\u001b[1;32m     30\u001b[0m                                 query_vec.unsqueeze(-1))\n\u001b[1;32m     31\u001b[0m     \u001b[0mdoc_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_ndcg10(run_doc2query_bm25_splade)"
      ],
      "metadata": {
        "id": "ch4PRDb_laWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5th Pipeline: Doc2Query + Splade Reranking"
      ],
      "metadata": {
        "id": "MVM9BCfYgFyR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RekbUmmBWXVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDu3cX2_sTG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Hfzi7hH2pkKY",
        "DXnckUxf4-wL",
        "AdoSaFnH5P7p"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}