{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SPLADE for Portuguese"
      ],
      "metadata": {
        "id": "fiqVPqooNy9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspired by https://github.com/naver/splade\n",
        "\n",
        "(Até o momento, tentei poder parte do código.  Falta refatorar a parte relativa a configurações e entender o código em mais detallhes - ex.: a parte relativa aos dados, avaliação e indexação.  Pode ser útil também quebrar este notebook em etapas, ou então mover as classes para arquivos .py)"
      ],
      "metadata": {
        "id": "CD8thWyKPEk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries installation"
      ],
      "metadata": {
        "id": "CKOOCRLIOMSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6tC1DWMPiOw",
        "outputId": "298ecc6c-7433-4e2c-f8ae-a5af00269282"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "import contextlib\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "N49vX4bqOIxB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "bZiTSwK5nYba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerRep(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, model_type_or_dir, output, fp16=False):\n",
        "    \"\"\"\n",
        "    output indicates which representation(s) to output from transformer (\"MLM\" for MLM model)\n",
        "    model_type_or_dir is either the name of a pre-trained model (e.g. bert-base-uncased), or the path to\n",
        "    directory containing model weights, vocab etc.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    assert output in (\"mean\", \"cls\", \"hidden_states\", \"MLM\"), \"provide valid output\"\n",
        "    model_class = AutoModel if output != \"MLM\" else AutoModelForMaskedLM\n",
        "    self.transformer = model_class.from_pretrained(model_type_or_dir)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_type_or_dir)\n",
        "    self.output = output\n",
        "    self.fp16 = fp16\n",
        "\n",
        "  def forward(self, **tokens):\n",
        "    #with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n",
        "    with torch.cuda.amp.autocast() if self.fp16 else contextlib.nullcontext():\n",
        "      # tokens: output of HF tokenizer\n",
        "      out = self.transformer(**tokens)\n",
        "      if self.output == \"MLM\":\n",
        "          return out\n",
        "      hidden_states = self.transformer(**tokens)[0]\n",
        "      # => forward from AutoModel returns a tuple, first element is hidden states, shape (bs, seq_len, hidden_dim)\n",
        "      if self.output == \"mean\":\n",
        "        return torch.sum(hidden_states * tokens[\"attention_mask\"].unsqueeze(-1),\n",
        "                          dim=1) / torch.sum(tokens[\"attention_mask\"], dim=-1, keepdim=True)\n",
        "      elif self.output == \"cls\":\n",
        "        return hidden_states[:, 0, :]  # returns [CLS] representation\n",
        "      else:\n",
        "        return hidden_states, tokens[\"attention_mask\"]\n",
        "        # no pooling, we return all the hidden states (+ the attention mask)"
      ],
      "metadata": {
        "id": "xDbl0XFCPVxr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(tensor, eps=1e-9):\n",
        "  \"\"\"normalize input tensor on last dimension\n",
        "  \"\"\"\n",
        "  return tensor / (torch.norm(tensor, dim=-1, keepdim=True) + eps)"
      ],
      "metadata": {
        "id": "p22eD0KuTXCg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseBase(torch.nn.Module, ABC):\n",
        "\n",
        "  def __init__(self, model_type_or_dir, output, match=\"dot_product\", model_type_or_dir_q=None, freeze_d_model=False,\n",
        "                fp16=False):\n",
        "    super().__init__()\n",
        "    self.output = output\n",
        "    assert match in (\"dot_product\", \"cosine_sim\"), \"specify right match argument\"\n",
        "    self.cosine = True if match == \"cosine_sim\" else False\n",
        "    self.match = match\n",
        "    self.fp16 = fp16\n",
        "    self.transformer_rep = TransformerRep(model_type_or_dir, output, fp16)\n",
        "    self.transformer_rep_q = TransformerRep(model_type_or_dir_q,\n",
        "                                            output, fp16) if model_type_or_dir_q is not None else None\n",
        "    assert not (freeze_d_model and model_type_or_dir_q is None)\n",
        "    self.freeze_d_model = freeze_d_model\n",
        "    if freeze_d_model:\n",
        "        self.transformer_rep.requires_grad_(False)\n",
        "\n",
        "  def encode(self, kwargs, is_q):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def encode_(self, tokens, is_q=False):\n",
        "    transformer = self.transformer_rep\n",
        "    if is_q and self.transformer_rep_q is not None:\n",
        "        transformer = self.transformer_rep_q\n",
        "    return transformer(**tokens)\n",
        "\n",
        "  def train(self, mode=True):\n",
        "    if self.transformer_rep_q is None:  # only one model, life is simple\n",
        "      self.transformer_rep.train(mode)\n",
        "    else:  # possibly freeze d model\n",
        "      self.transformer_rep_q.train(mode)\n",
        "      mode_d = False if not mode else not self.freeze_d_model\n",
        "      self.transformer_rep.train(mode_d)\n",
        "\n",
        "  def forward(self, **kwargs):\n",
        "    \"\"\"forward takes as inputs 1 or 2 dict\n",
        "    \"d_kwargs\" => contains all inputs for document encoding\n",
        "    \"q_kwargs\" => contains all inputs for query encoding ([OPTIONAL], e.g. for indexing)\n",
        "    \"\"\"\n",
        "    #with torch.cuda.amp.autocast() if self.fp16 else NullContextManager():\n",
        "    with torch.cuda.amp.autocast() if self.fp16 else contextlib.nullcontext():\n",
        "      out = {}\n",
        "      do_d, do_q = \"d_kwargs\" in kwargs, \"q_kwargs\" in kwargs\n",
        "      if do_d:\n",
        "        d_rep = self.encode(kwargs[\"d_kwargs\"], is_q=False)\n",
        "        if self.cosine:  # normalize embeddings\n",
        "          d_rep = normalize(d_rep)\n",
        "        out.update({\"d_rep\": d_rep})\n",
        "      if do_q:\n",
        "        q_rep = self.encode(kwargs[\"q_kwargs\"], is_q=True)\n",
        "        if self.cosine:  # normalize embeddings\n",
        "          q_rep = normalize(q_rep)\n",
        "        out.update({\"q_rep\": q_rep})\n",
        "      if do_d and do_q:\n",
        "        if \"nb_negatives\" in kwargs:\n",
        "          # in the cas of negative scoring, where there are several negatives per query\n",
        "          bs = q_rep.shape[0]\n",
        "          d_rep = d_rep.reshape(bs, kwargs[\"nb_negatives\"], -1)  # shape (bs, nb_neg, out_dim)\n",
        "          q_rep = q_rep.unsqueeze(1)  # shape (bs, 1, out_dim)\n",
        "          score = torch.sum(q_rep * d_rep, dim=-1)  # shape (bs, nb_neg)\n",
        "        else:\n",
        "          if \"score_batch\" in kwargs:\n",
        "            score = torch.matmul(q_rep, d_rep.t())  # shape (bs_q, bs_d)\n",
        "          else:\n",
        "            score = torch.sum(q_rep * d_rep, dim=1, keepdim=True)  # shape (bs, )\n",
        "        out.update({\"score\": score})\n",
        "    return out"
      ],
      "metadata": {
        "id": "hm8gnsW1N-Db"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Splade(SiameseBase):\n",
        "  \"\"\"SPLADE model\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model_type_or_dir, model_type_or_dir_q=None, freeze_d_model=False, agg=\"max\", fp16=True):\n",
        "    super().__init__(model_type_or_dir=model_type_or_dir,\n",
        "                      output=\"MLM\",\n",
        "                      match=\"dot_product\",\n",
        "                      model_type_or_dir_q=model_type_or_dir_q,\n",
        "                      freeze_d_model=freeze_d_model,\n",
        "                      fp16=fp16)\n",
        "    self.output_dim = self.transformer_rep.transformer.config.vocab_size  # output dim = vocab size = 30522 for BERT\n",
        "    assert agg in (\"sum\", \"max\")\n",
        "    self.agg = agg\n",
        "\n",
        "  def encode(self, tokens, is_q):\n",
        "    out = self.encode_(tokens, is_q)[\"logits\"]  # shape (bs, pad_len, voc_size)\n",
        "    if self.agg == \"sum\":\n",
        "        return torch.sum(torch.log(1 + torch.relu(out)) * tokens[\"attention_mask\"].unsqueeze(-1), dim=1)\n",
        "    else:\n",
        "        values, _ = torch.max(torch.log(1 + torch.relu(out)) * tokens[\"attention_mask\"].unsqueeze(-1), dim=1)\n",
        "        return values\n",
        "        # 0 masking also works with max because all activations are positive"
      ],
      "metadata": {
        "id": "j30OQKKCN2Tb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 123"
      ],
      "metadata": {
        "id": "j-TLKx-2P0RS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_simple_bert_optim(model, lr, weight_decay, warmup_steps, num_training_steps):\n",
        "  \"\"\"\n",
        "  inspired from https://github.com/ArthurCamara/bert-axioms/blob/master/scripts/bert.py\n",
        "  \"\"\"\n",
        "  optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n",
        "                                              num_warmup_steps=warmup_steps,\n",
        "                                              num_training_steps=num_training_steps)\n",
        "  return optimizer, scheduler\n"
      ],
      "metadata": {
        "id": "OcyBuyWOQDD1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class InBatchPairwiseNLL:\n",
        "  \"\"\"in batch negatives version\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def __call__(self, out_d):\n",
        "    in_batch_scores, neg_scores = out_d[\"pos_score\"], out_d[\"neg_score\"]\n",
        "    # here in_batch_scores is a matrix of size bs * (bs / nb_gpus)\n",
        "    nb_columns = in_batch_scores.shape[1]\n",
        "    nb_gpus = int(in_batch_scores.shape[0] / nb_columns)\n",
        "    temp = torch.cat([in_batch_scores, neg_scores], dim=1)  # concat neg score from BM25 sampling\n",
        "    # shape (batch_size, batch_size/nb_gpus + 1)\n",
        "    scores = self.logsoftmax(temp)\n",
        "    return torch.mean(-scores[torch.arange(in_batch_scores.shape[0]),\n",
        "                              torch.arange(nb_columns).repeat(nb_gpus)])"
      ],
      "metadata": {
        "id": "huzUKeFvVs2X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(config):\n",
        "  if config[\"loss\"] == \"InBatchPairwiseNLL\":\n",
        "      loss = InBatchPairwiseNLL()\n",
        "  else:\n",
        "      raise NotImplementedError(\"provide valid loss\")\n",
        "  return loss"
      ],
      "metadata": {
        "id": "KKzq0LVCW6Jb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FLOPS:\n",
        "  \"\"\"constraint from Minimizing FLOPs to Learn Efficient Sparse Representations\n",
        "  https://arxiv.org/abs/2004.05665\n",
        "  \"\"\"\n",
        "\n",
        "  def __call__(self, batch_rep):\n",
        "      return torch.sum(torch.mean(torch.abs(batch_rep), dim=0) ** 2)"
      ],
      "metadata": {
        "id": "VPelhIoHXAHb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegWeightScheduler:\n",
        "  \"\"\"same scheduling as in: Minimizing FLOPs to Learn Efficient Sparse Representations\n",
        "  https://arxiv.org/abs/2004.05665\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, lambda_, T):\n",
        "      self.lambda_ = lambda_\n",
        "      self.T = T\n",
        "      self.t = 0\n",
        "      self.lambda_t = 0\n",
        "\n",
        "  def step(self):\n",
        "      \"\"\"quadratic increase until time T\n",
        "      \"\"\"\n",
        "      if self.t >= self.T:\n",
        "          pass\n",
        "      else:\n",
        "          self.t += 1\n",
        "          self.lambda_t = self.lambda_ * (self.t / self.T) ** 2\n",
        "      return self.lambda_t\n",
        "\n",
        "  def get_lambda(self):\n",
        "      return self.lambda_t\n"
      ],
      "metadata": {
        "id": "7EhB_BJdasv1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize regularizer dict\n",
        "if \"regularizer\" in config and regularizer is None:  # else regularizer is loaded\n",
        "  output_dim = model.module.output_dim if hasattr(model, \"module\") else model.output_dim\n",
        "  regularizer = {\"eval\": {\"L0\": {\"loss\": init_regularizer(\"L0\")},\n",
        "                          \"sparsity_ratio\": {\"loss\": init_regularizer(\"sparsity_ratio\",\n",
        "                                                                      output_dim=output_dim)}},\n",
        "                  \"train\": {}}\n",
        "  if config[\"regularizer\"] == \"eval_only\":\n",
        "    # just in the case we train a model without reg but still want the eval metrics like L0\n",
        "    pass\n",
        "  else:\n",
        "    for reg in config[\"regularizer\"]:\n",
        "      temp = {\"loss\": init_regularizer(config[\"regularizer\"][reg][\"reg\"]),\n",
        "              \"targeted_rep\": config[\"regularizer\"][reg][\"targeted_rep\"]}\n",
        "      d_ = {}\n",
        "      if \"lambda_q\" in config[\"regularizer\"][reg]:\n",
        "          d_[\"lambda_q\"] = RegWeightScheduler(config[\"regularizer\"][reg][\"lambda_q\"],\n",
        "                                              config[\"regularizer\"][reg][\"T\"])\n",
        "      if \"lambda_d\" in config[\"regularizer\"][reg]:\n",
        "          d_[\"lambda_d\"] = RegWeightScheduler(config[\"regularizer\"][reg][\"lambda_d\"],\n",
        "                                              config[\"regularizer\"][reg][\"T\"])\n",
        "      temp[\"lambdas\"] = d_  # it is possible to have reg only on q or d if e.g. you only specify lambda_q\n",
        "      # in the reg config\n",
        "      # targeted_rep is just used to indicate which rep to constrain (if e.g. the model outputs several\n",
        "      # representations)\n",
        "      # the common case: model outputs \"rep\" (in forward) and this should be the value for this targeted_rep\n",
        "      regularizer[\"train\"][reg] = temp\n"
      ],
      "metadata": {
        "id": "z5aCnhtgbOVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estou assumindo por enquanto que vamos iniciar o treinamento apenas com triplets.  Ainda não olhei os datasets."
      ],
      "metadata": {
        "id": "j7El2eYmdrZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PairsDatasetPreLoad(Dataset):\n",
        "  \"\"\"\n",
        "  dataset to iterate over a collection of pairs, format per line: q \\t d_pos \\t d_neg\n",
        "  we preload everything in memory at init\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_dir):\n",
        "    self.data_dir = data_dir\n",
        "    self.id_style = \"row_id\"\n",
        "\n",
        "    self.data_dict = {}  # => dict that maps the id to the line offset (position of pointer in the file)\n",
        "    print(\"Preloading dataset\")\n",
        "    self.data_dir = os.path.join(self.data_dir, \"raw.tsv\")\n",
        "    with open(self.data_dir) as reader:\n",
        "        for i, line in enumerate(tqdm(reader)):\n",
        "            if len(line) > 1:\n",
        "                query, pos, neg = line.split(\"\\t\")  # first column is id\n",
        "                self.data_dict[i] = (query.strip(), pos.strip(), neg.strip())\n",
        "    self.nb_ex = len(self.data_dict)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.nb_ex\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data_dict[idx]"
      ],
      "metadata": {
        "id": "3SbD4p4Wd8e4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_keys(d, prefix):\n",
        "  return {prefix + \"_\" + k: v for k, v in d.items()}\n",
        "\n",
        "\n",
        "class DataLoaderWrapper(DataLoader):\n",
        "  def __init__(self, tokenizer_type, max_length, **kwargs):\n",
        "    self.max_length = max_length\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_type)\n",
        "    super().__init__(collate_fn=self.collate_fn, **kwargs, pin_memory=True)\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    raise NotImplementedError(\"must implement this method\")\n",
        "\n",
        "class SiamesePairsDataLoader(DataLoaderWrapper):\n",
        "  \"\"\"Siamese encoding (query and document independent)\n",
        "  train mode (pairs)\n",
        "  \"\"\"\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    \"\"\"\n",
        "    batch is a list of tuples, each tuple has 3 (text) items (q, d_pos, d_neg)\n",
        "    \"\"\"\n",
        "    q, d_pos, d_neg = zip(*batch)\n",
        "    q = self.tokenizer(list(q),\n",
        "                        add_special_tokens=True,\n",
        "                        padding=\"longest\",  # pad to max sequence length in batch\n",
        "                        truncation=\"longest_first\",  # truncates to self.max_length\n",
        "                        max_length=self.max_length,\n",
        "                        return_attention_mask=True)\n",
        "    d_pos = self.tokenizer(list(d_pos),\n",
        "                            add_special_tokens=True,\n",
        "                            padding=\"longest\",  # pad to max sequence length in batch\n",
        "                            truncation=\"longest_first\",  # truncates to self.max_length\n",
        "                            max_length=self.max_length,\n",
        "                            return_attention_mask=True)\n",
        "    d_neg = self.tokenizer(list(d_neg),\n",
        "                            add_special_tokens=True,\n",
        "                            padding=\"longest\",  # pad to max sequence length in batch\n",
        "                            truncation=\"longest_first\",  # truncates to self.max_length\n",
        "                            max_length=self.max_length,\n",
        "                            return_attention_mask=True)\n",
        "    sample = {**rename_keys(q, \"q\"), **rename_keys(d_pos, \"pos\"), **rename_keys(d_neg, \"neg\")}\n",
        "    return {k: torch.tensor(v) for k, v in sample.items()}\n",
        "\n"
      ],
      "metadata": {
        "id": "dCH_PGHqeedT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = PairsDatasetPreLoad(data_dir=exp_dict[\"data\"][\"TRAIN_DATA_DIR\"])"
      ],
      "metadata": {
        "id": "y9VPwKmrdERc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_loader = None  # default\n",
        "if \"VALIDATION_SIZE_FOR_LOSS\" in exp_dict[\"data\"]:\n",
        "  print(\"initialize loader for validation loss\")\n",
        "  print(\"split train, originally {} pairs\".format(len(data_train)))\n",
        "  data_train, data_val = torch.utils.data.random_split(data_train, lengths=[\n",
        "      len(data_train) - exp_dict[\"data\"][\"VALIDATION_SIZE_FOR_LOSS\"],\n",
        "      exp_dict[\"data\"][\"VALIDATION_SIZE_FOR_LOSS\"]])\n",
        "  print(\"train: {} pairs ~~ val: {} pairs\".format(len(data_train), len(data_val)))\n",
        "  val_loss_loader = SiamesePairsDataLoader(dataset=data_val, batch_size=config[\"eval_batch_size\"],\n",
        "                                                shuffle=False,\n",
        "                                                num_workers=4,\n",
        "                                                tokenizer_type=config[\"tokenizer_type\"],\n",
        "                                                max_length=config[\"max_length\"], drop_last=drop_last)\n"
      ],
      "metadata": {
        "id": "Q2YjTPKjdp0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = SiamesePairsDataLoader(dataset=data_train, batch_size=config[\"train_batch_size\"], shuffle=True,\n",
        "                                              num_workers=4,\n",
        "                                              tokenizer_type=config[\"tokenizer_type\"],\n",
        "                                              max_length=config[\"max_length\"], drop_last=drop_last)"
      ],
      "metadata": {
        "id": "iJKkVlkofv_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS.: Ainda não olhei o que o código abaixo está fazendo (o que seria full ranking?)"
      ],
      "metadata": {
        "id": "QKm61jmJf-re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import h5py\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "class IndexDictOfArray:\n",
        "  def __init__(self, index_path=None, force_new=False, filename=\"array_index.h5py\", dim_voc=None):\n",
        "    if index_path is not None:\n",
        "      self.index_path = index_path\n",
        "      if not os.path.exists(index_path):\n",
        "          os.makedirs(index_path)\n",
        "      self.filename = os.path.join(self.index_path, filename)\n",
        "      if os.path.exists(self.filename) and not force_new:\n",
        "        print(\"index already exists, loading...\")\n",
        "        self.file = h5py.File(self.filename, \"r\")\n",
        "        if dim_voc is not None:\n",
        "            dim = dim_voc\n",
        "        else:\n",
        "            dim = self.file[\"dim\"][()]\n",
        "        self.index_doc_id = dict()\n",
        "        self.index_doc_value = dict()\n",
        "        for key in tqdm(range(dim)):\n",
        "            try:\n",
        "                self.index_doc_id[key] = np.array(self.file[\"index_doc_id_{}\".format(key)],\n",
        "                                                  dtype=np.int32)\n",
        "                # ideally we would not convert to np.array() but we cannot give pool an object with hdf5\n",
        "                self.index_doc_value[key] = np.array(self.file[\"index_doc_value_{}\".format(key)],\n",
        "                                                      dtype=np.float32)\n",
        "            except:\n",
        "                self.index_doc_id[key] = np.array([], dtype=np.int32)\n",
        "                self.index_doc_value[key] = np.array([], dtype=np.float32)\n",
        "        self.file.close()\n",
        "        del self.file\n",
        "        print(\"done loading index...\")\n",
        "        doc_ids = pickle.load(open(os.path.join(self.index_path, \"doc_ids.pkl\"), \"rb\"))\n",
        "        self.n = len(doc_ids)\n",
        "      else:\n",
        "        self.n = 0\n",
        "        print(\"initializing new index...\")\n",
        "        self.index_doc_id = defaultdict(lambda: array.array(\"I\"))\n",
        "        self.index_doc_value = defaultdict(lambda: array.array(\"f\"))\n",
        "    else:\n",
        "      self.n = 0\n",
        "      print(\"initializing new index...\")\n",
        "      self.index_doc_id = defaultdict(lambda: array.array(\"I\"))\n",
        "      self.index_doc_value = defaultdict(lambda: array.array(\"f\"))\n",
        "\n",
        "    def add_batch_document(self, row, col, data, n_docs=-1):\n",
        "      \"\"\"add a batch of documents to the index\n",
        "      \"\"\"\n",
        "      if n_docs < 0:\n",
        "          self.n += len(set(row))\n",
        "      else:\n",
        "          self.n += n_docs\n",
        "      for doc_id, dim_id, value in zip(row, col, data):\n",
        "          self.index_doc_id[dim_id].append(doc_id)\n",
        "          self.index_doc_value[dim_id].append(value)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.index_doc_id)\n",
        "\n",
        "    def nb_docs(self):\n",
        "      return self.n\n",
        "\n",
        "    def save(self, dim=None):\n",
        "      print(\"converting to numpy\")\n",
        "      for key in tqdm(list(self.index_doc_id.keys())):\n",
        "        self.index_doc_id[key] = np.array(self.index_doc_id[key], dtype=np.int32)\n",
        "        self.index_doc_value[key] = np.array(self.index_doc_value[key], dtype=np.float32)\n",
        "      print(\"save to disk\")\n",
        "      with h5py.File(self.filename, \"w\") as f:\n",
        "        if dim:\n",
        "            f.create_dataset(\"dim\", data=int(dim))\n",
        "        else:\n",
        "            f.create_dataset(\"dim\", data=len(self.index_doc_id.keys()))\n",
        "        for key in tqdm(self.index_doc_id.keys()):\n",
        "            f.create_dataset(\"index_doc_id_{}\".format(key), data=self.index_doc_id[key])\n",
        "            f.create_dataset(\"index_doc_value_{}\".format(key), data=self.index_doc_value[key])\n",
        "        f.close()\n",
        "      print(\"saving index distribution...\")  # => size of each posting list in a dict\n",
        "      index_dist = {}\n",
        "      for k, v in self.index_doc_id.items():\n",
        "        index_dist[int(k)] = len(v)\n",
        "      json.dump(index_dist, open(os.path.join(self.index_path, \"index_dist.json\"), \"w\"))\n"
      ],
      "metadata": {
        "id": "-IGMs-viiEmQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class L0:\n",
        "  \"\"\"non-differentiable\n",
        "  \"\"\"\n",
        "\n",
        "  def __call__(self, batch_rep):\n",
        "    return torch.count_nonzero(batch_rep, dim=-1).float().mean()"
      ],
      "metadata": {
        "id": "qfJDl-gRj4H3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_list(tensor):\n",
        "  return tensor.detach().cpu().tolist()"
      ],
      "metadata": {
        "id": "lSCscc8IkPP6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "class Evaluator:\n",
        "  def __init__(self, model, config=None, restore=True):\n",
        "    \"\"\"base class for model evaluation (inference)\n",
        "    \"\"\"\n",
        "    self.model = model\n",
        "    self.config = config\n",
        "    self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    if restore:\n",
        "      if self.device == torch.device(\"cuda\"):\n",
        "        if \"pretrained_no_yamlconfig\" not in config or not config[\"pretrained_no_yamlconfig\"]:\n",
        "          checkpoint = torch.load(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\"))\n",
        "          restore_model(model, checkpoint[\"model_state_dict\"])\n",
        "          print(\n",
        "              \"restore model on GPU at {}\".format(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\")))\n",
        "        self.model.eval()\n",
        "        if torch.cuda.device_count() > 1:\n",
        "          print(\" --- use {} GPUs --- \".format(torch.cuda.device_count()))\n",
        "          self.model = torch.nn.DataParallel(self.model)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "      else:  # CPU\n",
        "        if \"pretrained_no_yamlconfig\" not in config or not config[\"pretrained_no_yamlconfig\"]:\n",
        "          checkpoint = torch.load(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\"),\n",
        "                                  map_location=self.device)\n",
        "          restore_model(model, checkpoint[\"model_state_dict\"])\n",
        "          print(\n",
        "              \"restore model on CPU at {}\".format(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\")))\n",
        "    else:\n",
        "        print(\"WARNING: init evaluator, NOT restoring the model, NOT placing on device\")\n",
        "    self.model.eval()  # => put in eval mode\n",
        "\n",
        "\n",
        "#Seria essa a parte relativa ao índice invertido? TODO: Ainda não olhei\n",
        "class SparseIndexing(Evaluator):\n",
        "  \"\"\"sparse indexing\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model, config, compute_stats=False, dim_voc=None, is_query=False, force_new=True,**kwargs):\n",
        "    super().__init__(model, config, **kwargs)\n",
        "    self.index_dir = config[\"index_dir\"] if config is not None else None\n",
        "    self.sparse_index = IndexDictOfArray(self.index_dir, dim_voc=dim_voc, force_new=force_new)\n",
        "    self.compute_stats = compute_stats\n",
        "    self.is_query = is_query\n",
        "    if self.compute_stats:\n",
        "        self.l0 = L0()\n",
        "\n",
        "  def index(self, collection_loader, id_dict=None):\n",
        "    doc_ids = []\n",
        "    if self.compute_stats:\n",
        "        stats = defaultdict(float)\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "      for t, batch in enumerate(tqdm(collection_loader)):\n",
        "        inputs = {k: v.to(self.device) for k, v in batch.items() if k not in {\"id\"}}\n",
        "        if self.is_query:\n",
        "            batch_documents = self.model(q_kwargs=inputs)[\"q_rep\"]\n",
        "        else:\n",
        "            batch_documents = self.model(d_kwargs=inputs)[\"d_rep\"]\n",
        "        if self.compute_stats:\n",
        "            stats[\"L0_d\"] += self.l0(batch_documents).item()\n",
        "        row, col = torch.nonzero(batch_documents, as_tuple=True)\n",
        "        data = batch_documents[row, col]\n",
        "        row = row + count\n",
        "        batch_ids = to_list(batch[\"id\"])\n",
        "        if id_dict:\n",
        "            batch_ids = [id_dict[x] for x in batch_ids]\n",
        "        count += len(batch_ids)\n",
        "        doc_ids.extend(batch_ids)\n",
        "        self.sparse_index.add_batch_document(row.cpu().numpy(), col.cpu().numpy(), data.cpu().numpy(),\n",
        "                                              n_docs=len(batch_ids))\n",
        "    if self.compute_stats:\n",
        "      stats = {key: value / len(collection_loader) for key, value in stats.items()}\n",
        "    if self.index_dir is not None:\n",
        "      self.sparse_index.save()\n",
        "      pickle.dump(doc_ids, open(os.path.join(self.index_dir, \"doc_ids.pkl\"), \"wb\"))\n",
        "      print(\"done iterating over the corpus...\")\n",
        "      print(\"index contains {} posting lists\".format(len(self.sparse_index)))\n",
        "      print(\"index contains {} documents\".format(len(doc_ids)))\n",
        "      if self.compute_stats:\n",
        "        with open(os.path.join(self.index_dir, \"index_stats.json\"), \"w\") as handler:\n",
        "          json.dump(stats, handler)\n",
        "    else:\n",
        "      # if no index_dir, we do not write the index to disk but return it\n",
        "      for key in list(self.sparse_index.index_doc_id.keys()):\n",
        "          # convert to numpy\n",
        "          self.sparse_index.index_doc_id[key] = np.array(self.sparse_index.index_doc_id[key], dtype=np.int32)\n",
        "          self.sparse_index.index_doc_value[key] = np.array(self.sparse_index.index_doc_value[key],\n",
        "                                                            dtype=np.float32)\n",
        "      out = {\"index\": self.sparse_index, \"ids_mapping\": doc_ids}\n",
        "      if self.compute_stats:\n",
        "          out[\"stats\"] = stats\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "icMbi8v-hZh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CollectionDatasetPreLoad(Dataset):\n",
        "  \"\"\"\n",
        "  dataset to iterate over a document/query collection, format per line: format per line: doc_id \\t doc\n",
        "  we preload everything in memory at init\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_dir, id_style):\n",
        "    self.data_dir = data_dir\n",
        "    assert id_style in (\"row_id\", \"content_id\"), \"provide valid id_style\"\n",
        "    # id_style indicates how we access the doc/q (row id or doc/q id)\n",
        "    self.id_style = id_style\n",
        "    self.data_dict = {}\n",
        "    self.line_dict = {}\n",
        "    print(\"Preloading dataset\")\n",
        "    with open(os.path.join(self.data_dir, \"raw.tsv\")) as reader:\n",
        "        for i, line in enumerate(tqdm(reader)):\n",
        "            if len(line) > 1:\n",
        "                id_, *data = line.split(\"\\t\")  # first column is id\n",
        "                data = \" \".join(\" \".join(data).splitlines())\n",
        "                if self.id_style == \"row_id\":\n",
        "                    self.data_dict[i] = data\n",
        "                    self.line_dict[i] = id_.strip()\n",
        "                else:\n",
        "                    self.data_dict[id_] = data.strip()\n",
        "    self.nb_ex = len(self.data_dict)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.nb_ex\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.id_style == \"row_id\":\n",
        "        return self.line_dict[idx], self.data_dict[idx]\n",
        "    else:\n",
        "        return str(idx), self.data_dict[str(idx)]"
      ],
      "metadata": {
        "id": "90mFbLA6gfyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CollectionDataLoader(DataLoaderWrapper):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    \"\"\"\n",
        "    batch is a list of tuples, each tuple has 2 (text) items (id_, doc)\n",
        "    \"\"\"\n",
        "    id_, d = zip(*batch)\n",
        "    processed_passage = self.tokenizer(list(d),\n",
        "                                        add_special_tokens=True,\n",
        "                                        padding=\"longest\",  # pad to max sequence length in batch\n",
        "                                        truncation=\"longest_first\",  # truncates to self.max_length\n",
        "                                        max_length=self.max_length,\n",
        "                                        return_attention_mask=True)\n",
        "    return {**{k: torch.tensor(v) for k, v in processed_passage.items()},\n",
        "            \"id\": torch.tensor([int(i) for i in id_], dtype=torch.long)}"
      ],
      "metadata": {
        "id": "jem1KFH-gRa0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makedir(dir_):\n",
        "    if not os.path.exists(dir_):\n",
        "        os.makedirs(dir_)"
      ],
      "metadata": {
        "id": "Se3OBVnGmrtk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restore_model(model, state_dict):\n",
        "    missing_keys, unexpected_keys = model.load_state_dict(state_dict=state_dict, strict=False)\n",
        "    # strict = False => it means that we just load the parameters of layers which are present in both and\n",
        "    # ignores the rest\n",
        "    if len(missing_keys) > 0:\n",
        "        print(\"~~ [WARNING] MISSING KEYS WHILE RESTORING THE MODEL ~~\")\n",
        "        print(missing_keys)\n",
        "    if len(unexpected_keys) > 0:\n",
        "        print(\"~~ [WARNING] UNEXPECTED KEYS WHILE RESTORING THE MODEL ~~\")\n",
        "        print(unexpected_keys)\n",
        "    print(\"restoring model:\", model.__class__.__name__)"
      ],
      "metadata": {
        "id": "5ax0DhzKmx3a"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "  def __init__(self, model, config=None, restore=True):\n",
        "    \"\"\"base class for model evaluation (inference)\n",
        "    \"\"\"\n",
        "    self.model = model\n",
        "    self.config = config\n",
        "    self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    if restore:\n",
        "      if self.device == torch.device(\"cuda\"):\n",
        "        if \"pretrained_no_yamlconfig\" not in config or not config[\"pretrained_no_yamlconfig\"]:\n",
        "          checkpoint = torch.load(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\"))\n",
        "          restore_model(model, checkpoint[\"model_state_dict\"])\n",
        "          print(\n",
        "              \"restore model on GPU at {}\".format(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\")))\n",
        "        self.model.eval()\n",
        "        if torch.cuda.device_count() > 1:\n",
        "          print(\" --- use {} GPUs --- \".format(torch.cuda.device_count()))\n",
        "          self.model = torch.nn.DataParallel(self.model)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "      else:  # CPU\n",
        "        if \"pretrained_no_yamlconfig\" not in config or not config[\"pretrained_no_yamlconfig\"]:\n",
        "          checkpoint = torch.load(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\"),\n",
        "                                  map_location=self.device)\n",
        "          restore_model(model, checkpoint[\"model_state_dict\"])\n",
        "          print(\n",
        "              \"restore model on CPU at {}\".format(os.path.join(config[\"checkpoint_dir\"], \"model/model.tar\")))\n",
        "    else:\n",
        "        print(\"WARNING: init evaluator, NOT restoring the model, NOT placing on device\")\n",
        "    self.model.eval()  # => put in eval mode\n"
      ],
      "metadata": {
        "id": "CBPtHDrum1xY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numba\n",
        "\n",
        "class SparseRetrieval(Evaluator):\n",
        "  \"\"\"retrieval from SparseIndexing\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def select_topk(filtered_indexes, scores, k):\n",
        "    if len(filtered_indexes) > k:\n",
        "        sorted_ = np.argpartition(scores, k)[:k]\n",
        "        filtered_indexes, scores = filtered_indexes[sorted_], -scores[sorted_]\n",
        "    else:\n",
        "        scores = -scores\n",
        "    return filtered_indexes, scores\n",
        "\n",
        "  @staticmethod\n",
        "  @numba.njit(nogil=True, parallel=True, cache=True)\n",
        "  def numba_score_float(inverted_index_ids: numba.typed.Dict,\n",
        "                        inverted_index_floats: numba.typed.Dict,\n",
        "                        indexes_to_retrieve: np.ndarray,\n",
        "                        query_values: np.ndarray,\n",
        "                        threshold: float,\n",
        "                        size_collection: int):\n",
        "      scores = np.zeros(size_collection, dtype=np.float32)  # initialize array with size = size of collection\n",
        "      n = len(indexes_to_retrieve)\n",
        "      for _idx in range(n):\n",
        "          local_idx = indexes_to_retrieve[_idx]  # which posting list to search\n",
        "          query_float = query_values[_idx]  # what is the value of the query for this posting list\n",
        "          retrieved_indexes = inverted_index_ids[local_idx]  # get indexes from posting list\n",
        "          retrieved_floats = inverted_index_floats[local_idx]  # get values from posting list\n",
        "          for j in numba.prange(len(retrieved_indexes)):\n",
        "              scores[retrieved_indexes[j]] += query_float * retrieved_floats[j]\n",
        "      filtered_indexes = np.argwhere(scores > threshold)[:, 0]  # ideally we should have a threshold to filter\n",
        "      # unused documents => this should be tuned, currently it is set to 0\n",
        "      return filtered_indexes, -scores[filtered_indexes]\n",
        "\n",
        "  def __init__(self, model, config, dim_voc, dataset_name=None, index_d=None, compute_stats=False, is_beir=False,\n",
        "                **kwargs):\n",
        "    super().__init__(model, config, **kwargs)\n",
        "    assert (\"index_dir\" in config and index_d is None) or (\n",
        "            \"index_dir\" not in config and index_d is not None)\n",
        "    if \"index_dir\" in config:\n",
        "      self.sparse_index = IndexDictOfArray(config[\"index_dir\"], dim_voc=dim_voc)\n",
        "      self.doc_ids = pickle.load(open(os.path.join(config[\"index_dir\"], \"doc_ids.pkl\"), \"rb\"))\n",
        "    else:\n",
        "      self.sparse_index = index_d[\"index\"]\n",
        "      self.doc_ids = index_d[\"ids_mapping\"]\n",
        "      for i in range(dim_voc):\n",
        "          # missing keys (== posting lists), causing issues for retrieval => fill with empty\n",
        "          if i not in self.sparse_index.index_doc_id:\n",
        "              self.sparse_index.index_doc_id[i] = np.array([], dtype=np.int32)\n",
        "              self.sparse_index.index_doc_value[i] = np.array([], dtype=np.float32)\n",
        "    # convert to numba\n",
        "    self.numba_index_doc_ids = numba.typed.Dict()\n",
        "    self.numba_index_doc_values = numba.typed.Dict()\n",
        "    for key, value in self.sparse_index.index_doc_id.items():\n",
        "      self.numba_index_doc_ids[key] = value\n",
        "    for key, value in self.sparse_index.index_doc_value.items():\n",
        "      self.numba_index_doc_values[key] = value\n",
        "    self.out_dir = os.path.join(config[\"out_dir\"], dataset_name) if (dataset_name is not None and not is_beir) \\\n",
        "        else config[\"out_dir\"]\n",
        "    self.doc_stats = index_d[\"stats\"] if (index_d is not None and compute_stats) else None\n",
        "    self.compute_stats = compute_stats\n",
        "    if self.compute_stats:\n",
        "      self.l0 = L0()\n",
        "\n",
        "  def retrieve(self, q_loader, top_k, name=None, return_d=False, id_dict=False, threshold=0):\n",
        "    makedir(self.out_dir)\n",
        "    if self.compute_stats:\n",
        "      makedir(os.path.join(self.out_dir, \"stats\"))\n",
        "    res = defaultdict(dict)\n",
        "    if self.compute_stats:\n",
        "      stats = defaultdict(float)\n",
        "    with torch.no_grad():\n",
        "      for t, batch in enumerate(tqdm(q_loader)):\n",
        "        q_id = to_list(batch[\"id\"])[0]\n",
        "        if id_dict:\n",
        "          q_id = id_dict[q_id]\n",
        "        inputs = {k: v for k, v in batch.items() if k not in {\"id\"}}\n",
        "        for k, v in inputs.items():\n",
        "          inputs[k] = v.to(self.device)\n",
        "        query = self.model(q_kwargs=inputs)[\"q_rep\"]  # we assume ONE query per batch here\n",
        "        if self.compute_stats:\n",
        "          stats[\"L0_q\"] += self.l0(query).item()\n",
        "        # TODO: batched version for retrieval\n",
        "        row, col = torch.nonzero(query, as_tuple=True)\n",
        "        values = query[to_list(row), to_list(col)]\n",
        "        filtered_indexes, scores = self.numba_score_float(self.numba_index_doc_ids,\n",
        "                                                          self.numba_index_doc_values,\n",
        "                                                          col.cpu().numpy(),\n",
        "                                                          values.cpu().numpy().astype(np.float32),\n",
        "                                                          threshold=threshold,\n",
        "                                                          size_collection=self.sparse_index.nb_docs())\n",
        "        # threshold set to 0 by default, could be better\n",
        "        filtered_indexes, scores = self.select_topk(filtered_indexes, scores, k=top_k)\n",
        "        for id_, sc in zip(filtered_indexes, scores):\n",
        "          res[str(q_id)][str(self.doc_ids[id_])] = float(sc)\n",
        "    if self.compute_stats:\n",
        "      stats = {key: value / len(q_loader) for key, value in stats.items()}\n",
        "    if self.compute_stats:\n",
        "      with open(os.path.join(self.out_dir, \"stats\",\n",
        "                              \"q_stats{}.json\".format(\"_iter_{}\".format(name) if name is not None else \"\")),\n",
        "                \"w\") as handler:\n",
        "        json.dump(stats, handler)\n",
        "      if self.doc_stats is not None:\n",
        "        with open(os.path.join(self.out_dir, \"stats\",\n",
        "                                \"d_stats{}.json\".format(\"_iter_{}\".format(name) if name is not None else \"\")),\n",
        "                  \"w\") as handler:\n",
        "            json.dump(self.doc_stats, handler)\n",
        "    with open(os.path.join(self.out_dir, \"run{}.json\".format(\"_iter_{}\".format(name) if name is not None else \"\")),\n",
        "              \"w\") as handler:\n",
        "      json.dump(res, handler)\n",
        "    if return_d:\n",
        "      out = {\"retrieval\": res}\n",
        "      if self.compute_stats:\n",
        "          out[\"stats\"] = stats if self.doc_stats is None else {**stats, **self.doc_stats}\n",
        "      return out\n",
        "\n"
      ],
      "metadata": {
        "id": "unHB5s8mgwF_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseApproxEvalWrapper(Evaluator):\n",
        "  \"\"\"\n",
        "  wrapper for sparse indexer + retriever during training\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model, config, collection_loader, q_loader, **kwargs):\n",
        "    super().__init__(model, config, **kwargs)\n",
        "    self.collection_loader = collection_loader\n",
        "    self.q_loader = q_loader\n",
        "    self.model_output_dim = self.model.module.output_dim if hasattr(self.model, \"module\") else self.model.output_dim\n",
        "\n",
        "  def index_and_retrieve(self, i):\n",
        "    indexer = SparseIndexing(self.model, config=None, restore=False, compute_stats=True)\n",
        "    sparse_index_d = indexer.index(self.collection_loader)\n",
        "    retriever = SparseRetrieval(self.model, self.config, dim_voc=self.model_output_dim, index_d=sparse_index_d,\n",
        "                                restore=False, compute_stats=True)\n",
        "    return retriever.retrieve(self.q_loader, top_k=self.config[\"top_k\"], name=i, return_d=True)"
      ],
      "metadata": {
        "id": "2FzaucQwm7Ko"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_evaluator = None\n",
        "if \"VALIDATION_FULL_RANKING\" in exp_dict[\"data\"]:\n",
        "    with open_dict(config):\n",
        "        config[\"val_full_rank_qrel_path\"] = exp_dict[\"data\"][\"VALIDATION_FULL_RANKING\"][\"QREL_PATH\"]\n",
        "    full_ranking_d_collection = CollectionDatasetPreLoad(\n",
        "        data_dir=exp_dict[\"data\"][\"VALIDATION_FULL_RANKING\"][\"D_COLLECTION_PATH\"], id_style=\"row_id\")\n",
        "    full_ranking_d_loader = CollectionDataLoader(dataset=full_ranking_d_collection,\n",
        "                                                  tokenizer_type=config[\"tokenizer_type\"],\n",
        "                                                  max_length=config[\"max_length\"],\n",
        "                                                  batch_size=config[\"eval_batch_size\"],\n",
        "                                                  shuffle=False, num_workers=4)\n",
        "    full_ranking_q_collection = CollectionDatasetPreLoad(\n",
        "        data_dir=exp_dict[\"data\"][\"VALIDATION_FULL_RANKING\"][\"Q_COLLECTION_PATH\"], id_style=\"row_id\")\n",
        "    full_ranking_q_loader = CollectionDataLoader(dataset=full_ranking_q_collection,\n",
        "                                                  tokenizer_type=config[\"tokenizer_type\"],\n",
        "                                                  max_length=config[\"max_length\"], batch_size=1,\n",
        "                                                  # TODO fix: bs currently set to 1\n",
        "                                                  shuffle=False, num_workers=4)\n",
        "    val_evaluator = SparseApproxEvalWrapper(model,\n",
        "                                            config={\"top_k\": exp_dict[\"data\"][\"VALIDATION_FULL_RANKING\"][\"TOP_K\"],\n",
        "                                                    \"out_dir\": os.path.join(config[\"checkpoint_dir\"],\n",
        "                                                                            \"val_full_ranking\")\n",
        "                                                    },\n",
        "                                            collection_loader=full_ranking_d_loader,\n",
        "                                            q_loader=full_ranking_q_loader,\n",
        "                                            restore=False)"
      ],
      "metadata": {
        "id": "5CATt3asf8I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #################################################################\n",
        "# # TRAIN\n",
        "# #################################################################\n",
        "print(\"+++++ BEGIN TRAINING +++++\")\n",
        "trainer = SiameseTransformerTrainer(model=model, iterations=iterations, loss=loss, optimizer=optimizer,\n",
        "                                    config=config, scheduler=scheduler,\n",
        "                                    train_loader=train_loader, validation_loss_loader=val_loss_loader,\n",
        "                                    validation_evaluator=val_evaluator,\n",
        "                                    regularizer=regularizer)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "CgcksBupnSYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}