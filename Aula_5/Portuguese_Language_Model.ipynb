{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a709ac496f64944933b4292e542e93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ceeef292ed9b43e2a4d07692754615f0",
              "IPY_MODEL_62e82c17bdb242e4a66cd57e43798657",
              "IPY_MODEL_ea503853f8314ec28514f75a7ad1587a"
            ],
            "layout": "IPY_MODEL_e847f43d60674390958db833ae43b027"
          }
        },
        "ceeef292ed9b43e2a4d07692754615f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d296db9f38b4b88bc1e273d97db389a",
            "placeholder": "​",
            "style": "IPY_MODEL_5a7952ca9fa449c495f89323d6518ce5",
            "value": "100%"
          }
        },
        "62e82c17bdb242e4a66cd57e43798657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a8dd46cbc64869ac1e9b1c662a2d7c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2cbc2d012bdd4a5e8c15afe9f8dd13d8",
            "value": 1
          }
        },
        "ea503853f8314ec28514f75a7ad1587a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddc23ce52e2945f89463ebb3addbf972",
            "placeholder": "​",
            "style": "IPY_MODEL_c7533a9e950b464a85d87a94de82d22a",
            "value": " 1/1 [00:00&lt;00:00,  6.90it/s]"
          }
        },
        "e847f43d60674390958db833ae43b027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d296db9f38b4b88bc1e273d97db389a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7952ca9fa449c495f89323d6518ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a8dd46cbc64869ac1e9b1c662a2d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cbc2d012bdd4a5e8c15afe9f8dd13d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddc23ce52e2945f89463ebb3addbf972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7533a9e950b464a85d87a94de82d22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "984ae16fac08429b9493c8d16d60e4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_821a86daf38045c7ac7b54a2c0504c38",
              "IPY_MODEL_37f6e815341a458aa4eb1dff93aad573",
              "IPY_MODEL_30d2091b9f9b465ab392466ef9c4e74a"
            ],
            "layout": "IPY_MODEL_43a644330ca74501952ca1008072ac0f"
          }
        },
        "821a86daf38045c7ac7b54a2c0504c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cec3cedd32a488fb6bf4205e4333f08",
            "placeholder": "​",
            "style": "IPY_MODEL_82ae4dc6dfa647cb8b794fedeae52326",
            "value": "Running tokenizer on dataset: 100%"
          }
        },
        "37f6e815341a458aa4eb1dff93aad573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3be5acfc516f44b7b80a2bf759dc77a6",
            "max": 237500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f8d4afb149b4650bbe40c4034122b4f",
            "value": 237500
          }
        },
        "30d2091b9f9b465ab392466ef9c4e74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c432f6f427e84c3fa12b633b2ed517bf",
            "placeholder": "​",
            "style": "IPY_MODEL_3029c1d2fb6b41cbb3d3bad1ae01632a",
            "value": " 237500/237500 [41:30&lt;00:00, 105.75 examples/s]"
          }
        },
        "43a644330ca74501952ca1008072ac0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "2cec3cedd32a488fb6bf4205e4333f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ae4dc6dfa647cb8b794fedeae52326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3be5acfc516f44b7b80a2bf759dc77a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f8d4afb149b4650bbe40c4034122b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c432f6f427e84c3fa12b633b2ed517bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3029c1d2fb6b41cbb3d3bad1ae01632a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ec9a23686c14d75bdc4e870ae9f3dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_721c84d07d834f968a14cf7d5a10b7f3",
              "IPY_MODEL_12314ad60e6e407b8a2d61e4b722adbe",
              "IPY_MODEL_b66d5d2d72e24e429cb6b8ef81d85cfe"
            ],
            "layout": "IPY_MODEL_9d4d4fd6acb24c21a53c2077e572092b"
          }
        },
        "721c84d07d834f968a14cf7d5a10b7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41bf22b7bf9e4846a934d9f6e4b88f68",
            "placeholder": "​",
            "style": "IPY_MODEL_c74586530021494c9603612d8584a851",
            "value": "Running tokenizer on dataset: 100%"
          }
        },
        "12314ad60e6e407b8a2d61e4b722adbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b2e51637584e67872af6f111deb96d",
            "max": 12500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d238a5be1414841af755f68956382f6",
            "value": 12500
          }
        },
        "b66d5d2d72e24e429cb6b8ef81d85cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10477b3da84041b68575c6e2ce729e12",
            "placeholder": "​",
            "style": "IPY_MODEL_d6eb4bdba3e34ef89f6c82e182f61649",
            "value": " 12500/12500 [02:15&lt;00:00, 95.56 examples/s]"
          }
        },
        "9d4d4fd6acb24c21a53c2077e572092b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "41bf22b7bf9e4846a934d9f6e4b88f68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74586530021494c9603612d8584a851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50b2e51637584e67872af6f111deb96d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d238a5be1414841af755f68956382f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10477b3da84041b68575c6e2ce729e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6eb4bdba3e34ef89f6c82e182f61649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cdbd11464a74c9796f575c6b0e2c9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_414193e871ac4fbeac9024a8333b4588",
              "IPY_MODEL_4f5978e36d454dcf9e1d328b04247c8b",
              "IPY_MODEL_7aac76d55c544c1c992869640c0c70cf"
            ],
            "layout": "IPY_MODEL_131e229e49e945619ff54f8435fa6e27"
          }
        },
        "414193e871ac4fbeac9024a8333b4588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb7f647191b14eb7b1dfced3e67a99be",
            "placeholder": "​",
            "style": "IPY_MODEL_4a419aab5de742f7bf55aa4985195bb4",
            "value": "Grouping texts in chunks of 1024:   7%"
          }
        },
        "4f5978e36d454dcf9e1d328b04247c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1685ecd53454ee9aefe490df9dc7d13",
            "max": 237500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e01638dad84e4bb8bdb4c3fd4ac762b3",
            "value": 16000
          }
        },
        "7aac76d55c544c1c992869640c0c70cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_636e4d1dc1e8466f85cc1791f9075731",
            "placeholder": "​",
            "style": "IPY_MODEL_bdecb3785db440d58edda1febbeaa1b1",
            "value": " 16000/237500 [00:52&lt;10:23, 355.32 examples/s]"
          }
        },
        "131e229e49e945619ff54f8435fa6e27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb7f647191b14eb7b1dfced3e67a99be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a419aab5de742f7bf55aa4985195bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1685ecd53454ee9aefe490df9dc7d13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01638dad84e4bb8bdb4c3fd4ac762b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "636e4d1dc1e8466f85cc1791f9075731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdecb3785db440d58edda1febbeaa1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDOCJUXvq1tl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py"
      ],
      "metadata": {
        "id": "UXA8cY0zq3pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zGzMWq5WQ9O",
        "outputId": "3340b981-e4de-4be9-fc2b-bfa1dcfab4e3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVCNgZxRq2nq",
        "outputId": "c023cdc1-e3d5-4db4-f22b-78e2cec02473"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt...\n",
            "/ [0 files][    0.0 B/  1.2 GiB]                                                \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "\\ [1 files][  1.2 GiB/  1.2 GiB]   80.5 MiB/s                                   \n",
            "Operation completed over 1 objects/1.2 GiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj2GizHUtFZu",
        "outputId": "bb353ffe-2e5b-4de5-d25b-b86e01e55962"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/469.0 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKH5urlxtOUB",
        "outputId": "81019320-b3c1-46ce-9e4b-f507ff01a863"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.4 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2023.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from evaluate) (1.4.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASdFSvDctXDI",
        "outputId": "e969e0e5-4d98-4c5f-bfc8-f9813515a4c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/6.8 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from itertools import chain\n",
        "from typing import Optional\n",
        "\n",
        "import datasets\n",
        "import evaluate\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    is_torch_tpu_available,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.testing_utils import CaptureLogger\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version, send_example_telemetry\n",
        "from transformers.utils.versions import require_version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr4JJcrJq5qe",
        "outputId": "09571489-1edb-46da-ae74-8eae714ef588"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())"
      ],
      "metadata": {
        "id": "82MjLnBosO3Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CONFIG_CLASSES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt5zbPQ1ttek",
        "outputId": "3782a4cc-4480-4d9c-c132-8045c732450d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[transformers.models.bart.configuration_bart.BartConfig,\n",
              " transformers.models.bert.configuration_bert.BertConfig,\n",
              " transformers.models.bert_generation.configuration_bert_generation.BertGenerationConfig,\n",
              " transformers.models.big_bird.configuration_big_bird.BigBirdConfig,\n",
              " transformers.models.bigbird_pegasus.configuration_bigbird_pegasus.BigBirdPegasusConfig,\n",
              " transformers.models.biogpt.configuration_biogpt.BioGptConfig,\n",
              " transformers.models.blenderbot.configuration_blenderbot.BlenderbotConfig,\n",
              " transformers.models.blenderbot_small.configuration_blenderbot_small.BlenderbotSmallConfig,\n",
              " transformers.models.bloom.configuration_bloom.BloomConfig,\n",
              " transformers.models.camembert.configuration_camembert.CamembertConfig,\n",
              " transformers.models.codegen.configuration_codegen.CodeGenConfig,\n",
              " transformers.models.ctrl.configuration_ctrl.CTRLConfig,\n",
              " transformers.models.data2vec.configuration_data2vec_text.Data2VecTextConfig,\n",
              " transformers.models.electra.configuration_electra.ElectraConfig,\n",
              " transformers.models.ernie.configuration_ernie.ErnieConfig,\n",
              " transformers.models.git.configuration_git.GitConfig,\n",
              " transformers.models.gpt2.configuration_gpt2.GPT2Config,\n",
              " transformers.models.gpt2.configuration_gpt2.GPT2Config,\n",
              " transformers.models.gpt_neo.configuration_gpt_neo.GPTNeoConfig,\n",
              " transformers.models.gpt_neox.configuration_gpt_neox.GPTNeoXConfig,\n",
              " transformers.models.gpt_neox_japanese.configuration_gpt_neox_japanese.GPTNeoXJapaneseConfig,\n",
              " transformers.models.gptj.configuration_gptj.GPTJConfig,\n",
              " transformers.models.marian.configuration_marian.MarianConfig,\n",
              " transformers.models.mbart.configuration_mbart.MBartConfig,\n",
              " transformers.models.megatron_bert.configuration_megatron_bert.MegatronBertConfig,\n",
              " transformers.models.mvp.configuration_mvp.MvpConfig,\n",
              " transformers.models.openai.configuration_openai.OpenAIGPTConfig,\n",
              " transformers.models.opt.configuration_opt.OPTConfig,\n",
              " transformers.models.pegasus.configuration_pegasus.PegasusConfig,\n",
              " transformers.models.plbart.configuration_plbart.PLBartConfig,\n",
              " transformers.models.prophetnet.configuration_prophetnet.ProphetNetConfig,\n",
              " transformers.models.qdqbert.configuration_qdqbert.QDQBertConfig,\n",
              " transformers.models.reformer.configuration_reformer.ReformerConfig,\n",
              " transformers.models.rembert.configuration_rembert.RemBertConfig,\n",
              " transformers.models.roberta.configuration_roberta.RobertaConfig,\n",
              " transformers.models.roberta_prelayernorm.configuration_roberta_prelayernorm.RobertaPreLayerNormConfig,\n",
              " transformers.models.roc_bert.configuration_roc_bert.RoCBertConfig,\n",
              " transformers.models.roformer.configuration_roformer.RoFormerConfig,\n",
              " transformers.models.speech_to_text_2.configuration_speech_to_text_2.Speech2Text2Config,\n",
              " transformers.models.transfo_xl.configuration_transfo_xl.TransfoXLConfig,\n",
              " transformers.models.trocr.configuration_trocr.TrOCRConfig,\n",
              " transformers.models.xglm.configuration_xglm.XGLMConfig,\n",
              " transformers.models.xlm.configuration_xlm.XLMConfig,\n",
              " transformers.models.xlm_prophetnet.configuration_xlm_prophetnet.XLMProphetNetConfig,\n",
              " transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig,\n",
              " transformers.models.xlm_roberta_xl.configuration_xlm_roberta_xl.XLMRobertaXLConfig,\n",
              " transformers.models.xlnet.configuration_xlnet.XLNetConfig,\n",
              " transformers.models.xmod.configuration_xmod.XmodConfig]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
      ],
      "metadata": {
        "id": "9cbX-k7KtwbP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TYPES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3f_XbdjuDo-",
        "outputId": "99ba29cc-f452-4c34-c5cc-730539b4e642"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bart',\n",
              " 'bert',\n",
              " 'bert-generation',\n",
              " 'big_bird',\n",
              " 'bigbird_pegasus',\n",
              " 'biogpt',\n",
              " 'blenderbot',\n",
              " 'blenderbot-small',\n",
              " 'bloom',\n",
              " 'camembert',\n",
              " 'codegen',\n",
              " 'ctrl',\n",
              " 'data2vec-text',\n",
              " 'electra',\n",
              " 'ernie',\n",
              " 'git',\n",
              " 'gpt2',\n",
              " 'gpt2',\n",
              " 'gpt_neo',\n",
              " 'gpt_neox',\n",
              " 'gpt_neox_japanese',\n",
              " 'gptj',\n",
              " 'marian',\n",
              " 'mbart',\n",
              " 'megatron-bert',\n",
              " 'mvp',\n",
              " 'openai-gpt',\n",
              " 'opt',\n",
              " 'pegasus',\n",
              " 'plbart',\n",
              " 'prophetnet',\n",
              " 'qdqbert',\n",
              " 'reformer',\n",
              " 'rembert',\n",
              " 'roberta',\n",
              " 'roberta-prelayernorm',\n",
              " 'roc_bert',\n",
              " 'roformer',\n",
              " 'speech_to_text_2',\n",
              " 'transfo-xl',\n",
              " 'trocr',\n",
              " 'xglm',\n",
              " 'xlm',\n",
              " 'xlm-prophetnet',\n",
              " 'xlm-roberta',\n",
              " 'xlm-roberta-xl',\n",
              " 'xlnet',\n",
              " 'xmod')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
        "    )\n",
        "    config_overrides: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n",
        "                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    torch_dtype: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n",
        "                \"dtype will be automatically derived from the model's weights.\"\n",
        "            ),\n",
        "            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
        "        },\n",
        "    )\n",
        "    low_cpu_mem_usage: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded.\"\n",
        "                \"set True will benefit LLM loading time and RAM consumption.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n",
        "            raise ValueError(\n",
        "                \"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"\n",
        "            )\n"
      ],
      "metadata": {
        "id": "nuFTXQ52uF-p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P69XhC-pvECm",
        "outputId": "66583d4a-5cf1-4faa-e3de-125a7a810c00"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linkbar Há alguns anos, o número de rapazes e moças que subiam ao púlpito para pregar era maior que o de hoje. Na sua simplicidade, falavam do amor de Deus, da Salvação e davam testemunho sob a unção do Espirito Santo. Hoje, parece que a figura do \"preletor oficial\" inibiu muitos de falarem com ousadia a Palavra de Deus. Parece que há um receio de falar diante de um público que, certamente, é mais intelectualizado que há alguns anos. Jovens pregadores ficam embaraçados e cometem certos deslizes, que poderiam ser evitados. Neste modesto trabalho, vamos dar apenas algumas sugestões, e não um estudo sobre a Homilética (Arte de Falar em Publico). I -O QUE PREGAR? É a comunicação verbal da Palavra de Deus aos ouvintes. É a transmissão do evangelho de Nosso Senhor Jesus Cristo às pessoas que precisam ouvi-lo. II- QUAL A FINALIDADE DA PREGAÇÃO? É persuadir as pessoas a aceitarem a mensagem da Palavra de Deus para sua salvação (descrentes) ou para seu crescimento espiritual (crentes). Diante disso, o pregador precisa saber para quem esta falando: Para crentes ou para descrentes? III- QUE DEVE CONTER A PREGAÇÃO? Três coisas são básicas: 1. OBJETIVIDADE. Refere-se ao alvo a atingir. Se pregamos para descrentes, desejamos que eles entendam que precisam crer em Jesus para ser salvos. Devemos orar muito, antes de pregar, para que o Espirito Santo convença as pessoas do seu pecado. Se isso acontecer, a pregação alcança seu alvo. O centro da pregação deve ser Cristo e não o pregador, como acontece em certas cruzadas ou movimentos evangelísticos. Há pregadores que se perdem no púlpito. Começam a falar do amor de Deus, e passam a divagar sobre o Apocalipse, vão até Gênesis, aos profetas e, ao final, não sabem como sair do emaranhado de palavras. É preciso ter objetividade. 2. TRANSMISSÃO. O pregador deve procurar transmitir a mensagem de Deus às pessoas. Paulo disse: \"Porque eu recebi do Senhor o que também vos ensinei...\" O mensageiro deve receber a mensagem de Deus e transmiti-la aos homens. Não deve ficar inventando mensagens, terias, filosofias para mostrar conhecimentos. 3. CONVICÇÃO. O pregador deve transmitir aquilo de que tem convicção, para que a mensagem seja aceita. Tem que viver aquilo que prega. IV - A BASE DA PREGAÇÃO (ou do sermão) 1. A PALAVRA DE DEUS A base da pregação deve ser a Palavra de Deus, a Bíblia Sagrada. Podemos dizer, em outras palavras que a base da pregação deve ser o TEXTO BÍBLICO . Ilustrações podem ser aproveitadas, desde que Que se relacionem com o tema da mensagem , mas não podem tomar o lugar da Palavra de Deus. Ouvimos um pregador que, não tendo êxito em \"abalar\" os ouvintes, apelou para uma história fantasiosa e tomou 80% do tempo destinado à mensagem. 2. QUE TEXTO ESCOLHER? O Pr. Elienai Cabral sugere (em resumo) 8 (oito) características para um bom tema a ser escolhido )p. 50-51). 3) Textos objetivos: que atendam às necessidades espirituais das pessoas (Com oração e unção). 4) Textos sobre os quais não haja dificuldade para a interpretação (hermenêutica). 5) Textos dentro dos limites de capacidade do pregador. 6) Textos que expressem o tema da pregação para não fugir ao objetivo. 7) Texto que desperte interesse (Com oração, o Espírito mostra o que deve ser pregado). 8) Textos cuja seqüência seja de fácil acompanhamento pelo pregador e pelo auditório. V - A ESTRUTURA DA PREGAÇÃO ( Do sermão) Toda pregação com esboço ou não, deve ser dividida, basicamente, em duas partes: 1. INTRODUÇÃO. É a parte inicial da mensagem, pela qual o pregador entra em contato com o auditório. Visa despertar o interesse pela pregação; \"prepara a mente dos ouvintes , para que possam compreender o assunto do sermão e as idéias a serem desenvolvidas...\" (Key, p. 31). Uma boa introdução deve ser BREVE, SIMPLES, INTERESSANTE E APROPRIADA. (Cabral, p. 66) Conhecemos um grande pregador que gasta 30 ou 40 minutos na introdução. Isso cansa, principalmente os descrentes. A introdução não deve ir além de 10 ou 15% do tempo da mensagem. (Normalmente, o pregador sabe de quanto tempo dispõe, exceto em casos especiais). 2. CORPO (ou desenvolvimento) DA MENSAGEM (Do sermão). É a parte mais importante da mensagem. Ela deve conter a seqüência das idéias a serem apresentadas. No corpo do sermão ou da mensagem , podemos ter: 1) Ordem ou divisões (1º , 2º, 3º , etc.); 2) Transição de um pensamento para outro. As divisões devem ser de acordo com os objetivos mensagem; devem-se evitar \" excesso de floreios\", \"rodeios\", ou \"conversa fiada\". O povo percebe. 3.CONCLUSÃO. É o auge da pregação. O seu clímax. Nela, o pregador faz a aplicação do que pregou no corpo do sermão. Nesse momento, o pregador e o auditório, pelo poder do Espirito Santo, devem chegar à conclusão de que a mensagem atingiu seu objetivo. Sem uma boa conclusão, o que foi dito pode perder o brilho. Uma conclusão pode ser feita através de: 1) Recapitulação. O pregador deve rever o que pregou, em resumo ou tópicos, evidenciando pensamentos-chave , pontos fortes da mensagem (Cabral, p. 70). 2) Narração. O pregador pode valer-se de um fato, uma rápida ilustração para comover o auditório, levando o descrente a uma decisão, na unção do Espírito Santo. 3) Persuasão . É a parte mais difícil da conclusão. Depende muito mais do Espírito Santo do que do pregador. Por isso, toda mensagem deve ter a unção do Espírito Santo. Para tanto, o pregador precisa orar muito, e até jejuar, diante de Deus, para que a mensagem atinja seu alvo. 4) Convite. Toda pregação deve terminar com um convite ou apelo, seja para pecadores, seja para a igreja. Um convite na unção do Espírito tem maravilhoso efeito no coração das pessoas. De acordo com Braga (p. 211-212), a conclusão deve ser breve e simples, e com palavras adequadas. Um certo jovem pregou numa igreja. Ao fazer o apelo, não vendo ninguém atender, passou a contar que alguém ganhou um grande prêmio porque deu uma grande oferta para a Obra. Desviou totalmente o alvo da mensagem. VI - TIPOS DE SERMÕES 1. SERMÃO TEMÁTICO (Ou Tópico). É aquele \"cujas divisões principais derivam do tema, independentemente do (Braga, p.17). É aquele em que as divisões principais do derivadas de um TEXTO constituído de UMA BREVE PORÇÃO DA BÍBLIA ( Braga, p. 30). Exemplo: Titulo: \"O Único Caminho Para Deus\" (Jo 14.6). 1) Através de Jesus, o único caminho. 2) Através de Jesus, a verdade. 3)Através de Jesus, a vida. 3. SERMÃO EXPOSITIVO É aquele em que as divisões baseiam-se numa porção mais extensa (texto) da Bíblia, não abrangendo \"um só versículo, mas uma passagem, um capítulo, vários capítulos, ou mesmo um livro inteiro\" (Cabral, p. 78). Nele , é mostrada (exposta) uma verdade contida num texto bíblico. Exige tempo, estudo e conhecimento bíblico. É o que caracteriza uma pessoa e a torna diferente de outra. \"É tudo quanto o indivíduo é\". Na pregação, o pregador demonstra que tem personalidade, quando se expressa, falando ou gesticulando, de acordo com aquilo que ele é e não imitando outras pessoas. De vez em quando, percebe-se pregadores , imitando evangelistas famosos, dando gritos, pulando e correndo no púlpito, torcendo o pescoço, ajeitando a gravata, falando rouco ou estridente. Isso é falta de personalidade. É querer ser ator, imitador e não um instrumento nas mãos do Espírito Santo. É o sentimento de devoção e amor pelos outros e pelas coisas de Deus. O pregador deve sentir pelo Espírito as necessidades do auditório, principalmente dos pecadores. (1 Tm 4.8; Hb 12.28). 2) Devoção É o sentimento religioso, de dedicação às práticas ensinadas na Palavra de Deus. Na devoção, o pregador busca inspirar-se na ORAÇÃO, na LEITURA DA BÍBLIA, e no LOUVAR A DEUS. Temos visto verdadeiros profissionais da pregação, técnicos, que sabem pregar, mas não sabem orar; sabem gritar, mas não sabem amar as almas. Pregam por interesse, por torpe ganância. Que os jovens pregadores (e os antigos) não entrem por esse caminho. Conta-se que Moody, o grande evangelista, orava uma hora para pregar cinco minutos. Enquanto isso, temos pregadores que oram cinco minutos para pregarem uma hora! 3) Sinceridade Reflete a verdade contida na própria alma. O pregador deve pregar aquilo que vive e viver aquilo que prega (Tg 2.12). Um jovem, dirigente de Mocidade, pregava bem. O povo se alegrava. Mas, um dia, uma jovem descrente procurou a direção da igreja para dizer que estava grávida dele e, o pior, o jovem não assumiu a paternidade. Por fim, confessou o pecado, foi excluído, e contribuiu para uma alma descrer do evangelho. 4) Humildade \"Nenhum pregador pode subir ao púlpito sem antes ter descido, pela oração, os degraus da humildade. Na oração, o egoísmo se quebranta. O medo se desfaz, e a certeza da vitória aparece clara como a luz do sol ao meio-dia\" (Cabral, p. 43). (Ler Pv 15.33). Um jovem vivia criticando quem ia pregar, dizendo que, se fosse ele, pregaria muito melhor. Um dia, o pastor deu oportunidade ao moço para pregar. Ele subiu ao púlpito, orgulhoso, sorridente. Tentou achar um texto na Bíblia, de um lado para outro, e nada. Suou, pediu desculpa, e desceu cabisbaixo. Sentou noutro lugar, junto a um irmão experiente, que, percebendo sua tristeza, disse: \"Moço, se você tivesse subido como desceu (humilde), teria descido como subiu (alegre)\". E uma grande lição para todo pregador. 5) Poder O pregador (jovem ou não) precisa do Poder de Deus. S. Paulo disse que não pregava sabedoria humana, mas com poder (1 Co 1.4-5). É preciso ter unção e graça para pregar. Do contrário, ocupa-se o púlpito e o tempo para dizer coisas inoportunas. E melhor um sermão fora da Homilética, mas na unção de Deus, do que dentro da técnica, e sem poder. Isso só se consegue com oração, jejum, leitura bíblica, e vida consagrada. Não se obtém num curso de Homilética. Um mosaico com inscrições em hebraico contando a histórica bíblica de Sansão foi encontrado na Galiléia (Israel), nas ruínas de uma sinagoga do século 4 a.C. A escavação foi conduzida pelo Dr. Jodi Magnees, da North Carolina University (EUA), em parceria com arqueólogos da Israel Antiquities Authority (Autoridade de Antiguidades de Israel). A descoberta ocorreu perto do kibbutz Hokuk, local que várias fontes talmúdicas se referem como sendo a Hokuk histórica, onde rabinos se reuniam para escrever o Talmude (livro sagrado dos judeus). Um livro do rabino mediveal Ashtori Ishtori, do século XIV, faz referência a uma sinagoga na mesma área onde as descobertas foram feitas. Segundo o dr. David Amit, da Autoridade de Antiguidades de Israel, o mosaico \"contém uma descrição do Sansão bíblico e dois pares de raposas com uma tocha flamejante ligando suas caudas\", como descrito nas obras de Sansão no Livro dos Juízes.Read more... \"Não dizeis vós: Ainda há quatro meses até que venha a ceifa? Ora eu vos digo: Levantai os vossos olhos, e vede os campos, que já estão brancos para a ceifa. Quem ceifa já está recebendo recompensa e ajuntando fruto para a vida eterna; para que o que semeia e o que ceifa juntamente se regozijem\" João 4:35,36. Logo após se encontrar com a mulher samaritana e ela ter saído para evangelizar sua cidade, Jesus se encontra com os seus discípulos e estes insistem para que ele coma a comida que eles trouxeram da cidade. Jesus se recusa a comer e passa a ensinar-lhes verdades espirituais sobre o tempo da colheita de Deus. (as vezes precisamos recusar algumas comidas, mesmo que pareçam muito boas e tenham sido trazidas a nós por pessoas com as melhores intenções). Calendários diferentesJesus disse: No calendário de vocês, ainda faltam quatro meses para a colheita, no calendário de Deus, o tempo é agora. É possível que a nossa forma de ver as coisas possa estar diferente da de Deus. Temos discernido que este é um tempo de colheita, mas alguns talvez estejam pensando: Falta muito tempo, não se preocupe, não esquenta a cabeça, temos outras prioridades. Queridos, este é o tempo de Deus para colheita! Com mais ou menos tempo de igreja, com muito preparo ou pouco, com situações pessoais todas resolvidas ou ainda algumas dificuldades para resolver, este é um tempo de colheita e todos poderão participar. A mulher samaritana tinha muitas dificuldades ainda, mas isso não impediu que ela largasse o cântaro e fosse colher vidas para Jesus. Olhos voltados para as coisas terrenasJesus disse: \"Levantai os vossos olhos\", olhos que precisam ser levantados, são olhos que não estão focados onde deveriam estar. Olhos tem relação com o coração. Onde o meu coração está? Nas coisas que desejo? no meu estilo de vida que não pode ser mudado? Nos meus conceitos que são perfeitos e indiscutíveis? Nas coisas terrenas? A recomendação de Jesus é que os nossos olhos estejam nos campos do Senhor, mas somente quem ergue os olhos para olhar firmemente para Jesus é que pode ver e discernir o tempo e perceber que uma grande colheita já está preparada. Mas se o nosso calendário (propósitos e desejos do coração) não estiver alinhado com o do Senhor, nada acontecerá. Nem em sua vida, nem na vida dos que precisam ser alcançados.Semeadura e colheita, só se realizam quando arde no coração o desejo de plantar e colher, quando os olhos do coração estão focalizados no propósito de Deus. Fazendo a coisa certa no lugar certoA ordem de Jesus é semear e colher, não adianta dizer: Senhor, eu trabalho na fazenda, mas só quero cuidar do gado. Em nossa igreja, plantamos e colhemos principalmente através dos grupos familiares, é por isso que líderes não devem somente administrar o grupo ou simplesmente cuidar, mas pastorear e investir plantando coisas do reino de Deus na vida dos seus liderados. O grupo é o celeiro e lugar de treinamento para novos ceifeiros. É o lugar onde recebemos diversos tipos de sementes diferentes ( fé, amor, autoridade, unidade...), traçamos as estratégias apropriadas para plantarmos na vidas das pessoas as coisas do reino de Deus, é onde consideramos a forma de plantio, os adubos a irrigação, os cuidados com as ervas daninhas, é lugar de preparo e encaminhamento (encontro, café com leite puro, discipulado, curso preparatório, veredas, diaconato, treinamento de líderes....) deve haver unidade de propósito e de visão entre os ceifeiros, do contrário a colheita não acontece. Alcançando o resultado finalSemear e colher não acontece por acaso. É necessário um processo. O processo realizado corretamente é o que nos permite chegar ao resultado final. No grupo familiar devemos agir como garimpeiros procurando as pedras preciosas. Primeiro o garimpeiro sonda o terreno, analisa as barrancas do rio, as suas areias, o seu aspecto. Na esperança de que algo de muito valor possa sair dali, ele monta o acampamento (decide investir o seu tempo naquele lugar). Então, entra na água e começa o processo de procura ( pode ser longo). É um processo sistemático mas que deve ser feito com toda atenção ( no descuido podemos perder pedras preciosas). Ele está disposto a enfrentar as dificuldades (friagem no corpo pelo contato com a água fria, dor nas costas, sol na cabeça, picadas de inseto...) desconforto é um preço a ser pago. Mas eis que as pedras começam a surgir na bateia. Hoje uma, daqui a dois meses, duas, depois outras e ele vai encontrando seu tesouro (vidas). Geralmente saem da areia e do barro sujas, sem forma bonita e sem brilho. Mas, logo em seguida começa a fase de lavagem e formação (encontro, libertação, curso preparatório...) e depois a lapidação e polimento (pressões e experiências com Deus). Destino final: Ser parte de uma jóia com muitas outras pedras que refletem a luz da glória de Deus. O local do seu grupo familiar é o canto do riacho que o Senhor te deu para garimpar, as pedras são pessoas esperando para serem alcançadas. Peça a Deus sabedoria, força, direção e entusiasmo para cumprir o propósito. O papa Bento XVI iliba totalmente o povo judeu da morte de Jesus Cristo, um dos assuntos mais controversos do cristianismo, num novo livro de que foram hoje publicados. No livro, intitulado \"Jesus de Nazaré\", Bento XVI recorre a uma análise bíblica e teológica para explicar por que não é verdade que o povo judeu no seu conjunto seja responsável pela morte de Jesus. Embora o Vaticano sustente há cinco décadas que os judeus não foram coletivamente responsáveis, académicos judeus ouvidos pela agência noticiosa norte-americana AP consideraram que o argumento agora exposto pelo papa é significativo e vai contribuir para combater o antissemitismo. \"Há uma tendência humana natural para aceitar as coisas como verdadeiras e muitas vezes isto leva a erros de perceção\" quanto aos riscos de antissemitismo, considerou o rabi David Rosen, responsável para os assuntos inter-religiosos do comité judaico americano que há vários anos lidera o diálogo entre as duas religiões. Segundo Rosen, o Vaticano divulgou em 1965 a sua nota mais autorizada sobre o assunto, \"Nostra Aetate\", nota que revolucionou as relações da Igreja Católica com o judaísmo ao afirmar que a morte de Cristo não pode ser atribuída ao povo judeu nem na altura nem atualmente. Para o rabi, as palavras de Bento XVI podem representar um marco mais importante e duradouro na medida em que os crentes tendem a ler mais as escrituras e artigos ou livros do que os documentos da Igreja, especialmente os mais antigos. Este é o segundo volume de \"Jesus de Nazaré\" de Bento XVI lançado em 2007, o primeiro livro que lançou como papa, sobre os primeiros anos da vida e dos ensinamentos de Jesus Cristo. Este segundo volume, com lançamento previsto para 10 de março, é sobre a segunda parte da vida de Cristo. Quatro cristãos foram presos em Laos por serem surpreendidos pela polícia enquanto tentavam explicar a Bíblia para um homem. Dois deles eram cidadãos tailandeses. O caso aconteceu na aldeia de Luang Namtha na segunda quinzena do mês de junho e só foi divulgado dias depois por agências internacionais. Os relatos afirmam que supostamente um ex-policial ficou chocado por ver esses homens citando a Bíblia e resolveu denunciar. Os presos estavam dentro de uma casa fazendo visitas e falando sobre o Livro Sagrado para o morador. \"Quando a polícia chegou eles procuraram e acharam entre os pertences desses cristãos discos de história da Bíblia e um livro de figuras em preto e branco. A medida da polícia foi tomar os passaportes de todos eles\", disse Prasertsee que é diretor da ONG HRWLRF que defende dos direitos humanos. \"De noite a polícia voltou para prendê-los e os transportaram para a prisão\", continuou Prasertsee. \"Todos os pertences, inclusive dinheiro, fones e coisas pessoas foram confiscados\", denuncia.As famílias estão preocupadas com os prisioneiros, dois dos tailandeses tiveram os nomes divulgados: Phanthakorn, de 40 anos, e Wiwardamrong, de 54. A preocupação se dá pelo fato de que em Laos os prisioneiros não recebem alimento, é a família deles quem fornecem comida. \"As famílias estão se esforçando para ter contato com os presos, mas as autoridades de Laos tem dificultado essa relação\", afirma o direto da HRWLRF. hebraico TRANSLITERAÇÃO DO GREGO É possível fazer a transliteração de qualquer palavra constituída pelo alfabeto grego para o latino e vice-versa. ARQUEOLOGIA E TEOLOGIA pastor paulo Mestrado em Ciências da Religião; Bacharel em Teologia; Psicologia Pastoral; Doutor em Teologia, Doutor Honoris causa em Teologia Sistemática; Psicanálise Clinica com Mestrado em Psicanalise;É bacharel em linguística e hebraico e mestre em hebraico. Professor da área bíblica e de hebraico do Seminário, em São Paulo.\n",
            "Comissão de Construção da Nova Sede GOB. A Comissão de Construção da Nova Sede do Grande Oriente do Brasil - Minas Gerais disponibiliza esta área para que os IIr.'. Maçons, devidamente regularizados, possam acompanhar todo o processo de construção da nova sede do GOB-MG; inclusive esclarecendo possíveis dúvidas que possam existir. Acesse a página da SMI - Sociedade Maçônica de Investimentos S/A e vejam todo conteúdo.\n",
            "06/12/2011 MPT realiza audiência pública e alerta para aumento de acidentes na construção civil 180°.com/PI A construção civil passa por um \"boom\" na cidade de Picos, especialmente na iniciativa privada. Motivado pela necessidade de debater as normas de segurança para os funcionários da construção civil, o Ministério Público do Trabalho do Piauí promoveu na tarde desta sexta-feira (02) uma audiência pública na Câmara Municipal. Além de explanar sobre as normas de segurança a serem seguidas pelos trabalhadores, os procuradores do MPT também alertaram para o aumento de acidentes registrados na construção civil local. \"Agente observa, aqui, que a maioria das obras não se segue as normas mínimas de segurança do trabalho, especialmente a Norma Regulamentada N° 18, que é a norma do Ministério do Trabalho para a construção civil, e o que observamos é um crescente número de acidentes de trabalho e exposição de risco a vida dos trabalhadores\", explicou o procurador do Ministério Público do Trabalho, Carlos Henrique Pereira Leite. Dentre as irregularidades mais graves e constatadas com mais freqüência pelo MPT, estão: trabalho em altura sem proteção e trabalho com instalação elétrica. \"Em que trabalham pessoas não qualificadas e sem a utilização dos equipamentos de proteção individual e coletiva\", informou. Carlos Henrique alerta que o dono de uma obra pode ser responsabilizado caso ocorra um acidente de trabalho. Ele informou que as denúncias ainda são subnotificadas, devido em parte a ausência de um sindicato que lute pelos direitos dos trabalhadores da construção civil em Picos. \"É oportuno lembrar que não há sindicato da construção civil na cidade de Picos\", declarou. A chefe de fiscalização da Superintendência Regional do Trabalho, Soraya Lima declarou que há empenho por parte da Superintendência, mas que esse é um momento de \"boom\" da construção civil em que há um grande número de obras e o consequente aumento das contratações. \"Temos de reconhecer que ainda falta muito para atingir o ideal na construção civil\", declarou. A audiência pública era voltada para os trabalhadores da construção civil e também para os empregadores, oportunidade em que as normas e leis foram explicadas, mas infelizmente a audiência registrou um comparecimento reduzido de pessoas.\n",
            "Usando como exemplo um pneu com código \"P175/70 R14 86V\" já sabemos que é para um veículo de passeio, pois começou com a letra P. Já o código 175/70 é o tamanho do pneu: o número 175 significa a largura de rodagem, medida em milímetros; já o 70 é a altura do pneu, indicando que tem 70% da largura de rodagem – nesse caso 122,5mm. A letra R que vem em seguida significa que o pneu é radial, ou seja, tem uma malha de aço com vergalhões paralelos e transversais. Esses pneus operam sem câmara. O número 14 indicado no código do pneu indica o raio da roda: dependendo do seu veículo, mesmo sendo de passeio, o raio da roda pode ser modificado. Se desejar alterá-lo por qualquer motivo, terá que trocar as rodas também. Por fim, o número 86 significa o índice da carga do pneu, ou seja, ele diz o quanto de peso o pneu aguenta.Aqui, um pneu 86 suporta 530 kgs; como são usados quatro deles num automóvel, ele suporta 2.120 kgs – um peso de carro popular. Existem sites de venda de pneus online que fornecem os tamanhos ideais de aros de pneus conforme a marca do seu carro. Como Cuidar dos Pneus do seu Carro Para que os seus pneus durem mais, é importante mantê-los calibrados no nível correto informado no manual do proprietário, assim como fazer o alinhamento e balanceamento periodicamente. A calibragem deve acontecer a cada 15 dias. Tente memorizar o nível de calibragem para facilitar esse processo. O modo de dirigir também conta: evite aceleradas e freadas bruscas. Vá com calma. Dicas para a Hora de Trocar os Pneus Desgastados Mesmo tomando esses cuidados necessários, eventualmente chega o momento da troca dos pneus por simples desgaste. Uma dica para reduzir o impacto no orçamento é trocá-los aos poucos: compre apenas dois e posteriormente substitua os outros dois. Lembre-se de que o eixo traseiro tem a preferência dos pneus novos. Aproveite e coloque bicos e válvulas novos também. Inovação: Pneus Verdes Recentemente a marca de pneus Pirelli lançou no mercado os chamados \"pneus verdes\". Eles contam com a tecnologia Novateck, desenvolvida especialmente para sua própria reconstrução. Ou seja, ao adquirir o produto, o consumidor, através de reformadores credenciados pela empresa, pode reformar o seu pneu quando desgastes ocorrerem. A marca oferece a mesma qualidade do produto original e garantia de fábrica até a terceira reforma do seu pneu. É um diferencial que a empresa buscar perante os seus clientes e uma forma sustentável de contribuir com o meio ambiente.\n",
            "Utiliza-se quando a ação não ocorre por intervenção deliberada de um agente, mas acidentalmente. Usa-se se + pronome átono + verbo na 3ª pessoa (concordando com o substantivo). ¡Cuidado! ¡El perro se te está escapando! (Cuidado! O cachorro está escapando!) Se me rompió la taza. (A xícara quebrou.) Se nos cayeron los relojes. (Os relógios caíram.) FORMAS ÁTONAS: lo / la / los / las Empregam-se sempre como complemento direto, ou seja, substituindo objetos diretos. 1) Llama un taxi, por favor. (Chama um táxi, por favor.) Llámalo, por favor. (Chama-o, por favor.) 2) Visitaré a mi familia en mis vacaciones. (Visitarei minha família em minhas férias.) La visitaré en mis vacaciones. (Eu a visitarei em minhas férias.) 3) ¿Has encontrado a tus amigos? (Encontraste teus amigos?) No, los estoy buscando. (Não, estou procurando-os.) Atenção! Diferentemente do português, em espanhol os objetos diretos de pessoa ou coisa/animal personificado aparecem precedidos da preposição a. Repare nos exemplos dados acima: os objetos diretos precedidos da preposição a (segundo e terceiro exemplo) são objetos de pessoa, enquanto que o objeto direto não preposicionado (primeiro exemplo) refere-se à coisa não personificada.\n",
            "ATUAÇÃO CINEPASS Uma equipe de alemães está por trás deste novo mercado de vendas de ingressos de cinema. Eles planejam ocupar os 83% de assentos não utilizados nas extremidades traseiras das salas de cinema. Por meio do website, oferecem ofertas exclusivas e preços acessíveis de ingressos e combo de alimentos. Home Página do Filme Página com dados técnicos do filme como Descrição, Duração, Gênero e Elenco. Além das informações sobre o filme, o cliente realiza um filtro de salas de cinema por cidade. Página Promocional Filmes promocionais de determinada sala de cinema. Combos Promocionais O cliente escolhe o combo promocional para determinado filme com data e hora agendada.\n",
            "Agentes de saúde fazem manifestação por melhores condições de trabalho Os agentes de endemias e agentes comunitários da saúde se reuniram hoje pela manhã (17) em uma manifestação na porta da prefeitura reivindicando melhores condições de trabalho. Segundo os representantes do Sindicato dos trabalhadores do Sistema único de Saúde no Estado de Goiás (Sindisaúde) os funcionários cobram melhores salários, além de outros benefícios como vale transporte, equipamentos de proteção individual e uniformes. O sindicato tentou por várias vezes se reunirem com a ex-secretaria de saúde e o prefeito, mas as tentativas não chegaram a lugar algum, o que causou a manifestação. O movimento que começou na porta da prefeitura se estendeu em uma passeata até a Secretária de Saúde, onde estava sendo empossado o novo secretario municipal de saúde. De acordo com Orlando Pedro, um dos cabeça da manifestação, o salário de um agente de endemias é de R$ 779,00, considerado baixo quando relacionado as despesas que cada um tem. \"Tem gente que trabalha em Goiânia e tem que tirar o vale transporte do próprio bolso. Quem fica em áreas mais longes e tem moto, ou carro, também tira do salário para fazer o seu serviço\". Fato este confirmado por uma agente de endemia que diz retirar todo mês do salário mais de R$ 100 só para cumprir sua rota de trabalho. A ex-secretária de saúde, Sônia Maria Martins, disse em uma entrevista de que os funcionários da saúde \"trabalhavam no céu\" e justificou dizendo que até o protetor solar era um dos melhores. Já os agentes discordam da declaração e alegam que poucos recebem o protetor solar, e que o filtro não é o essencial, e trabalham com os mesmos uniformes distribuídos há mais de dois anos. Leocides José Souza, diretor do interior e regional do SindiSaúde, disse que a categoria precisa lutar por um plano de carreira no município e que há uma portaria do Ministério da Saúde que o repasse para os agentes comunitários seria de R$ 950,00 , sendo a diferença entre o salário base e a portaria (algo em torno de R$200) investidos na melhoria de condições dos trabalhadores ou em repasse financeiro como complemento salarial, porém nenhum dos dois é feito na cidade. \"A única coisa que eles dizem ter feito com este valor é comprar algumas bicicletas para os agentes e que todo mês há a manutenção, mas isso não basta. Se há investimentos, prove através de notas que uma melhoria tem sido buscada\", completou o diretor. O prefeito tentou conversar com os agentes antes da manifestação, mas os representantes do sindicato não fariam parte da reunião, o que foi negado. Já no final da passeata, que percorreu as ruas dizendo \"saúde na rua, prefeito a culpa é sua\", Miller Assis conversou com os agentes e disse que tem feito o que a lei permite e o limite do possível para dar condições aos trabalhadores. Ele pediu que as reinvindicações fossem entregues ao novo secretário de saúde, Dirley Côrrea, e que em um prazo breve novas reuniões seriam marcadas. O prefeito disse que o salário dos agentes foi reajustado neste ano, assim como manda a lei e que em um consenso com Conselho Municipal de Saúde decidiu cortas as gratificações recebidas por alguns funcionários, por entender que deve haver isonomia entre os agentes. Em relação aos protetores solares Miller disse que este material já foi comprado, mas não disse quando seria entregue, e que os uniformes estão em fase de licitação. A diretora financeira do Sindisaúde se prontificou a acompanhar as negociações do município e que há outras cidades com menor arrecadação que já pagam o piso salarial de R$ 930,00. Galeria do post Comentários0Comentar!Este artigo ainda não possue comentários. Seja o primeiro a comentar!\n",
            "O governo da província de Misiones, na Argentina, já investiu R$ 13,5 milhões para revitalizar a orla de Puerto Iguazú, às margens do Rio Iguaçu: iniciativa serve de inspiração para o projeto Beira FozFronteira Proposta quer urbanizar orla de Foz Projeto Beira Foz prevê instalação de parques, restaurantes, avenidas e hotéis nas margens do rios Paraná e Iguaçu para combater a criminalidade Exemplo \"Costanera\" trouxe paz e progresso na margem argentina A ocupação das margens dos rios para reduzir a criminalidade e proporcionar bem-estar aos moradores já é uma realidade na Argentina. Em Puerto Iguazú, cidade vizinha a Foz do Iguaçu, o governo da província de Misiones investiu R$ 13,5 milhões para remodelar a orla do rio. A \"costanera\" argentina, como é chamada pelos brasileiros, tem restaurantes, parques, pistas de caminhada e internet gratuita. Omar Rodriguez, diretor de Turismo em Puerto Iguazú, diz que, antes de a orla ser revitalizada, o contrabando de mercadorias do Paraguai via Rio Iguaçu era intenso. \"Agora é paz. A costanera trouxe paz e progresso\", afirma. A orla de Puerto Iguazú é bem menor se comparada à brasileira – são três quilômetros – e a primeira etapa das obras, no trecho de 1.500 metros, está praticamente pronta. Os argentinos aguardam a finalização da construção de restaurantes e bares, um deles já confirmado é o Hard Rock Café em um local cedido pela prefeitura em sistema de concessão pública. Nova etapa Rodriguez adianta que uma segunda etapa da obra está prevista com a ocupação das margens do Rio Iguaçu até a entrada do parque nacional. Entre esses dois pontos já existe uma área de 600 hectares onde está sendo construído um conjunto de hotéis, entre eles, um da rede Hilton. Alguns deles, situados em meio à floresta, já funcionam. A cidade de Posadas, capital de Misiones, tem uma orla exemplar. Situada na fronteira com Encarnación, no Paraguai, a cidade, às margens do Rio Paraná, tem uma extensa orla, com praia artificial, calçadão e restaurantes. Urbanização Novas avenidas aproximam comunidade de rios Um dos aspectos mais importantes do projeto Beira Foz é a reintegração dos moradores à orla. \"Em todo lugar do mundo o rio é o elemento principal para uma cidade, e aqui é o problema. Mas agora vamos resgatar esse potencial\", diz o engenheiro Fábio Prado, representante do Conselho Consultivo da Associação Comercial e Industrial de Foz do Iguaçu (Acifi) e da União Dinâmica de Faculdades Cataratas (UDC) no projeto. Para possibilitar a integração entre a população e os rios, o projeto prevê a construção de avenidas e pistas de caminhada às margens dos Rios Iguaçu e Paraná. A ideia é fazer três traçados. O primeiro, chamado Beira Rio PR-Norte, começa no trevo da Vila C, região da Itaipu Binacional, e termina na Ponte da Amizade. O segundo se estende da ponte até o Marco das Três Fronteiras. E o terceiro liga o marco à entrada do Parque Nacional do Iguaçu. Esse braço do projeto, chamado de Sistema Viário, é coordenado pela Itaipu e pelo Parque Tecnológico de Itaipu (PTI). Entre os locais públicos, estão contemplados no projeto a construção do Parque da Memória, da torre dos 100 anos, em alusão ao centenário de Foz em 2014, a revitalização do Marco das Três Fronteiras, da Colônia de Pescadores, além de pistas para caminhadas e ciclovias. Outros espaços serão ocupados pela iniciativa privada com a construção de hotéis, bares e restaurantes. Aduana Prado lembra que a reurbanização da orla do Rio Paraná tem relação com um movimento iniciado na fronteira em 2001, junto com a Receita Federal, para a ampliação da aduana da Ponte da Amizade, lado brasileiro, cuja obra foi entregue em 2007. Na sequência, a aduana paraguaia também foi reformulada, o que melhorou a receptividade aos turistas nas alfândegas de Foz e Ciudad del Este. Um projeto de iniciativa pública e privada pretende mudar a paisagem da orla do Rio Paraná, na fronteira do Brasil com o Paraguai. Hoje, a área entre a barragem da Itaipu Binacional e o Marco das Três Fronteiras é cercada por portos clandestinos e moradias irregulares, usados pelo contrabando para trazer mercadorias, drogas e armas para o Brasil. A proposta para alterar essa realidade é ocupar os 21 quilômetros de margens do rio com parques, restaurantes, avenidas e hotéis. A ideia – chamada de Projeto Beira Foz – também se estende às margens do Rio Iguaçu, entre o Marco das Três Fronteiras e a entrada do Parque Nacional do Iguaçu, em um trajeto de 17 quilômetros. Além de criar condições para a população de Foz se aproximar e usufruir dos rios, a intervenção urbanística é uma aposta contra a criminalidade. \"O projeto em si é de fundamental importância para mudar não só a cara de Foz do Iguaçu, mas a segurança de todo o país\", opina o delegado-chefe da Polícia Federal (PF) em Foz do Iguaçu, Guilherme de Biagi. Segundo ele, com a ocupação da orla, a própria sociedade se torna um agente fiscalizador. Atualmente, a orla do Paraná tem pelo menos 12 conglomerados de moradias irregulares e uma população de quase mil pessoas. Em meio às residências é comum ver imóveis de fachadas que são depósitos de drogas, cigarros e produtos contrabandeados. O povoamento da beira rio é importante, segundo Biagi, porque a PF pode direcionar o efetivo que hoje patrulha o Rio Paraná para pontos nevrálgicos da fronteira, como o Lago de Itaipu. Ao longo do reservatório, de 1.350 quilômetros, a polícia já mapeou 3 mil miniportos e picadas na mata usados pelos criminosos. Três eixos O Beira Foz está dividido em três eixos: segurança e justiça; habitação e promoção social; e sistema viário. Em relação à segurança, a proposta é instalar três bases ao longo do Rio Paraná chamadas Unidades de Policiamento de Fronteira (UPFron). As estruturas blindadas que serão usadas pela Marinha, PF, Polícia Am­biental e Receita Federal estão sendo projetadas no Parque Tecnológico de Itaipu (PTI), uma das parceiras da proposta. Câmeras para monitoramento eletrônico com visão noturna também auxiliarão o trabalho policial. O projeto envolve os governos federal e estadual, prefeitura de Foz e já tem aval dos Ministérios da Justiça e do Meio Ambiente. Ainda não há estimativa de custo para implementá-lo, mas o valor deve superar R$ 300 milhões. Uma comissão com representantes de instituições públicas e privadas, incluindo a Itaipu Binacional, União Dinâmica de Faculdades Cataratas (UDC) e Associação Comercial e Industrial de Foz do Iguaçu (Acifi), foi formada para levar a proposta adiante. O diretor-superintenden­te da Fundação PTI Juan Sotuyo diz que o Beira Foz vai atrair investimentos privados e beneficiar a cidade. \"O custo benefício é maior que o prejuízo gerado hoje pelo tráfico e contrabando\", ressalta. O mesmo movimento deve ocorrer no Paraguai, segundo Sotuyo. O governo paraguaio acompanha o desenvolvimento do projeto para montar uma estrutura semelhante do outro lado da fronteira, nas cidades de Hernandárias, Ciudad del Este e Presidente Franco, margeadas pelo Rio Paraná. O que você acha O Beira Foz vai conter o trânsito de criminosos na tríplice fronteira? Por quê? Assine a Gazeta do Povo A Cobertura Mais Completa Assine o plano completo da Gazeta do Povo e receba as edições impressas todos os dias da semana + acesso ilimitado no celular, computador e tablet. Tenha a cobertura mais completa do Paraná com a opinião e credibilidade dos melhores colunistas!\n",
            "Ameaçados de punição severa, os ex-palmeirenses Maurício e Obina puderam respirar aliviados, nesta quarta-feira, após o resultado de seus julgamentos no STJD. O zagueiro foi suspenso por três partidas, enquanto o atacante por duas. Os jogadores, que haviam sido denunciados no artigo 253 do Código Brasileiro de Justiça Desportiva (agressão física) por conta da briga em que se envolveram no intervalo da partida entre Palmeiras e Grêmio, no Olímpico em Porto Alegre, pela 36ª rodada do Campeonato Brasileiro, corriam o risco de pegar gancho de 120 a 540 dias. A confusão aconteceu durante uma discussão a respeito do lance que originou o primeiro gol dos gremistas. Antes do início do segundo tempo, ambos foram expulsos e os gaúchos venceram o duelo por 2 a 0. As informações são do site \"Justiça Desportiva\". A defesa de Maurício - que deverá ser emprestado - foi feita pelo advogado do Palmeiras, Rafael Pestana, que negou a agressão e ressaltou o arrependimento do atleta, que teria deixado o estádio chorando. O defensor teve acatado o pedido para que a denúncia fosse desclassificada para o artigo 255 (ato de hostilidade) - que tem pena máxima de três partidas. Obina foi defendido por Michel Assef Filho, advogado do Flamengo - para onde o atacante deverá voltar em 2010. O tribunal desclassificou a denúncia do jogador para o artigo 258 do CBJD (atitude antidesportiva). Assef sustentou que Obina teria reagido a uma ameaça de agressão e usou as imagens para desmentir a súmula do árbitro, afirmando não ter havido troca de socos.\n",
            "Autoridades participam de reabertura dos trabalhos legislativos Os trabalhos legislativos da Assembleia Legislativa de Mato Grosso do Sul foram reabertos na manhã desta terça-feira (2/2), com a solenidade de instalação da 2ª Sessão Legislativa da 10ª Legislatura. O evento, que reúne várias autoridades, teve início às 8h50, no Pavilhão das Bandeiras, entrada principal do Palácio Guaicurus, no Parque dos Poderes. A cerimônia segue um protocolo determinado pelo Regimento Interno da Casa de Leis. Na chegada ao Palácio Guaicurus, o governador Reinaldo Azambuja passou em revista a tropa formada pela Polícia Militar, sendo recebido logo depois pelo presidente Junior Mochi (PMDB) e demais deputados. Prosseguindo o rito, as Bandeiras foram hasteadas. A expectativa com relação a economia foi ressaltada por algumas das autoridades presentes. \"A união existente em nosso Estado, sob o comando do governador Reinaldo e com apoio da Assembleia e Tribunal de Justiça, nos dá esperança que vamos superar a crise econômica e trilhar o melhor caminho possível\", afirmou o desembargador João Maria Lós, presidente do Tribunal de Justiça de Mato Grosso do Sul. Presidente do Tribunal de Contas do Estado, o conselheiro Waldir Neves destacou a importância da relação harmônica entre os Poderes para enfrentar a crise econômica. \"Na atual conjuntura econômica vivida pelo País, os Poderes estão unidos para superarem as dificuldades. Otimismo, garra e vontade são fundamentais. A relação harmônica ajuda muito neste momento\". O Procurador- Geral de Justiça, Humberto de Matos Brites, também comentou sobre relação institucional com a Casa de Leis . \"Sempre tivemos uma relação institucional muito boa com a Assembleia. O Ministério Publico está aqui para desejar um excelente ano legislativo, não obstante a crise que vivemos, temos a certeza que os deputados irão cumprir a risca todas as suas atribuições legais\". Os deputados e autoridades já se dirigiram ao Plenário Júlio Maia para o início da sessão solene. O governador fará a leitura da Mensagem Constitucional de prestação de contas do Poder Executivo referente ao exercício de 2015. Ainda acontecerá o pronunciamento do presidente Junior Mochi e os deputados farão uso da palavra pelo Protocolo. Permitida a reprodução, desde que contenha a assinatura \"Agência ALMS\".Crédito obrigatório para as fotografias, no formato \"Nome do fotógrafo/ALMS\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n",
        "    block_size: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Optional input sequence length after tokenization. \"\n",
        "                \"The training dataset will be truncated in block of this size for training. \"\n",
        "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    validation_split_percentage: Optional[int] = field(\n",
        "        default=5,\n",
        "        metadata={\n",
        "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
        "        },\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    keep_linebreaks: bool = field(\n",
        "        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.streaming:\n",
        "            require_version(\"datasets>=2.0.0\", \"The streaming feature requires `datasets>=2.0.0`\")\n",
        "\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\""
      ],
      "metadata": {
        "id": "_wRPVXRJvxby"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    training_args = TrainingArguments(output_dir='opt-PT')\n",
        "    training_args.resume_from_checkpoint = True\n",
        "    training_args.do_train = True\n",
        "    training_args.do_eval = True\n",
        "    \n",
        "    if training_args.should_log:\n",
        "        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "    log_level = training_args.get_process_log_level()\n",
        "    datasets.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.enable_default_handler()\n",
        "    transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    print(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    print(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Detecting last checkpoint.\n",
        "    last_checkpoint = None\n",
        "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
        "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
        "            raise ValueError(\n",
        "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
        "                \"Use --overwrite_output_dir to overcome.\"\n",
        "            )\n",
        "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
        "            print(\n",
        "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
        "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
        "            )\n",
        "\n",
        "    # Set seed before initializing model.\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    data_args = DataTrainingArguments(train_file = 'sample-1gb.txt')\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
        "    # 'text' is found. You can easily tweak this behavior (see below).\n",
        "    #\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    \n",
        "    data_files = {}\n",
        "    dataset_args = {}\n",
        "    data_files[\"train\"] = data_args.train_file\n",
        "    extension = (\n",
        "        data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.train_file is not None\n",
        "        else data_args.validation_file.split(\".\")[-1]\n",
        "    )\n",
        "    extension = \"text\"\n",
        "    dataset_args[\"keep_linebreaks\"] = data_args.keep_linebreaks\n",
        "\n",
        "    model_args = ModelArguments(cache_dir='.', model_type=\"opt\", \n",
        "                                model_name_or_path=\"facebook/opt-125m\", \n",
        "                                #TODO: Revisar a questão do fast tokenizer\n",
        "                                use_fast_tokenizer=False)\n",
        "\n",
        "    raw_datasets = load_dataset(extension, data_files=data_files,\n",
        "      cache_dir=model_args.cache_dir, \n",
        "      use_auth_token=True if model_args.use_auth_token else None, \n",
        "      **dataset_args,\n",
        "    )\n",
        "    # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
        "    if \"validation\" not in raw_datasets.keys():\n",
        "        raw_datasets[\"validation\"] = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "            **dataset_args,\n",
        "        )\n",
        "        raw_datasets[\"train\"] = load_dataset(\n",
        "            extension,\n",
        "            data_files=data_files,\n",
        "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
        "            cache_dir=model_args.cache_dir,\n",
        "            use_auth_token=True if model_args.use_auth_token else None,\n",
        "            **dataset_args,\n",
        "        )\n",
        "\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # Distributed training:\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    config_kwargs = {\n",
        "        \"cache_dir\": model_args.cache_dir,\n",
        "        \"revision\": model_args.model_revision,\n",
        "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
        "    }\n",
        "    \n",
        "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, \n",
        "                                        **config_kwargs)\n",
        "    \n",
        "    tokenizer_kwargs = {\n",
        "        \"cache_dir\": model_args.cache_dir,\n",
        "        \"use_fast\": model_args.use_fast_tokenizer,\n",
        "        \"revision\": model_args.model_revision,\n",
        "        \"use_auth_token\": True if model_args.use_auth_token else None,\n",
        "    }\n",
        "    if model_args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
        "    elif model_args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
        "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
        "        )\n",
        "\n",
        "    torch_dtype = (\n",
        "        model_args.torch_dtype\n",
        "        if model_args.torch_dtype in [\"auto\", None]\n",
        "        else getattr(torch, model_args.torch_dtype)\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "        torch_dtype=torch_dtype,\n",
        "        low_cpu_mem_usage=model_args.low_cpu_mem_usage,\n",
        "    )\n",
        "\n",
        "    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
        "    # on a small vocab and want a smaller embedding size, remove this test.\n",
        "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
        "    if len(tokenizer) > embedding_size:\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # First we tokenize all the texts.\n",
        "    if training_args.do_train:\n",
        "        column_names = list(raw_datasets[\"train\"].features)\n",
        "    else:\n",
        "        column_names = list(raw_datasets[\"validation\"].features)\n",
        "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "    # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n",
        "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        with CaptureLogger(tok_logger) as cl:\n",
        "            output = tokenizer(examples[text_column_name])\n",
        "        # clm input could be much much longer than block_size\n",
        "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
        "            tok_logger.warning(\n",
        "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
        "                \" before being passed to the model.\"\n",
        "            )\n",
        "        return output\n",
        "\n",
        "    with training_args.main_process_first(desc=\"dataset map tokenization\"):\n",
        "        if not data_args.streaming:\n",
        "            tokenized_datasets = raw_datasets.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                num_proc=data_args.preprocessing_num_workers,\n",
        "                remove_columns=column_names,\n",
        "                load_from_cache_file=not data_args.overwrite_cache,\n",
        "                desc=\"Running tokenizer on dataset\",\n",
        "            )\n",
        "        else:\n",
        "            tokenized_datasets = raw_datasets.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                remove_columns=column_names,\n",
        "            )\n",
        "\n",
        "    if data_args.block_size is None:\n",
        "        block_size = tokenizer.model_max_length\n",
        "        if block_size > 1024:\n",
        "            print(\n",
        "                \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
        "                \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
        "                \" override this default with `--block_size xxx`.\"\n",
        "            )\n",
        "            block_size = 1024\n",
        "    else:\n",
        "        if data_args.block_size > tokenizer.model_max_length:\n",
        "            print(\n",
        "                f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model\"\n",
        "                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
        "            )\n",
        "        block_size = min(data_args.block_size, tokenizer.model_max_length)\n",
        "\n",
        "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
        "    def group_texts(examples):\n",
        "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%% IN group_texts: \")\n",
        "        print(\"examples.keys(): \", examples.keys())\n",
        "        # Concatenate all texts.\n",
        "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "        if total_length >= block_size:\n",
        "            total_length = (total_length // block_size) * block_size\n",
        "        # Split by chunks of max_len.\n",
        "        result = {\n",
        "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "\n",
        "    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
        "    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
        "    # to preprocess.\n",
        "    #\n",
        "    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
        "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
        "\n",
        "    with training_args.main_process_first(desc=\"grouping texts together\"):\n",
        "        if not data_args.streaming:\n",
        "            lm_datasets = tokenized_datasets.map(\n",
        "                group_texts,\n",
        "                batched=True,\n",
        "                num_proc=data_args.preprocessing_num_workers,\n",
        "                load_from_cache_file=not data_args.overwrite_cache,\n",
        "                desc=f\"Grouping texts in chunks of {block_size}\",\n",
        "            )\n",
        "        else:\n",
        "            lm_datasets = tokenized_datasets.map(\n",
        "                group_texts,\n",
        "                batched=True,\n",
        "            )\n",
        "\n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in tokenized_datasets:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = lm_datasets[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        if \"validation\" not in tokenized_datasets:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = lm_datasets[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "\n",
        "        def preprocess_logits_for_metrics(logits, labels):\n",
        "            if isinstance(logits, tuple):\n",
        "                # Depending on the model and config, logits may contain extra tensors,\n",
        "                # like past_key_values, but logits always come first\n",
        "                logits = logits[0]\n",
        "            return logits.argmax(dim=-1)\n",
        "\n",
        "        metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "        def compute_metrics(eval_preds):\n",
        "            preds, labels = eval_preds\n",
        "            # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
        "            # by preprocess_logits_for_metrics but we need to shift the labels\n",
        "            labels = labels[:, 1:].reshape(-1)\n",
        "            preds = preds[:, :-1].reshape(-1)\n",
        "            return metric.compute(predictions=preds, references=labels)\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset if training_args.do_train else None,\n",
        "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "        tokenizer=tokenizer,\n",
        "        # Data collator will default to DataCollatorWithPadding, so we change it.\n",
        "        data_collator=default_data_collator,\n",
        "        compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,\n",
        "        preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
        "        if training_args.do_eval and not is_torch_tpu_available()\n",
        "        else None,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        checkpoint = None\n",
        "        if training_args.resume_from_checkpoint is not None:\n",
        "            checkpoint = training_args.resume_from_checkpoint\n",
        "        elif last_checkpoint is not None:\n",
        "            checkpoint = last_checkpoint\n",
        "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "        metrics = train_result.metrics\n",
        "\n",
        "        max_train_samples = (\n",
        "            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        "        )\n",
        "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "\n",
        "    # Evaluation\n",
        "    if training_args.do_eval:\n",
        "        print(\"*** Evaluate ***\")\n",
        "\n",
        "        metrics = trainer.evaluate()\n",
        "\n",
        "        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
        "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "        try:\n",
        "            perplexity = math.exp(metrics[\"eval_loss\"])\n",
        "        except OverflowError:\n",
        "            perplexity = float(\"inf\")\n",
        "        metrics[\"perplexity\"] = perplexity\n",
        "\n",
        "        trainer.log_metrics(\"eval\", metrics)\n",
        "        trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "    kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"text-generation\"}\n",
        "    if data_args.dataset_name is not None:\n",
        "        kwargs[\"dataset_tags\"] = data_args.dataset_name\n",
        "        if data_args.dataset_config_name is not None:\n",
        "            kwargs[\"dataset_args\"] = data_args.dataset_config_name\n",
        "            kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n",
        "        else:\n",
        "            kwargs[\"dataset\"] = data_args.dataset_name\n",
        "\n",
        "    if training_args.push_to_hub:\n",
        "        trainer.push_to_hub(**kwargs)\n",
        "    else:\n",
        "        trainer.create_model_card(**kwargs)\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "vyKmzZgUwdnY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0a709ac496f64944933b4292e542e93a",
            "ceeef292ed9b43e2a4d07692754615f0",
            "62e82c17bdb242e4a66cd57e43798657",
            "ea503853f8314ec28514f75a7ad1587a",
            "e847f43d60674390958db833ae43b027",
            "7d296db9f38b4b88bc1e273d97db389a",
            "5a7952ca9fa449c495f89323d6518ce5",
            "71a8dd46cbc64869ac1e9b1c662a2d7c",
            "2cbc2d012bdd4a5e8c15afe9f8dd13d8",
            "ddc23ce52e2945f89463ebb3addbf972",
            "c7533a9e950b464a85d87a94de82d22a",
            "984ae16fac08429b9493c8d16d60e4a7",
            "821a86daf38045c7ac7b54a2c0504c38",
            "37f6e815341a458aa4eb1dff93aad573",
            "30d2091b9f9b465ab392466ef9c4e74a",
            "43a644330ca74501952ca1008072ac0f",
            "2cec3cedd32a488fb6bf4205e4333f08",
            "82ae4dc6dfa647cb8b794fedeae52326",
            "3be5acfc516f44b7b80a2bf759dc77a6",
            "2f8d4afb149b4650bbe40c4034122b4f",
            "c432f6f427e84c3fa12b633b2ed517bf",
            "3029c1d2fb6b41cbb3d3bad1ae01632a",
            "2ec9a23686c14d75bdc4e870ae9f3dad",
            "721c84d07d834f968a14cf7d5a10b7f3",
            "12314ad60e6e407b8a2d61e4b722adbe",
            "b66d5d2d72e24e429cb6b8ef81d85cfe",
            "9d4d4fd6acb24c21a53c2077e572092b",
            "41bf22b7bf9e4846a934d9f6e4b88f68",
            "c74586530021494c9603612d8584a851",
            "50b2e51637584e67872af6f111deb96d",
            "9d238a5be1414841af755f68956382f6",
            "10477b3da84041b68575c6e2ce729e12",
            "d6eb4bdba3e34ef89f6c82e182f61649",
            "4cdbd11464a74c9796f575c6b0e2c9d2",
            "414193e871ac4fbeac9024a8333b4588",
            "4f5978e36d454dcf9e1d328b04247c8b",
            "7aac76d55c544c1c992869640c0c70cf",
            "131e229e49e945619ff54f8435fa6e27",
            "cb7f647191b14eb7b1dfced3e67a99be",
            "4a419aab5de742f7bf55aa4985195bb4",
            "c1685ecd53454ee9aefe490df9dc7d13",
            "e01638dad84e4bb8bdb4c3fd4ac762b3",
            "636e4d1dc1e8466f85cc1791f9075731",
            "bdecb3785db440d58edda1febbeaa1b1"
          ]
        },
        "id": "T8Mbhx8g9c92",
        "outputId": "4a7ad415-e931-4255-975b-32f28401d81b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|training_args.py:1505] 2023-03-25 00:33:01,188 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:1285] 2023-03-25 00:33:01,918 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "INFO:datasets.builder:Using custom data configuration default-474352f1126badf3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=opt-PT/runs/Mar25_00-33-01_170b8646fc0d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=opt-PT,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=True,\n",
            "run_name=opt-PT,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from ./text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "WARNING:datasets.builder:Found cached dataset text (/content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "INFO:datasets.info:Loading Dataset info from /content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a709ac496f64944933b4292e542e93a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:datasets.builder:Using custom data configuration default-474352f1126badf3\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from ./text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "WARNING:datasets.builder:Found cached dataset text (/content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "INFO:datasets.info:Loading Dataset info from /content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "INFO:datasets.builder:Using custom data configuration default-474352f1126badf3\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.9/dist-packages/datasets/packaged_modules/text\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from ./text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "WARNING:datasets.builder:Found cached dataset text (/content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "INFO:datasets.info:Loading Dataset info from /content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "[INFO|configuration_utils.py:668] 2023-03-25 00:33:05,054 >> loading configuration file config.json from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-03-25 00:33:05,062 >> Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:668] 2023-03-25 00:33:05,141 >> loading configuration file config.json from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-03-25 00:33:05,158 >> Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-25 00:33:05,167 >> loading file vocab.json from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-25 00:33:05,172 >> loading file merges.txt from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-25 00:33:05,174 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-25 00:33:05,183 >> loading file special_tokens_map.json from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-03-25 00:33:05,185 >> loading file tokenizer_config.json from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:668] 2023-03-25 00:33:05,189 >> loading configuration file config.json from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-03-25 00:33:05,192 >> Model config OPTConfig {\n",
            "  \"_name_or_path\": \"facebook/opt-125m\",\n",
            "  \"_remove_final_layer_norm\": false,\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"OPTForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"do_layer_norm_before\": true,\n",
            "  \"dropout\": 0.1,\n",
            "  \"enable_bias\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_dim\": 3072,\n",
            "  \"hidden_size\": 768,\n",
            "  \"init_std\": 0.02,\n",
            "  \"layer_norm_elementwise_affine\": true,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"opt\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": \"</s>\",\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50272,\n",
            "  \"word_embed_proj_dim\": 768\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2403] 2023-03-25 00:33:05,370 >> loading weights file pytorch_model.bin from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-03-25 00:33:05,825 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3034] 2023-03-25 00:33:11,793 >> All model checkpoint weights were used when initializing OPTForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:3042] 2023-03-25 00:33:11,795 >> All the weights of OPTForCausalLM were initialized from the model checkpoint at facebook/opt-125m.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OPTForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:497] 2023-03-25 00:33:11,860 >> loading configuration file generation_config.json from cache at ./models--facebook--opt-125m/snapshots/3d2b5f275bdf882b8775f902e1bfdb790e2cfc32/generation_config.json\n",
            "[INFO|configuration_utils.py:575] 2023-03-25 00:33:12,060 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.27.3\"\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/237500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "984ae16fac08429b9493c8d16d60e4a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:datasets.arrow_dataset:Caching processed dataset at /content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-757e59c26af7ecc4.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/12500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ec9a23686c14d75bdc4e870ae9f3dad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:datasets.arrow_dataset:Caching processed dataset at /content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-e0b33a670f90ddee.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Grouping texts in chunks of 1024:   0%|          | 0/237500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cdbd11464a74c9796f575c6b0e2c9d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%% IN group_texts: \n",
            "examples.keys():  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:datasets.arrow_dataset:Caching processed dataset at /content/text/default-474352f1126badf3/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-6db65a4c1e023b5f.arrow\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "%%%%%%%%%%%%%%%%%%%%%%%%%%%% IN group_texts: \n",
            "examples.keys():  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-f457fd20eaa8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                                 use_fast_tokenizer=False)\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     raw_datasets = load_dataset(extension, data_files=data_files,\n\u001b[0m\u001b[1;32m     71\u001b[0m       \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_auth_token\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         return DatasetDict(\n\u001b[0;32m--> 852\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    853\u001b[0m                 k: dataset.map(\n\u001b[1;32m    854\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    851\u001b[0m         return DatasetDict(\n\u001b[1;32m    852\u001b[0m             {\n\u001b[0;32m--> 853\u001b[0;31m                 k: dataset.map(\n\u001b[0m\u001b[1;32m    854\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m         }\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2951\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2952\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 2953\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2954\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3327\u001b[0m                         )  # Something simpler?\n\u001b[1;32m   3328\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[1;32m   3330\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m                 processed_inputs = {\n",
            "\u001b[0;32m<ipython-input-29-f457fd20eaa8>\u001b[0m in \u001b[0;36mgroup_texts\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgroup_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%%%%%%%%%%%%%%%%%%%%%%%%%%%% IN group_texts: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"examples.keys(): \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Concatenate all texts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mconcatenated_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/_collections_abc.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m'{0.__class__.__name__}({0._mapping!r})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0m__class_getitem__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGenericAlias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naZubOLhWy7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}