{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Buscador TF-IDF no TREC-DL 2020"
      ],
      "metadata": {
        "id": "bDiqaFQPuNar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, é implementado um buscador com vetorização TF-IDF, que leva em conta tanto a frequência de termos em cada documento, como também a \"raridade\" de cada termo no corpus."
      ],
      "metadata": {
        "id": "Gxw_ExbPU6K9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download do dataset"
      ],
      "metadata": {
        "id": "2FtZGrPKuTwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Montagem local para acesso posterior aos arquivos"
      ],
      "metadata": {
        "id": "0UKk3WK6Kfkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkWo-SJ3uUe-",
        "outputId": "e8fd06f8-3adb-4d0d-b9ae-9a04dc8ff2df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = '/content/drive/MyDrive/Unicamp-aula-2/'\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(main_path):\n",
        "  os.makedirs(main_path)\n",
        "else:\n",
        "  print('Diretório já existente')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLcUasvzuXPB",
        "outputId": "db3801b2-26eb-44ed-9a21-96bae7f73b4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório já existente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download de ferramentas auxiliares"
      ],
      "metadata": {
        "id": "-RLVOaq8TUJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação do Pyserini para uso do Lucene Analyzer para fins de pré-processamento(tokenização, remoção de stopwords, stemming)."
      ],
      "metadata": {
        "id": "q-XUVA5NKmO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyserini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be_swSbxurh7",
        "outputId": "3fe2c294-a451-41f6-9d70-3a1ddd2e7058"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyserini\n",
            "  Downloading pyserini-0.20.0-py3-none-any.whl (137.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nmslib>=2.1.1\n",
            "  Downloading nmslib-2.1.1-cp39-cp39-manylinux2010_x86_64.whl (13.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.8.1\n",
            "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (1.2.1)\n",
            "Collecting lightgbm>=3.3.2\n",
            "  Downloading lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.9/dist-packages (from pyserini) (0.29.33)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pyserini) (4.65.0)\n",
            "Collecting pyjnius>=1.4.0\n",
            "  Downloading pyjnius-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas>=1.4.0\n",
            "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece>=0.1.95\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from lightgbm>=3.3.2->pyserini) (0.38.4)\n",
            "Collecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from nmslib>=2.1.1->pyserini) (5.4.8)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.7.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.4.0->pyserini) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from pyjnius>=1.4.0->pyserini) (1.15.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.1.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (8.1.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (1.10.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->pyserini) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->pyserini) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->pyserini) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.6.0->pyserini) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (4.0.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.1->pyserini) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.1->pyserini) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.3)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.2.1)\n",
            "Installing collected packages: tokenizers, sentencepiece, pyjnius, pybind11, humanfriendly, pandas, nmslib, huggingface-hub, coloredlogs, transformers, onnxruntime, lightgbm, pyserini\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed coloredlogs-15.0.1 huggingface-hub-0.13.0 humanfriendly-10.0 lightgbm-3.3.5 nmslib-2.1.1 onnxruntime-1.14.1 pandas-1.5.3 pybind11-2.6.1 pyjnius-1.4.2 pyserini-0.20.0 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download de ferramentas para avaliação do TREC DL 2020"
      ],
      "metadata": {
        "id": "b6byWPYsKyzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/castorini/pyserini.git --recurse-submodules {main_path}/pyserini"
      ],
      "metadata": {
        "id": "O97kuBrZTyZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd {main_path}/pyserini/tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make && cd ../../..\n",
        "!cd {main_path}/pyserini/tools/eval/ndeval && make && cd ../../.."
      ],
      "metadata": {
        "id": "OjR4FDswUJeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c163cc8-aecf-41bf-8b77-10543575bbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trec_eval.9.0.4/\n",
            "trec_eval.9.0.4/m_prefs_pair.c\n",
            "trec_eval.9.0.4/m_ndcg_p.c\n",
            "trec_eval.9.0.4/m_infap.c\n",
            "trec_eval.9.0.4/m_num_q.c\n",
            "trec_eval.9.0.4/m_iprec_at_recall.c\n",
            "trec_eval.9.0.4/form_prefs_counts.c\n",
            "trec_eval.9.0.4/m_prefs_num_prefs_ful_ret.c\n",
            "trec_eval.9.0.4/utility_pool.c\n",
            "trec_eval.9.0.4/m_binG.c\n",
            "trec_eval.9.0.4/meas_avg.c\n",
            "trec_eval.9.0.4/m_gm_bpref.c\n",
            "trec_eval.9.0.4/m_runid.c\n",
            "trec_eval.9.0.4/m_bpref.c\n",
            "trec_eval.9.0.4/m_gm_map.c\n",
            "trec_eval.9.0.4/trec_eval.h\n",
            "trec_eval.9.0.4/m_yaap.c\n",
            "trec_eval.9.0.4/m_relstring.c\n",
            "trec_eval.9.0.4/m_Rprec.c\n",
            "trec_eval.9.0.4/m_prefs_avgjg.c\n",
            "trec_eval.9.0.4/m_success.c\n",
            "trec_eval.9.0.4/m_ndcg.c\n",
            "trec_eval.9.0.4/functions.h\n",
            "trec_eval.9.0.4/m_P_avgjg.c\n",
            "trec_eval.9.0.4/test/\n",
            "trec_eval.9.0.4/test/qrels.rel_level\n",
            "trec_eval.9.0.4/test/results.test\n",
            "trec_eval.9.0.4/test/qrels.test\n",
            "trec_eval.9.0.4/test/out.test.qrels_jg\n",
            "trec_eval.9.0.4/test/out.test.meas_params\n",
            "trec_eval.9.0.4/test/out.test.a\n",
            "trec_eval.9.0.4/test/out.test.prefs\n",
            "trec_eval.9.0.4/test/out.test.aqcM\n",
            "trec_eval.9.0.4/test/out.test.aql\n",
            "trec_eval.9.0.4/test/prefs.test\n",
            "trec_eval.9.0.4/test/out.test\n",
            "trec_eval.9.0.4/test/out.test.aq\n",
            "trec_eval.9.0.4/test/out.test.aqc\n",
            "trec_eval.9.0.4/test/out.test.qrels_prefs\n",
            "trec_eval.9.0.4/test/zscores_file\n",
            "trec_eval.9.0.4/test/qrels.123\n",
            "trec_eval.9.0.4/test/out.test.aqZ\n",
            "trec_eval.9.0.4/test/results.trunc\n",
            "trec_eval.9.0.4/test/prefs.results.test\n",
            "trec_eval.9.0.4/test/prefs.rank20\n",
            "trec_eval.9.0.4/m_11pt_avg.c\n",
            "trec_eval.9.0.4/m_G.c\n",
            "trec_eval.9.0.4/m_num_rel.c\n",
            "trec_eval.9.0.4/m_map_cut.c\n",
            "trec_eval.9.0.4/m_prefs_avgjg_ret.c\n",
            "trec_eval.9.0.4/m_Rprec_mult.c\n",
            "trec_eval.9.0.4/Makefile\n",
            "trec_eval.9.0.4/m_map_avgjg.c\n",
            "trec_eval.9.0.4/get_qrels_prefs.c\n",
            "trec_eval.9.0.4/README\n",
            "trec_eval.9.0.4/m_set_rel_P.c\n",
            "trec_eval.9.0.4/sysfunc.h\n",
            "trec_eval.9.0.4/m_prefs_pair_ret.c\n",
            "trec_eval.9.0.4/convert_zscores.c\n",
            "trec_eval.9.0.4/m_ndcg_cut.c\n",
            "trec_eval.9.0.4/m_prefs_pair_imp.c\n",
            "trec_eval.9.0.4/meas_print_single.c\n",
            "trec_eval.9.0.4/meas_print_final.c\n",
            "trec_eval.9.0.4/trec_eval.c\n",
            "trec_eval.9.0.4/m_num_ret.c\n",
            "trec_eval.9.0.4/get_prefs.c\n",
            "trec_eval.9.0.4/m_P.c\n",
            "trec_eval.9.0.4/get_qrels_jg.c\n",
            "trec_eval.9.0.4/m_rel_P.c\n",
            "trec_eval.9.0.4/meas_acc.c\n",
            "trec_eval.9.0.4/m_prefs_simp.c\n",
            "trec_eval.9.0.4/m_recall.c\n",
            "trec_eval.9.0.4/trec_format.h\n",
            "trec_eval.9.0.4/m_ndcg_rel.c\n",
            "trec_eval.9.0.4/m_num_nonrel_judged_ret.c\n",
            "trec_eval.9.0.4/formats.c\n",
            "trec_eval.9.0.4/bpref_bug\n",
            "trec_eval.9.0.4/README.windows.md\n",
            "trec_eval.9.0.4/m_prefs_num_prefs_ful.c\n",
            "trec_eval.9.0.4/m_set_map.c\n",
            "trec_eval.9.0.4/get_qrels.c\n",
            "trec_eval.9.0.4/m_set_F.c\n",
            "trec_eval.9.0.4/measures.c\n",
            "trec_eval.9.0.4/common.h\n",
            "trec_eval.9.0.4/meas_init.c\n",
            "trec_eval.9.0.4/m_recip_rank.c\n",
            "trec_eval.9.0.4/m_set_recall.c\n",
            "trec_eval.9.0.4/get_trec_results.c\n",
            "trec_eval.9.0.4/m_prefs_simp_ret.c\n",
            "trec_eval.9.0.4/m_num_rel_ret.c\n",
            "trec_eval.9.0.4/m_map.c\n",
            "trec_eval.9.0.4/m_utility.c\n",
            "trec_eval.9.0.4/form_res_rels.c\n",
            "trec_eval.9.0.4/form_res_rels_jg.c\n",
            "trec_eval.9.0.4/m_prefs_simp_imp.c\n",
            "trec_eval.9.0.4/m_prefs_num_prefs_poss.c\n",
            "trec_eval.9.0.4/m_prefs_avgjg_Rnonrel.c\n",
            "trec_eval.9.0.4/m_prefs_avgjg_Rnonrel_ret.c\n",
            "trec_eval.9.0.4/m_set_P.c\n",
            "trec_eval.9.0.4/m_prefs_avgjg_imp.c\n",
            "trec_eval.9.0.4/m_Rndcg.c\n",
            "trec_eval.9.0.4/CHANGELOG\n",
            "trec_eval.9.0.4/m_Rprec_mult_avgjg.c\n",
            "trec_eval.9.0.4/get_zscores.c\n",
            "make: 'trec_eval' is up to date.\n",
            "make: 'ndeval' is up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construção do índice invertido"
      ],
      "metadata": {
        "id": "ZmvMxtynTmKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.analysis import Analyzer, get_lucene_analyzer"
      ],
      "metadata": {
        "id": "cCpsvCE7u0o7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSMFneIku5L3",
        "outputId": "966447c4-6a8f-4a19-e34a-e5aa2b2e5e2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código para pré-processamento textual"
      ],
      "metadata": {
        "id": "WJURusuaK7Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49JWfNjWu8ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "collection_path = main_path + '/collections/msmarco-passage/collection.tsv'\n",
        "\n"
      ],
      "metadata": {
        "id": "kwy8QAUCvBi1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foi verificado que o analisador do Lucene não elimina suficientemente stopwords.  Assim, foi utilizada adicionalmente a lista de stopwords do NLTK."
      ],
      "metadata": {
        "id": "Rvf_VswGLCz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')  # Download stopwords if not already downloaded\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words = set(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhc11tGLJPWe",
        "outputId": "4349eb50-d230-4822-a9a1-eeb915f08d4f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer:Analyzer = Analyzer(get_lucene_analyzer(stemmer='porter'))\n",
        "\n",
        "def preprocess_and_tokenize(text):\n",
        "  #remove previamente as stopwords do NLTK -> isso pode mais adiante interferir na contagem de termos/frquências\n",
        "  tokens = text.lower().split()\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "  text = ' '.join(tokens)\n",
        "  return analyzer.analyze(text)"
      ],
      "metadata": {
        "id": "eEhVpOBO6bhk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, será construído o índice invertido para associar os tokens aos documentos nos quais eles ocorrem.  O uso da estrutura de dados array (sugestão do colega [Leandro Carísio](https://colab.research.google.com/drive/1hELJYqsvUyja9HPeDzc9FU8okqdIjODE?usp=sharing)) resultou em um ganho relevante em memória em relação a listas.  Tal estrutura de dados, ao contrário dos arrays do Numpy, assemelha-se a um array dinâmico, ou lista, porém ocupa bem menos memória.\n",
        "\n",
        "Para subsidir posteriormente o algoritmo baseado em TF-IDF, é armazenada adicionalmente a frequência de termos (TF) para cada documento, na entrada correspondente."
      ],
      "metadata": {
        "id": "knnsEWfGLRk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import array\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "index_path = f\"{main_path}/index-tf-idf.pickle\"\n",
        "\n",
        "def load_or_build_inverted_index():\n",
        "  if os.path.exists(index_path):\n",
        "    with open(index_path, \"rb\") as f:\n",
        "      print(\"Loading index...\")\n",
        "      index = pickle.load(f)\n",
        "  else:\n",
        "    print(\"Building inverted index and vocabulary...\")\n",
        "    # set the chunk size\n",
        "    chunk_size = 1000\n",
        "    chunks = []\n",
        "    inverted_index = dict()\n",
        "    full_text = ''\n",
        "\n",
        "    def process(row):\n",
        "      tokenized_text = preprocess_and_tokenize(row[1])\n",
        "      counter = Counter(tokenized_text)\n",
        "      doc_length = len(tokenized_text)\n",
        "      doc_id = row[0]\n",
        "      for token, count in counter.items():\n",
        "        if token not in stop_words:\n",
        "          #Para cada token, temos 2 arrays paralelos que armazenam os documentos e as frquências dos termos\n",
        "          inverted_index.setdefault(token, {\"docs\":array.array(\"L\", []), \"tf\":array.array(\"f\", [])})[\"docs\"].append(int(doc_id))\n",
        "          inverted_index.setdefault(token, {\"docs\":array.array(\"L\", []), \"tf\":array.array(\"f\", [])})[\"tf\"].append(count/doc_length)\n",
        "\n",
        "    chunk_id = 0\n",
        "    n_documents = 0\n",
        "\n",
        "    # iterate through the file in chunks\n",
        "    for chunk in pd.read_csv(collection_path, sep='\\t', header=None, chunksize=chunk_size):\n",
        "      # process the chunk here\n",
        "      if (chunk_id % 1000) == 0:\n",
        "        print(f'Processing chunk {chunk_id}')\n",
        "      for index, row in chunk.iterrows():\n",
        "        process(row)\n",
        "        n_documents += 1\n",
        "      del(chunk)\n",
        "      chunk_id += 1\n",
        "\n",
        "    index = {\"inverted_index\": inverted_index, \"n_documents\": n_documents}\n",
        "\n",
        "    with open(index_path, \"wb\") as f:\n",
        "      pickle.dump(index, f)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "O8GurqwF5yhX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = load_or_build_inverted_index()\n",
        "inverted_index = index[\"inverted_index\"]\n",
        "n_documents = index[\"n_documents\"]"
      ],
      "metadata": {
        "id": "N22tbji3zYev",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "4254ccb1-2a73-440e-e1fe-6d4a82056b19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building inverted index and vocabulary...\n",
            "Processing chunk 0\n",
            "Processing chunk 1000\n",
            "Processing chunk 2000\n",
            "Processing chunk 3000\n",
            "Processing chunk 4000\n",
            "Processing chunk 5000\n",
            "Processing chunk 6000\n",
            "Processing chunk 7000\n",
            "Processing chunk 8000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7f9f4bbf813a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#OBS.: Na versão do índice invertido salva inicialmente, foi salvo também o vocabulário.  Posteriormente, verificou-se que não havia necessidade desse vocabulário em memória.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'vocab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(inverted_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD1BrbHRvMDL",
        "outputId": "0c8dbfb9-f12a-432d-c183-9213d8833707"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2660662"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head {collection_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqUvI7BC5VHw",
        "outputId": "08bffacf-d2ea-4907-de1a-6fa83b55bedb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tThe presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.\n",
            "1\tThe Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.\n",
            "2\tEssay on The Manhattan Project - The Manhattan Project The Manhattan Project was to see if making an atomic bomb possible. The success of this project would forever change the world forever making it known that something this powerful can be manmade.\n",
            "3\tThe Manhattan Project was the name for a project conducted during World War II, to develop the first atomic bomb. It refers specifically to the period of the project from 194 â¦ 2-1946 under the control of the U.S. Army Corps of Engineers, under the administration of General Leslie R. Groves.\n",
            "4\tversions of each volume as well as complementary websites. The first websiteâThe Manhattan Project: An Interactive Historyâis available on the Office of History and Heritage Resources website, http://www.cfo. doe.gov/me70/history. The Office of History and Heritage Resources and the National Nuclear Security\n",
            "5\tThe Manhattan Project. This once classified photograph features the first atomic bomb â a weapon that atomic scientists had nicknamed Gadget.. The nuclear age began on July 16, 1945, when it was detonated in the New Mexico desert.\n",
            "6\tNor will it attempt to substitute for the extraordinarily rich literature on the atomic bombs and the end of World War II. This collection does not attempt to document the origins and development of the Manhattan Project.\n",
            "7\tManhattan Project. The Manhattan Project was a research and development undertaking during World War II that produced the first nuclear weapons. It was led by the United States with the support of the United Kingdom and Canada. From 1942 to 1946, the project was under the direction of Major General Leslie Groves of the U.S. Army Corps of Engineers. Nuclear physicist Robert Oppenheimer was the director of the Los Alamos Laboratory that designed the actual bombs. The Army component of the project was designated the\n",
            "8\tIn June 1942, the United States Army Corps of Engineersbegan the Manhattan Project- The secret name for the 2 atomic bombs.\n",
            "9\tOne of the main reasons Hanford was selected as a site for the Manhattan Project's B Reactor was its proximity to the Columbia River, the largest river flowing into the Pacific Ocean from the North American coast.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação"
      ],
      "metadata": {
        "id": "K4t5V238T7d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics_file = main_path + '/pyserini/tools/topics-and-qrels/topics.dl20.txt'\n",
        "qrels_eval = main_path + '/pyserini/tools/topics-and-qrels/qrels.dl20-passage.txt'"
      ],
      "metadata": {
        "id": "aiujR-Y6KUtf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head {topics_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F0hqvNoLta6",
        "outputId": "c5cdb06c-d005-47b3-cd01-b7ff90fc5b55"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1030303\twho is aziz hashim\r\n",
            "1037496\twho is rep scalise?\r\n",
            "1043135\twho killed nicholas ii of russia\r\n",
            "1045109\twho owns barnhart crane\r\n",
            "1049519\twho said no one can make you feel inferior\r\n",
            "1051399\twho sings monk theme song\r\n",
            "1056416\twho was the highest career passer  rating in the nfl\r\n",
            "1064670\twhy do hunters pattern their shotguns?\r\n",
            "1065636\twhy do some places on my scalp feel sore\r\n",
            "1071750\twhy is pete rose banned from hall of fame\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head {qrels_eval}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ8H_jWOfBMs",
        "outputId": "5464dd50-610c-4666-98db-b67877cef7dd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23849 0 1020327 2\n",
            "23849 0 1034183 3\n",
            "23849 0 1120730 0\n",
            "23849 0 1139571 1\n",
            "23849 0 1143724 0\n",
            "23849 0 1147202 0\n",
            "23849 0 1150311 0\n",
            "23849 0 1158886 2\n",
            "23849 0 1175024 1\n",
            "23849 0 1201385 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir, a rotina de busca.  Para escore de similaridade dentre queries e documentos, foi utilizado o produto escalar. A normalização (similaridade de cossenos) não foi realizada para evitar o overhead de memória associado à construçãod e vetores do mesmo tamanho do vocabulárop (que term cerca de 2,6 milhões de termos)."
      ],
      "metadata": {
        "id": "z9XozRXZMBPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def search(query):\n",
        "  doc_scores = defaultdict(int) # int (doc_id) -> int (score)\n",
        "  query_tokens = preprocess_and_tokenize(query)\n",
        "  n_query_tokens = len(query_tokens)\n",
        "  query_counter = Counter(query_tokens)\n",
        "\n",
        "  for token in query_counter.keys():\n",
        "    if token in inverted_index:\n",
        "      #Calcula TF-IDF para par (termo, query)\n",
        "      query_tf = query_counter[token]/n_query_tokens\n",
        "      doc_ids = inverted_index[token][\"docs\"]\n",
        "      n_docs_contain_term = len(set(doc_ids))\n",
        "      idf = math.log(n_documents / n_docs_contain_term)\n",
        "      query_tf_idf = query_tf * idf\n",
        "\n",
        "      for i, doc_id in enumerate(doc_ids):\n",
        "        #Calcula TF-IDF para par (termo, documento)\n",
        "        doc_tf = inverted_index[token][\"tf\"][i]\n",
        "        doc_tf_idf = doc_tf * idf\n",
        "        \n",
        "        #Incrementa para implementar o produto escalar entre o \"vetor\" da \n",
        "        #query e o \"vetor\" do documento\n",
        "        doc_scores[doc_id] += query_tf_idf * doc_tf_idf\n",
        "    \n",
        "          \n",
        "  return doc_scores"
      ],
      "metadata": {
        "id": "1VjKwZJMLvgI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = search('who is aziz hashim')"
      ],
      "metadata": {
        "id": "yfHwaJWhNYBF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0nNth1oNnNe",
        "outputId": "221f4399-4f62-4785-8d9f-a31073c90e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "245"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_document_by_id(id):\n",
        "  result = None\n",
        "  with open(collection_path, 'r') as f:\n",
        "    for line in f:\n",
        "      fields = line.strip().split('\\t')\n",
        "      doc_id = fields[0]\n",
        "      if doc_id == id:\n",
        "        result = fields[1]\n",
        "        break\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "eExn_oy6Wh3w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "id": "A0O3yBIEYXDg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-udZL3mbY32L",
        "outputId": "7e0410ab-6e78-477b-9e12-232e067dcaec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(8726436, 14.123521231345256),\n",
              " (309441, 9.833169050913053),\n",
              " (7168318, 7.111725523870475),\n",
              " (7970461, 7.111725523870475),\n",
              " (1305521, 6.807578362373994),\n",
              " (1305520, 6.704433293948597),\n",
              " (3738183, 6.693388753283498),\n",
              " (1305528, 6.507244047098867),\n",
              " (8726435, 6.011509794808267),\n",
              " (1451848, 5.689380503874754)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_document_by_id('8726436')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xnUN599MY5XY",
        "outputId": "cc6daa6e-ccd4-4601-b5e3-0c4eb9705ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Share on LinkedInShare on FacebookShare on TwitterShare on Google+. 1  Public Profile Aziz Hashim Managing Partner at NRD Capital. 2  Public Profile Hazizul Aziz Mohamad Hashim Project Accountant/ Cost control. 3  Public Profile Aziz Hashim 4  --. Public Profile syed aziz syed hashim --.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As próximas células executam as rotinas de preparação e cálculo das métricas segundo o dataset de teste do TREC DL 2020."
      ],
      "metadata": {
        "id": "Y807fPQcMqxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_to_results = dict()\n",
        "i = 0\n",
        "\n",
        "with open(topics_file, 'r') as f:\n",
        "  for line in f:\n",
        "      i += 1\n",
        "      fields = line.strip().split('\\t')\n",
        "      query_id = fields[0]\n",
        "      query_text = fields[1]\n",
        "      results = search(query_text)\n",
        "      query_to_results[int(query_id)] = sorted(results.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "with open('run.dl20.tfidf.trec', 'w') as f:\n",
        "  for query_id, results in query_to_results.items():\n",
        "    for i, (doc_id, score) in enumerate(results):\n",
        "      f.write(f'{query_id}\\tQ0\\t{doc_id}\\t{i+1}\\t{score}\\ttf_idf\\n')"
      ],
      "metadata": {
        "id": "cLL1cRNcZAt2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head run.dl20.boolean.trec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiRHnKrlFPyw",
        "outputId": "5601f631-131b-44ae-9973-418c200cd441"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1030303\tQ0\t8726436\t1\t14.123521231345256\tboolean\n",
            "1030303\tQ0\t309441\t2\t9.833169050913053\tboolean\n",
            "1030303\tQ0\t7168318\t3\t7.111725523870475\tboolean\n",
            "1030303\tQ0\t7970461\t4\t7.111725523870475\tboolean\n",
            "1030303\tQ0\t1305521\t5\t6.807578362373994\tboolean\n",
            "1030303\tQ0\t1305520\t6\t6.704433293948597\tboolean\n",
            "1030303\tQ0\t3738183\t7\t6.693388753283498\tboolean\n",
            "1030303\tQ0\t1305528\t8\t6.507244047098867\tboolean\n",
            "1030303\tQ0\t8726435\t9\t6.011509794808267\tboolean\n",
            "1030303\tQ0\t1451848\t10\t5.689380503874754\tboolean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python {main_path}/pyserini/tools/scripts/msmarco/convert_msmarco_to_trec_qrels.py \\\n",
        "   --input {qrels_eval} \\\n",
        "   --output qrels.dl20.trec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u51xHcmNGJ2i",
        "outputId": "13628523-9b09-42de-d52d-b520a1ba8ce1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head qrels.dl20.trec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtocDxsYHZDV",
        "outputId": "a2e0d39f-34b5-4389-f17d-b71a47c2e47f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23849 0 1020327 2\n",
            "23849 0 1034183 3\n",
            "23849 0 1120730 0\n",
            "23849 0 1139571 1\n",
            "23849 0 1143724 0\n",
            "23849 0 1147202 0\n",
            "23849 0 1150311 0\n",
            "23849 0 1158886 2\n",
            "23849 0 1175024 1\n",
            "23849 0 1201385 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 755 {main_path}/pyserini/tools/eval/trec_eval.9.0.4/trec_eval"
      ],
      "metadata": {
        "id": "4fNz7EIP_ua0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!{main_path}/pyserini/tools/eval/trec_eval.9.0.4/trec_eval -c -m map -m ndcg_cut.10 -l 2 \\\n",
        "   qrels.dl20.trec run.dl20.tfidf.trec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMNRIGZZHpu6",
        "outputId": "d533fdc8-ef4e-48d7-8f3c-9c3628207655"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "map                   \tall\t0.0781\n",
            "ndcg_cut_10           \tall\t0.2043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrtE3v2-zZNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}